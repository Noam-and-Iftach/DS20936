{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKfQV703K8jL",
        "outputId": "e811b085-f754-422d-a6cb-4fc4abeaef05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiOHq5bDJUiM"
      },
      "outputs": [],
      "source": [
        "file_path=f\"/content/drive/MyDrive/books_to_kindle_and_books_parquets/books/1_book_after_preprocessing.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDAsEQj1LXeR"
      },
      "outputs": [],
      "source": [
        "df = pl.read_parquet(file_path)\n",
        "df_pandas = df.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKMCDLz3gJ45"
      },
      "source": [
        "We take only those who are had reviewed at least 50 reviews and no more than 100. On one hand, we do not want users that reviewed 3000 books, with the same review, because this are not legitimic users. On the other hand, we understood that the way recommendation system works is based on data that the user voted, and this is the minimun to make the recommendation system work. Also, some models collapsed while we had other filters such as 10 to 100, and this is just the first file.\n",
        " At first we just want to build few pytorch-based recommendation models, and then, once deciding which model we would use, based on its preformance, we will expand the training input, to be not more than just the first file (we have 14 in the drive, making it able to read the whole data in parts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey_6JH66L2qf",
        "outputId": "4d650a43-e0c2-47d4-cb96-8c6c126ea410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users with 50 -100 records: 1869\n"
          ]
        }
      ],
      "source": [
        "user_counts = df_pandas['reviewerID'].value_counts()\n",
        "users_with_50_100_reviews = user_counts[(user_counts >= 50) & (user_counts <= 100)]\n",
        "num_users_50_100_reviews = len(users_with_50_100_reviews)\n",
        "print(f\"Number of users with 50 -100 records: {num_users_50_100_reviews}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Il8SbZMfL9",
        "outputId": "523a82c4-6544-44f1-da97-217630fb305f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered DataFrame:\n",
            "         overall  verified      reviewerID        asin  \\\n",
            "15             5     False  A3H9YD6K9TVKDP  0001713353   \n",
            "47             5     False  A3QYDL5CDNYN66  0001061240   \n",
            "72             5     False  A1BNWEJ7RVPLQ1  0001712799   \n",
            "98             4     False  A3CKPNSGA7JOLK  0001712799   \n",
            "106            5     False  A2MOBMVHECYVLE  0002006448   \n",
            "...          ...       ...             ...         ...   \n",
            "3688416        5     False  A1NSJ7IFI4IZ4Q  0316403490   \n",
            "3688435        5     False  A25OO2VLCIWV51  0316403490   \n",
            "3688471        5     False  A2GGSOEKD5VCPK  0316403784   \n",
            "3688480        1      True  A1MD99Z7WM27LS  0316403784   \n",
            "3688493        4      True  A2CISZ4JLKQRJA  0316403784   \n",
            "\n",
            "                             style  \\\n",
            "15       {'Format:': ' Hardcover'}   \n",
            "47       {'Format:': ' Hardcover'}   \n",
            "72       {'Format:': ' Hardcover'}   \n",
            "98       {'Format:': ' Hardcover'}   \n",
            "106      {'Format:': ' Hardcover'}   \n",
            "...                            ...   \n",
            "3688416  {'Format:': ' Hardcover'}   \n",
            "3688435  {'Format:': ' Hardcover'}   \n",
            "3688471  {'Format:': ' Paperback'}   \n",
            "3688480  {'Format:': ' Paperback'}   \n",
            "3688493  {'Format:': ' Paperback'}   \n",
            "\n",
            "                                                reviewText  \\\n",
            "15       Over and over the king has problems.  Fortunat...   \n",
            "47       This was a favorite. I think it changed my lif...   \n",
            "72       In this early reader, Dr. Seuss explores the c...   \n",
            "98       Dr. Suess's scansion and made-up words annoy m...   \n",
            "106      Christopher Kremmer's book takes you on a jour...   \n",
            "...                                                    ...   \n",
            "3688416  Eleven-year-old Cornelia Warne is destitute wh...   \n",
            "3688435  Summary:  11-year-old Nell Warne has had a tou...   \n",
            "3688471  Jim Thompson has a well-deserved reputation as...   \n",
            "3688480  Yes, readers luv it. They swoon over it.\\n\\nI ...   \n",
            "3688493  This book, by Jim Thompson, is narrated by the...   \n",
            "\n",
            "                                                   summary  __index_level_0__  \n",
            "15                                           Not Nice Mice                 15  \n",
            "47                                         Changed my life                 47  \n",
            "72                     Two thumbs up for this early reader                 72  \n",
            "98          A fine first read with wonderful illustrations                 98  \n",
            "106                                      A compelling read                106  \n",
            "...                                                    ...                ...  \n",
            "3688416         Courtesy of Mother Daughter Book Club. com            3739741  \n",
            "3688435  Like a series of 19th-century Nancy Drew myste...            3739760  \n",
            "3688471                     A Literary Masterpiece Of Pulp            3739796  \n",
            "3688480            A WTF OUTTA MIND ADVENTURE ZZZZZZZZZZZZ            3739805  \n",
            "3688493                        Things Are Not As They Seem            3739818  \n",
            "\n",
            "[127675 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "filtered_users = users_with_50_100_reviews.index.tolist()\n",
        "df_filtered = df_pandas[df_pandas['reviewerID'].isin(filtered_users)]\n",
        "print(\"Filtered DataFrame:\")\n",
        "print(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVPKl31iti7z",
        "outputId": "455d86b0-ea21-4bdf-c589-24535fa809b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "del df_pandas, df\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydc8aNZQQN5u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "3Enx_hvdLXfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import json\n",
        "from sklearn.metrics import ndcg_score"
      ],
      "metadata": {
        "id": "KpBUiW1gQ3RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFre2VdR5FDu"
      },
      "source": [
        "We start with the most simple model and then step by step improve it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu5XojbEZC9a"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df, test_df = train_test_split(df_filtered, test_size=0.2)\n",
        "\n",
        "# Create dictionaries to map user and book IDs to integers\n",
        "user_to_idx = {user_id: i for i, user_id in enumerate(df_filtered['reviewerID'].unique())}\n",
        "book_to_idx = {asin: i for i, asin in enumerate(df_filtered['asin'].unique())}\n",
        "\n",
        "# Convert user and book IDs to integers in the DataFrame\n",
        "train_df['user_idx'] = train_df['reviewerID'].map(user_to_idx)\n",
        "train_df['book_idx'] = train_df['asin'].map(book_to_idx)\n",
        "test_df['user_idx'] = test_df['reviewerID'].map(user_to_idx)\n",
        "test_df['book_idx'] = test_df['asin'].map(book_to_idx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkV3n_2WhrgM"
      },
      "source": [
        "# First model: simple user/books embedding\n",
        "The embedding is the part that creates random values which represent the similarity between users and books (with length of n_user or n_books multiplied by n_factors). It could have been achieved also with nn.Parameter(user_factors), and then creating the random values with randn, however, it seems more common in recommendation system to use the embedding in the creation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xZ7WBeSdCbB"
      },
      "outputs": [],
      "source": [
        "class CFModel(nn.Module):\n",
        "    def __init__(self, df_filtered, n_factors=5):\n",
        "        super(CFModel, self).__init__()\n",
        "        n_users = df_filtered['reviewerID'].nunique()\n",
        "        n_books = df_filtered['asin'].nunique()\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.books_factors = nn.Embedding(n_books, n_factors)\n",
        "\n",
        "    def forward(self, user_idx, book_idx):\n",
        "        user_embed = self.user_factors(user_idx)\n",
        "        book_embed = self.books_factors(book_idx)\n",
        "        pred_rating = (user_embed * book_embed).sum(dim=1)\n",
        "        return pred_rating\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are storing the models with pickle using built-in pytorch saving syntax, after the training process"
      ],
      "metadata": {
        "id": "-xBPBLFm_Eun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KasOcnWrdFp6",
        "outputId": "0a156a77-0830-4cb5-a316-d15083352bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 22.1751\n",
            "Epoch 2/20, Average Loss: 19.4319\n",
            "Epoch 3/20, Average Loss: 17.6031\n",
            "Epoch 4/20, Average Loss: 16.2088\n",
            "Epoch 5/20, Average Loss: 15.0018\n",
            "Epoch 6/20, Average Loss: 13.8425\n",
            "Epoch 7/20, Average Loss: 12.6764\n",
            "Epoch 8/20, Average Loss: 11.4865\n",
            "Epoch 9/20, Average Loss: 10.2646\n",
            "Epoch 10/20, Average Loss: 9.0469\n",
            "Epoch 11/20, Average Loss: 7.8633\n",
            "Epoch 12/20, Average Loss: 6.7476\n",
            "Epoch 13/20, Average Loss: 5.7357\n",
            "Epoch 14/20, Average Loss: 4.8451\n",
            "Epoch 15/20, Average Loss: 4.0881\n",
            "Epoch 16/20, Average Loss: 3.4515\n",
            "Epoch 17/20, Average Loss: 2.9226\n",
            "Epoch 18/20, Average Loss: 2.4878\n",
            "Epoch 19/20, Average Loss: 2.1303\n",
            "Epoch 20/20, Average Loss: 1.8364\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "user_idx_tensor = torch.LongTensor(train_df['user_idx'].values).to(device)\n",
        "book_idx_tensor = torch.LongTensor(train_df['book_idx'].values).to(device)\n",
        "ratings_tensor = torch.FloatTensor(train_df['overall'].values).to(device)\n",
        "train_dataset = TensorDataset(user_idx_tensor, book_idx_tensor, ratings_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "model = CFModel(df_filtered).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_idx, book_idx, ratings in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred_ratings = model(user_idx, book_idx)\n",
        "        loss = criterion(pred_ratings, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/models/first_cf_model.pth')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ueioJb6JuG2"
      },
      "source": [
        "\n",
        "At the begining we started with evaluation of the RMSE, however, after a while, we decided to add NDCG since it prioritizes the ranking of recommendations over the absolute accuracy of predicted ratings, which is crucial in scenarios like movie or product suggestions where a precise rating (such as 4.2 vs. 4.5) is less critical than ensuring the most appreciated items top the list. NDCG also emphasizes the relevance of items at the start of the recommendation list, where user engagement is highest, thus enhancing user satisfaction as users typically interact more with these initial recommendations. Additionally, unlike RMSE, NDCG can be applied to data without explicit numerical ratings, handling both binary and graded relevance, making it versatile across various recommendation system types. This focus on user experience aligns closely with business objectives, such as increasing user engagement, satisfaction, and retention, by ensuring the quality of the rankings and prioritizing the most impactful part of the list.\n",
        "\n",
        "Evantually we ended up with many more metrics so we can evaluate the model performances based on multiple values, hearing the instructor opinion, and also maybe gaining a deeper perspective."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, precision_recall_fscore_support, ndcg_score, label_ranking_average_precision_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MstQEj8Oj-4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mrr(true_ratings, pred_ratings):\n",
        "    order = np.argsort(-pred_ratings)  # Get indices of sorted ratings in descending order\n",
        "    true_order = np.argsort(-true_ratings)\n",
        "    rank = np.empty_like(order)\n",
        "    true_rank = np.empty_like(true_order)\n",
        "    rank[order] = np.arange(len(pred_ratings))\n",
        "    true_rank[true_order] = np.arange(len(true_ratings))\n",
        "\n",
        "    # Determine ranks of the true relevant items\n",
        "    relevance_ranks = true_rank[order]\n",
        "    mrr = 0.0\n",
        "    for r in relevance_ranks:\n",
        "        mrr += 1.0 / (r + 1)\n",
        "    mrr /= len(relevance_ranks)\n",
        "    return mrr\n",
        "\n"
      ],
      "metadata": {
        "id": "voN9mnkgm_oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "\n",
        "def calculate_map(true_ratings, pred_ratings):\n",
        "    # Create a sorted index based on true ratings, high to low\n",
        "    ideal_rank = np.argsort(-true_ratings)\n",
        "    # Create a sorted index based on predicted ratings, high to low\n",
        "    predicted_rank = np.argsort(-pred_ratings)\n",
        "\n",
        "    # Generate ideal and predicted rank lists\n",
        "    ideal_rank_list = [np.where(ideal_rank == i)[0][0] for i in range(len(true_ratings))]\n",
        "    predicted_rank_list = [np.where(predicted_rank == i)[0][0] for i in range(len(true_ratings))]\n",
        "\n",
        "    # Convert ranks to binary relevance: top X% as relevant\n",
        "    cutoff_percent = 20\n",
        "    cutoff = len(true_ratings) * cutoff_percent // 100\n",
        "    ideal_relevance = [1 if x < cutoff else 0 for x in ideal_rank_list]\n",
        "    predicted_relevance = [1 if x < cutoff else 0 for x in predicted_rank_list]\n",
        "\n",
        "    return label_ranking_average_precision_score([ideal_relevance], [predicted_relevance])\n"
      ],
      "metadata": {
        "id": "iVAXmYrcnvNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDl5LoCrH91S",
        "outputId": "e3298573-7c8a-4cc1-9d51-385d28d30555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "NDCG: 0.9836\n",
            "MAE: 2.8247\n",
            "MRR: 0.0004\n",
            "Precision: 0.0423\n",
            "Recall: 0.0290\n",
            "F1-score: 0.0282\n",
            "RMSE: 1.9486\n",
            "MAP: 0.2064\n"
          ]
        }
      ],
      "source": [
        "user_idx_tensor_test = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
        "book_idx_tensor_test = torch.LongTensor(test_df['book_idx'].values).to(device)\n",
        "model.eval()\n",
        "pred_ratings_test = model(user_idx_tensor_test, book_idx_tensor_test).detach().cpu().numpy()\n",
        "true_ratings = test_df['overall'].values\n",
        "\n",
        "ndcg = ndcg_score([true_ratings], [pred_ratings_test])\n",
        "\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings_test)\n",
        "\n",
        "mrr = calculate_mrr(true_ratings, pred_ratings_test)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_ratings, pred_ratings_test.round(), average='macro')\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings_test, squared=False))\n",
        "\n",
        "map_score = calculate_map(true_ratings, pred_ratings_test)\n",
        "print(f\"Metrics:\\nNDCG: {ndcg:.4f}\\nMAE: {mae:.4f}\\nMRR: {mrr:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\\nRMSE: {rmse:.4f}\\nMAP: {map_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to store that metrics for later use, so we would be able to compare the models without retraining them."
      ],
      "metadata": {
        "id": "FOhs1El6-2LT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0s_2vBfImg2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def save_performance_data(model_name, mae, rmse, precision, recall, f1, ndcg, mrr, map_score):\n",
        "    performance_data = {\n",
        "        'model_name': model_name, 'mae': mae, 'rmse': rmse,\n",
        "        'precision': precision, 'recall': recall, 'f1': f1,\n",
        "        'ndcg': ndcg, 'mrr': mrr, 'map': map_score\n",
        "    }\n",
        "    file_path = '/content/drive/MyDrive/models/model_performance.json'\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        data = []\n",
        "\n",
        "    data.append(performance_data)\n",
        "\n",
        "    # Write back to JSON\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N694tBFeIxVZ"
      },
      "outputs": [],
      "source": [
        "save_performance_data('first_cf_model', mae, rmse, precision, recall, f1, ndcg, mrr, map_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTNOdCkVepxb"
      },
      "source": [
        "### Adding bias\n",
        "This is done since there are users who rate more positive or negative than others, and there are some books that are plain better or worse than others. We will create number for each user that we can add to our socres and ditto for each book, to handle that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PjdVGTKuY3c"
      },
      "outputs": [],
      "source": [
        "n_factors = 5\n",
        "n_users = df_filtered['reviewerID'].nunique()\n",
        "n_books = df_filtered['asin'].nunique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We read that for a 1-5 rating range (and 0 as \"not rated\") it is recommended to define y_range as 0-5.5 because it turns out that it has better accuracy."
      ],
      "metadata": {
        "id": "sMGfUulSBhQL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5Z7Z-6eTGH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CollaborativeFilteringModel(nn.Module):\n",
        "    def __init__(self, n_users, n_books, n_factors=5, y_range=(0, 5.5)):\n",
        "        super(CollaborativeFilteringModel, self).__init__()\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.user_bias = nn.Embedding(n_users, 1)\n",
        "        self.book_factors = nn.Embedding(n_books, n_factors)\n",
        "        self.book_bias = nn.Embedding(n_books, 1)\n",
        "        self.y_range = y_range\n",
        "\n",
        "    def forward(self, user_idx, book_idx):\n",
        "        user_embed = self.user_factors(user_idx)\n",
        "        book_embed = self.book_factors(book_idx)\n",
        "        user_bias = self.user_bias(user_idx)\n",
        "        book_bias = self.book_bias(book_idx)\n",
        "        res = (user_embed * book_embed).sum(dim=1, keepdim=True)\n",
        "        res += user_bias + book_bias\n",
        "        predicted_rating = torch.sigmoid(res)\n",
        "        return predicted_rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n",
        "\n",
        "model = CollaborativeFilteringModel(n_users,n_books, n_factors).to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu6gc5-ffKxO"
      },
      "outputs": [],
      "source": [
        "user_idx_tensor = torch.LongTensor(train_df['user_idx'].values).to(device)\n",
        "book_idx_tensor = torch.LongTensor(train_df['book_idx'].values).to(device)\n",
        "ratings_tensor = torch.FloatTensor(train_df['overall'].values).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(user_idx_tensor, book_idx_tensor, ratings_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6KUzlJ2Ydb3",
        "outputId": "2f8f23bf-733e-489f-b6d9-41eb2a598d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1024])) that is different to the input size (torch.Size([1024, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([764])) that is different to the input size (torch.Size([764, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 5.7856\n",
            "Epoch 2/20, Average Loss: 3.9938\n",
            "Epoch 3/20, Average Loss: 2.8274\n",
            "Epoch 4/20, Average Loss: 2.1661\n",
            "Epoch 5/20, Average Loss: 1.7927\n",
            "Epoch 6/20, Average Loss: 1.5679\n",
            "Epoch 7/20, Average Loss: 1.4239\n",
            "Epoch 8/20, Average Loss: 1.3254\n",
            "Epoch 9/20, Average Loss: 1.2557\n",
            "Epoch 10/20, Average Loss: 1.2047\n",
            "Epoch 11/20, Average Loss: 1.1670\n",
            "Epoch 12/20, Average Loss: 1.1376\n",
            "Epoch 13/20, Average Loss: 1.1154\n",
            "Epoch 14/20, Average Loss: 1.0983\n",
            "Epoch 15/20, Average Loss: 1.0846\n",
            "Epoch 16/20, Average Loss: 1.0736\n",
            "Epoch 17/20, Average Loss: 1.0652\n",
            "Epoch 18/20, Average Loss: 1.0578\n",
            "Epoch 19/20, Average Loss: 1.0522\n",
            "Epoch 20/20, Average Loss: 1.0474\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_idx, book_idx, ratings in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred_ratings = model(user_idx, book_idx)\n",
        "        loss = criterion(pred_ratings, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/models/fc_with_bias_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqPsOVTQYkaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d900f3f-1e46-4d5d-c137-cf754b7c9c4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "NDCG: 0.9815\n",
            "MAE: 0.9407\n",
            "MRR: 0.0004\n",
            "Precision: 0.1694\n",
            "Recall: 0.1665\n",
            "F1-score: 0.1386\n",
            "RMSE: 1.1043\n",
            "MAP: 0.2028\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "user_idx_tensor_test = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
        "book_idx_tensor_test = torch.LongTensor(test_df['book_idx'].values).to(device)\n",
        "model.eval()\n",
        "pred_ratings_test = model(user_idx_tensor_test, book_idx_tensor_test).detach().cpu().numpy()\n",
        "true_ratings = test_df['overall'].values\n",
        "true_ratings, pred_ratings_test =  true_ratings.reshape(-1), pred_ratings_test.reshape(-1)\n",
        "\n",
        "ndcg = ndcg_score([true_ratings], [pred_ratings_test])\n",
        "\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings_test)\n",
        "\n",
        "mrr = calculate_mrr(true_ratings, pred_ratings_test)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_ratings, pred_ratings_test.round(), average='macro')\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings_test, squared=False))\n",
        "\n",
        "map_score = calculate_map(true_ratings, pred_ratings_test)\n",
        "print(f\"Metrics:\\nNDCG: {ndcg:.4f}\\nMAE: {mae:.4f}\\nMRR: {mrr:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\\nRMSE: {rmse:.4f}\\nMAP: {map_score:.4f}\")\n",
        "save_performance_data('fc_with_bias_model', mae, rmse, precision, recall, f1, ndcg, mrr, map_score)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AQvvSCc3e5m"
      },
      "source": [
        "### Regularization:\n",
        "In order to prevent overfitting, we would now create another model that uses regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqylO2J6iKOA"
      },
      "outputs": [],
      "source": [
        "model_with_regularization = CollaborativeFilteringModel(n_users,n_books, n_factors).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model_with_regularization.parameters(), lr=0.01, weight_decay=0.01)\n",
        "# Added L2 regularization with weight_decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_idx_tensor = torch.LongTensor(train_df['user_idx'].values).to(device)\n",
        "book_idx_tensor = torch.LongTensor(train_df['book_idx'].values).to(device)\n",
        "ratings_tensor = torch.FloatTensor(train_df['overall'].values).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(user_idx_tensor, book_idx_tensor, ratings_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
      ],
      "metadata": {
        "id": "jbhgV6r9vlBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model_with_regularization.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_idx, book_idx, ratings in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred_ratings = model_with_regularization(user_idx, book_idx)\n",
        "        loss = criterion(pred_ratings, ratings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "torch.save(model_with_regularization.state_dict(), '/content/drive/MyDrive/models/fc_with_bias_and_regularization_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZWim2LNvlfD",
        "outputId": "46fdc895-7242-41e0-eb13-09d8c3cf4081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 2.7491\n",
            "Epoch 2/20, Average Loss: 2.3193\n",
            "Epoch 3/20, Average Loss: 2.2246\n",
            "Epoch 4/20, Average Loss: 2.2026\n",
            "Epoch 5/20, Average Loss: 2.1964\n",
            "Epoch 6/20, Average Loss: 2.1965\n",
            "Epoch 7/20, Average Loss: 2.1956\n",
            "Epoch 8/20, Average Loss: 2.1965\n",
            "Epoch 9/20, Average Loss: 2.1957\n",
            "Epoch 10/20, Average Loss: 2.1969\n",
            "Epoch 11/20, Average Loss: 2.1955\n",
            "Epoch 12/20, Average Loss: 2.1961\n",
            "Epoch 13/20, Average Loss: 2.1955\n",
            "Epoch 14/20, Average Loss: 2.1963\n",
            "Epoch 15/20, Average Loss: 2.1969\n",
            "Epoch 16/20, Average Loss: 2.1967\n",
            "Epoch 17/20, Average Loss: 2.1969\n",
            "Epoch 18/20, Average Loss: 2.1971\n",
            "Epoch 19/20, Average Loss: 2.1960\n",
            "Epoch 20/20, Average Loss: 2.1968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "user_idx_tensor_test = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
        "book_idx_tensor_test = torch.LongTensor(test_df['book_idx'].values).to(device)\n",
        "model.eval()\n",
        "pred_ratings_test = model(user_idx_tensor_test, book_idx_tensor_test).detach().cpu().numpy()\n",
        "true_ratings = test_df['overall'].values\n",
        "true_ratings, pred_ratings_test =  true_ratings.reshape(-1), pred_ratings_test.reshape(-1)\n",
        "\n",
        "ndcg = ndcg_score([true_ratings], [pred_ratings_test])\n",
        "\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings_test)\n",
        "\n",
        "mrr = calculate_mrr(true_ratings, pred_ratings_test)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_ratings, pred_ratings_test.round(), average='macro')\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings_test, squared=False))\n",
        "\n",
        "map_score = calculate_map(true_ratings, pred_ratings_test)\n",
        "print(f\"Metrics:\\nNDCG: {ndcg:.4f}\\nMAE: {mae:.4f}\\nMRR: {mrr:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\\nRMSE: {rmse:.4f}\\nMAP: {map_score:.4f}\")\n",
        "save_performance_data('fc_with_bias_and_regularization_model', mae, rmse, precision, recall, f1, ndcg, mrr, map_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdZC-Zu3v_Hb",
        "outputId": "455ea2d0-db54-403e-cf60-d56a9e41bbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "NDCG: 0.9815\n",
            "MAE: 0.9407\n",
            "MRR: 0.0004\n",
            "Precision: 0.1694\n",
            "Recall: 0.1665\n",
            "F1-score: 0.1386\n",
            "RMSE: 1.1043\n",
            "MAP: 0.2028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can get recommendation based on our models so far (future work will include to change the recommended books numbers to actually boooks):"
      ],
      "metadata": {
        "id": "z7PB0BKYAaeB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWqT0A2RDXci",
        "outputId": "6df24a03-0fae-4763-cc7b-1156381beaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommendations for user 1658: [ 4615  9086 24907 20836 23997  3847 30258 24074 11187  4097]\n",
            "Top recommendations for user 996: [22124 21134 24701 18960   640 23952 25982 19187  7150  6347]\n",
            "Top recommendations for user 1684: [24907 17350 11268 22215  5486 19665  2003  6885  4615  3479]\n",
            "Top recommendations for user 1640: [17350 11062 29434  4615 28162  6347  3479 28028 24701 18960]\n",
            "Top recommendations for user 360: [11062 17350 17524  2245 21134  5486 10894 24701 15068 22215]\n",
            "Top recommendations for user 1414: [18960 17350  6347   396 22124  4615 24074 28028 24701 24220]\n",
            "Top recommendations for user 1315: [14504  6885  3710 10807 15068 11062  9392  8975 24721 24809]\n",
            "Top recommendations for user 1665: [15068  6885  7031  8198 18724 17350 24907 23952 24161 11062]\n",
            "Top recommendations for user 1276: [28028 15068 17350 23952 21134 25982  8198 24160 24701  6347]\n",
            "Top recommendations for user 78: [ 8198 28028 13881 23997 24243 24160  6347 17350  4097 20931]\n",
            "Top recommendations for user 1077: [18960   396 17350 24701 28028 25334  6347  4615 24074 24745]\n",
            "Top recommendations for user 292: [17350 11062 15068  8198  6347  4615 29434  3479  7031 24701]\n",
            "Top recommendations for user 1190: [11062  4615 29434 28162 17350  3479 18960 24701  6347 25334]\n",
            "Top recommendations for user 1144: [17350  6347  4615 24907  7031 23997 20836  8198 11062 28028]\n",
            "Top recommendations for user 1311: [17350 28028 11062  6347 24701  8198 25334 15068 21134 25982]\n",
            "Top recommendations for user 1408: [11062 24701 18960 25334 29434 16706  4615 15068 17350  6347]\n",
            "Top recommendations for user 1143: [17350 11062 29434 24701 15068 17524 28028  8198 25334  6347]\n",
            "Top recommendations for user 373: [ 8198  1451 13881 15068 20487  4097 24160 28028 20985 28213]\n",
            "Top recommendations for user 1004: [15068 11062  8198 17350 24701  2245 23952 21134  6347 29434]\n",
            "Top recommendations for user 589: [ 4723   396   380 24745 27345 22755  2805 16019 18960 29907]\n",
            "Top recommendations for user 154: [ 4615 17350 11062  3479 28162 29434  3847 20836  6347  1277]\n",
            "Top recommendations for user 964: [ 8198 15068 11062 17350 29434 24701 25334 25982  4385  6347]\n",
            "Top recommendations for user 1117: [ 4615 28162  3847  7205  6492 27312  6424  8024  7453 24074]\n",
            "Top recommendations for user 82: [11062 29434 28162  4615  3479 22215 17350  4468  1277 10894]\n",
            "Top recommendations for user 566: [ 4615  3847 28162  3479  9086  6885  1277 29434 24074 11062]\n",
            "Top recommendations for user 1096: [28028 17350  8198   396 25982 24701 24160  6347 25334 24745]\n",
            "Top recommendations for user 725: [21134 17350 22124 18960 24701  6347 17524   640  3404 24745]\n",
            "Top recommendations for user 546: [ 4615 28162 24074  4694  6492  3847 29434  4097 11062  7988]\n",
            "Top recommendations for user 68: [ 6885 19682 24907  7031 25401 27709   640 10199  1380 22124]\n",
            "Top recommendations for user 393: [24074  9886  6889  4097    45 29120 25468 12533 11597 24240]\n",
            "Top recommendations for user 306: [17350 28028 18960 24701   396 22124  6347 21134 24745 25982]\n",
            "Top recommendations for user 1286: [11062 15068  8198 29434 17350 15203  2245 24701 25334 13055]\n",
            "Top recommendations for user 728: [28162  4615 29434 11062  6492 22935 24074  1449 18960 20555]\n",
            "Top recommendations for user 1498: [17350  8198 28028  6347 11062 15068 24701 25982  4615 23997]\n",
            "Top recommendations for user 325: [17350 24907  7031  6347  2003 11062  4615  3479 22215  6885]\n",
            "Top recommendations for user 1737: [11062  8975 15068 21058  1449 23662 29434 24701 13055  9392]\n",
            "Top recommendations for user 1522: [11062 29434  1449 28162  8198 25334 20555 24701 27931  4385]\n",
            "Top recommendations for user 831: [17350 24907  4615  6347 20836  7031 23997 24220  2003  9086]\n",
            "Top recommendations for user 1279: [ 4615 28162 29434 17350  3479 11062 20836  3847  2553 18960]\n",
            "Top recommendations for user 1463: [24161  4651 20140 24907 23952  5238 19680 13881  2842 13685]\n",
            "Top recommendations for user 817: [23740 24907  4651 25765  5864 11268 16053 27697  4416  3631]\n",
            "Top recommendations for user 1300: [15068 11062  2245  8975 23952 14504 21134 17350 18332  8198]\n",
            "Top recommendations for user 953: [ 4615 24907 17350 20836  6347  9086  2003  5472 10380 24220]\n",
            "Top recommendations for user 593: [ 6903 19665 18332 28987 20527 26638 12128 29288 15925 19691]\n",
            "Top recommendations for user 1572: [ 6885 21134  3404 18960 22124 24621 17350  6347   640 17524]\n",
            "Top recommendations for user 580: [ 6885  4615 11062  3847  3479 14504  7988 10894 25401  4694]\n",
            "Top recommendations for user 1308: [17350 28028 24701  6347 21134 18960 25982 25334   396 24745]\n",
            "Top recommendations for user 462: [ 8198 28028 24160 15068 13881 24161 23952 20985 25982 23997]\n",
            "Top recommendations for user 1099: [28028 17350 15068  7031  8198 24907 23952  6347 20985 21134]\n",
            "Top recommendations for user 1159: [15068 11062  8198 17350 29434  6885  4615  3479  4385  6347]\n",
            "Top recommendations for user 1328: [ 4097 19680 13881  9886  8198 14397 24074 25468 20487  7921]\n",
            "Top recommendations for user 1110: [17350 28028 10380  6903 11062 29434  1059  7316 23825  1634]\n",
            "Top recommendations for user 1579: [17350 28028  6347 11062 24701  8198 21134 15068 25982 25334]\n",
            "Top recommendations for user 1061: [24907  6885  4615  7031  2627 30258 11268 22215 17350 20836]\n",
            "Top recommendations for user 1764: [17350 17524  5486 21134  2003 22544  3479  2245 11062 10894]\n",
            "Top recommendations for user 347: [ 8198 15068 11062  4615 29434  4097  6347 17350 25334 24701]\n",
            "Top recommendations for user 586: [28028 24243 26638 22124 25982 23952 24160 21134   396   640]\n",
            "Top recommendations for user 1053: [28162  4615 29434 11062  2126  2553  3847  3479  1277 23825]\n",
            "Top recommendations for user 1638: [ 4615 29434 11062  8198 28162 20931 17350  4097  3479 24074]\n",
            "Top recommendations for user 756: [ 4615 11062 28162 29434  3479  3847 17350  1277 24074  6492]\n",
            "Top recommendations for user 1397: [24907  4615  9086 20836 11533 11268  2126 23997  1820  6347]\n",
            "Top recommendations for user 361: [15068 11062 17350 24701  8198 21134 25334  2245 25982 17524]\n",
            "Top recommendations for user 346: [ 4615 17350  6347 23997  8198 20836 11062 24907 29434 20931]\n",
            "Top recommendations for user 94: [17350 15068  7031 11062 28028  8198  7316 24907  6903  2003]\n",
            "Top recommendations for user 811: [24074  6492 24240  2339  4694  9886 25568 19713  7057 18581]\n",
            "Top recommendations for user 877: [17350 28028  4615 20836   396  6347 29434 11062  2003 28162]\n",
            "Top recommendations for user 780: [28028 13881 24243 20985  9536   396 24160 10931  5238 26638]\n",
            "Top recommendations for user 59: [24701 25334  8198 18960 29434 11062 25982 16706 24074 24243]\n",
            "Top recommendations for user 619: [11062  4615 29434  8198 15068  4097 28162 24074 16706  4385]\n",
            "Top recommendations for user 1211: [21134 24701 17524 17350 18960  3404 22124 24745  2245  6347]\n",
            "Top recommendations for user 576: [17350 11062 17524  4615 18960 24745 24701  3479 28162 29434]\n",
            "Top recommendations for user 112: [24243 23616 13881 10931 24074   396  8198 28028    45 20931]\n",
            "Top recommendations for user 1127: [15068  8198 14504  6885 18724 20487 23952 21806 23820 27968]\n",
            "Top recommendations for user 864: [11062 15068 29434  8198 22215 17350  2245 10894  3479  6885]\n",
            "Top recommendations for user 31: [11062 29434 15068 24701 17350  2245 17524 25334  8975 10894]\n",
            "Top recommendations for user 234: [ 4615 11062 29434 17350 28162  3479 20836  1277 22215  6347]\n",
            "Top recommendations for user 138: [17350 15068 11062  8198  6347 24701 29434 28028 25982  7031]\n",
            "Top recommendations for user 1241: [18960 22124  6347 17350 24243   396 28028 24701 24220 21134]\n",
            "Top recommendations for user 249: [24701 18960 22124 21134 16706  3404 29178 25334  7920  7150]\n",
            "Top recommendations for user 323: [ 7031 24907 17350 23952  6885 15068 19665 21134 24161  6347]\n",
            "Top recommendations for user 319: [11062  6885 14504 24721 22215  9392 10807 21058  2245  2476]\n",
            "Top recommendations for user 1357: [28028 17350  6903 21134 26638  6347 23952 22124 25982  7031]\n",
            "Top recommendations for user 228: [10380  2126  8663 17954  8696 27871 20466 15303 23825 11029]\n",
            "Top recommendations for user 17: [11062  8975 15068 21058 23662  1449  2245 24701  9392 29434]\n",
            "Top recommendations for user 715: [ 4615 29434  4097 28162  3847  4694 11062 20487  8198 19713]\n",
            "Top recommendations for user 1656: [15068  6885 10807 11062  7988 20487 14504  8198  4385  5452]\n",
            "Top recommendations for user 815: [ 4615 28162 29434 11062  3479 17350  3847  1277 20931 20836]\n",
            "Top recommendations for user 406: [17350 28028  6347  8198 24701 25982 21134 11062   396 25334]\n",
            "Top recommendations for user 1116: [17350  4615 24907  6347 20836  3479  2003  7031 18960  9086]\n",
            "Top recommendations for user 1121: [ 8198 15068 11062 17350 29434 20487  3321 22215 15203 10894]\n",
            "Top recommendations for user 1047: [17350 21134 28028  6347 15068 24701 23952 25982  7031  2003]\n",
            "Top recommendations for user 1457: [ 6885 15068 23952  7031 18724 24161 20140 24907  4651   640]\n",
            "Top recommendations for user 1866: [29434 28162  4615 11062 20931 24074  1449  8198  4723 23825]\n",
            "Top recommendations for user 1236: [17350 28028 11062  8198 29434  6347  2003 20836  7316  1059]\n",
            "Top recommendations for user 1246: [17350 28028  6347 25982 24701 21134  8198 15068 24160 18960]\n",
            "Top recommendations for user 1079: [11062 17350 29434 24701 17524 28162  4615  3479 18960 24745]\n",
            "Top recommendations for user 805: [11062  6885 15068 14504  2245 10894 10807 29434  3479  4615]\n",
            "Top recommendations for user 603: [11062 17350 29434 24701 28162 24745 17524 25334 18960  3479]\n",
            "Top recommendations for user 1528: [17350 11062  4615  6347  3479 15068 29434  8198  7031  2003]\n",
            "Top recommendations for user 1331: [17350 11062  4615  3479 22215  2003 10894  6885 29434  5486]\n",
            "Top recommendations for user 281: [24701 25334 16706  8198 25982 15068 27931 24243 18960  4572]\n",
            "Top recommendations for user 496: [28028  8198 26638 15203 17350 25982 24701 25334 24160   396]\n",
            "Top recommendations for user 1338: [11062 29434  4615 17350 24701 28162  3479 25334  2245 18960]\n",
            "Top recommendations for user 382: [27931 24701  4572 25334 27801 18960 12300 17968  2128 13546]\n",
            "Top recommendations for user 1347: [17350  8198 28028 11062 15068  6347 25982 24701 25334 24160]\n",
            "Top recommendations for user 1303: [11062  6903 15068 17350  8198 12128 15203 26638  7316 28028]\n",
            "Top recommendations for user 164: [17350 11062 22215 24907  7316  2003 20731 20836  8198  3479]\n",
            "Top recommendations for user 662: [28028 26638 24243  1826  5238 23952 24160 20985 25362 29342]\n",
            "Top recommendations for user 293: [15068 11062 17350  8198  6347  7031  6885  2245 21134 29434]\n",
            "Top recommendations for user 1229: [17350  8198 28028 11062 15068  6347 24701 25982 25334 24160]\n",
            "Top recommendations for user 894: [11062 15068  8198 17350 29434 24701  6347 25334  4615  3479]\n",
            "Top recommendations for user 1218: [11062  4615 17350 15068 29434  3479  6347  8198  6885 28162]\n",
            "Top recommendations for user 384: [17350 21134 24701 17524 11062  2245 18960  3404  6347 24745]\n",
            "Top recommendations for user 1769: [ 8198 28213 15203 28028  2319  1451 27931 24160 15068 11396]\n",
            "Top recommendations for user 1376: [10710 24907 17504  3631  7510 22124 11268  3477 30175  1380]\n",
            "Top recommendations for user 152: [11062 17350  3479 29434  2245 17524 10894  4615  5486 22215]\n",
            "Top recommendations for user 1604: [17350 28028 11062 29434  8198  6347  4615   396 24701 25334]\n",
            "Top recommendations for user 316: [15068  8198 23952  7031 17350  6885 18724 24161 11062  3321]\n",
            "Top recommendations for user 1115: [17350 11062 15068  8198 24701 28028  6347 25334 21134 25982]\n",
            "Top recommendations for user 935: [21134 22124   640 24701 23952  3404 18960  7150 17350 19187]\n",
            "Top recommendations for user 1154: [11062 29434  4615 28162 22215  3479 17350  8198  1277  4468]\n",
            "Top recommendations for user 1555: [ 8198 28028 29434 17350 11062 20931 23825  1451 23997 28213]\n",
            "Top recommendations for user 1517: [17350 11062 17524  2245  3479 10894 21134  6885  5486  2003]\n",
            "Top recommendations for user 810: [ 6885 14504 24621 25401 10894 13352  5486  2245 22215 24721]\n",
            "Top recommendations for user 1418: [11062  4615 17350 29434  3479 15068 28162  8198  6347 10894]\n",
            "Top recommendations for user 901: [28028 17350  8198   396 20985  6347  1059 24160 23997 13881]\n",
            "Top recommendations for user 544: [15068 11062  8198 17350 24701  2245 15203 29434 21134 25334]\n",
            "Top recommendations for user 1807: [28028 24701 25982  8198 17350 24160 25334 15068 21134 24243]\n",
            "Top recommendations for user 51: [28162  4615 29434  6492 24074  3847 22935 15504 28679 11062]\n",
            "Top recommendations for user 1135: [ 8198 15203 15068 11062 28213 28028 25334 24701 27931  2319]\n",
            "Top recommendations for user 1299: [29434 17350  4723 28162 11062 11396 23825 24745  1634   396]\n",
            "Top recommendations for user 504: [ 6885 11062 15068 17350  4615  3479  2245 10894 14504  7031]\n",
            "Top recommendations for user 974: [ 8975  7890  6903 27569 11062 17524 28608 26638  9783 21058]\n",
            "Top recommendations for user 363: [ 6885  4615  3847  3479  3477 25401 22215 11268 10894  5486]\n",
            "Top recommendations for user 101: [17350 28028 24907  7031  6347  2003  7316  6903 15068 11062]\n",
            "Top recommendations for user 24: [17350 21134 28028 24701  6347 18960 22124 25982   396 24745]\n",
            "Top recommendations for user 1106: [ 8198 15068 11062  4097 20487 29434 20931  4385  4615  1451]\n",
            "Top recommendations for user 172: [28162 29434 11062  4615  1449  6492 22935 18960  3479 20555]\n",
            "Top recommendations for user 542: [11062 17350 24701 21134 17524 25334 15068 24745  2245 18960]\n",
            "Top recommendations for user 465: [ 8198 11062 15068 17350 29434  4615 20931 28028  6347 23997]\n",
            "Top recommendations for user 196: [15068  6347 23952 17350 21134 24701  7031 25982 24161   640]\n",
            "Top recommendations for user 104: [17350 10380  6903 28028 17524   396  2003 24745 12858  1634]\n",
            "Top recommendations for user 118: [24243 28028   396 23616 10931 24160 25982 18960 22124 13881]\n",
            "Top recommendations for user 214: [17350  6885  7031  6347 21134 15068  4615 11062  2003  3479]\n",
            "Top recommendations for user 240: [17350  6347 18960 22124  2003 21134 24907   396 17524  4615]\n",
            "Top recommendations for user 1854: [15068  6885 14504 11062  2245 23952 21134  4385  3404 16706]\n",
            "Top recommendations for user 1197: [17350 21134  6347  7031 28028 23952 15068 24701 25982   640]\n",
            "Top recommendations for user 972: [17350 24701 28028 11062  6347 25334 18960 21134 25982 24745]\n",
            "Top recommendations for user 839: [17350 11062 15068 21134  2245 24701 17524  6347 10894  2003]\n",
            "Top recommendations for user 1294: [17350  4615  6347 18960 11062  3479 20836   396  2003 29434]\n",
            "Top recommendations for user 123: [15068 23952  7031 21134 17350  6885 24161  6347  2245   640]\n",
            "Top recommendations for user 1409: [17350 11062 15068  6885  2245 21134 10894 17524  7031 14504]\n",
            "Top recommendations for user 1256: [11062 24701 17350 25334 15068 29434 17524  2245 21134 24745]\n",
            "Top recommendations for user 1306: [ 7031 23952 21134 17350 15068  6885   640  6347 24161 24907]\n",
            "Top recommendations for user 1097: [17350 11062  7031  2003  6347  4615  3479 24907 22215 21134]\n",
            "Top recommendations for user 35: [24907 19665 11533 17350 28028  7031  6903 10380 13685 20527]\n",
            "Top recommendations for user 826: [11396 17328  4723  2805 21671 29434 23825 19700 30119 23807]\n",
            "Top recommendations for user 1216: [17350 28028 11062   396 24701 29434 25334  6347 24745 18960]\n",
            "Top recommendations for user 1214: [24907  4615 11268  3631  6885  7031  3477  9086  3847 20836]\n",
            "Top recommendations for user 1363: [  396 18960 17350  4615 29434 28028 25334 24701 24074  4723]\n",
            "Top recommendations for user 1327: [11062  8198 17350 29434 15068 28028 15203 25334 24701 23825]\n",
            "Top recommendations for user 1194: [29434  4615 11062 28162  3479  1277  3847  2126  8198 20931]\n",
            "Top recommendations for user 997: [ 8975 21058  9392 14504  7156 11062  3710 24809  9783 15068]\n",
            "Top recommendations for user 457: [ 6885 23952 15068 21134 29178 16706   640  3404  6347 22124]\n",
            "Top recommendations for user 200: [ 4615 11062 29434 28162  3479 17350  3847  1277 22215  6885]\n",
            "Top recommendations for user 698: [  396 23616 10931 24074 24243 18960  2126  4723  3666 10495]\n",
            "Top recommendations for user 840: [28028 17350 21134  6347 24701   396 22124 18960  6903 25982]\n",
            "Top recommendations for user 1492: [15068 11062  8198 17350 24701 29434 15203 25334  2245  4385]\n",
            "Top recommendations for user 1790: [22124 28368 18960   640 21134 19187  7920  9774  6347   396]\n",
            "Top recommendations for user 1139: [11062 24701 17350 29434 25334 18960 17524 24745 21134 15068]\n",
            "Top recommendations for user 1714: [24243 23616 22124 10931   396  5238 18960 28028 24074 10495]\n",
            "Top recommendations for user 522: [11062  4615 29434 17350 28162  3479  8198 20836  1277 20931]\n",
            "Top recommendations for user 340: [17350 11062 29434  8198 15068 28028 24701 17524  3479 25334]\n",
            "Top recommendations for user 1272: [11062 29434 28162 24701  1449 15068 25334 17350  4615  8198]\n",
            "Top recommendations for user 3: [ 8198  4097  1451 13881 20931 29434 24243 24074    45 12533]\n",
            "Top recommendations for user 467: [17350 11062 21134  6347 24701 15068 17524  2003  3479 25334]\n",
            "Top recommendations for user 174: [24701 18960 11062 21134 25334 17350 17524 24745  3404  2128]\n",
            "Top recommendations for user 592: [24907 17350  7031 28028 11533 20836 19665 10380  2003 13685]\n",
            "Top recommendations for user 1454: [17350 28028 24907  7031  6347  6903 21134  2003 19665 23952]\n",
            "Top recommendations for user 910: [ 8975 11062 21058  9392  1449 23662  7156 27931 17571 13055]\n",
            "Top recommendations for user 1459: [ 7031 24907  6347 17350 24161 23952   640 21134 15068 24220]\n",
            "Top recommendations for user 404: [17350 24701 17524 21134 28028 24745  6347 18960  6903 11062]\n",
            "Top recommendations for user 843: [15068 17350 24701 11062  8198 28028 25334 25982 21134 15203]\n",
            "Top recommendations for user 1090: [17350  6347 21134 24701  7031 18960 28028 15068 25982 11062]\n",
            "Top recommendations for user 1506: [11062  4615 29434 15068  6885 28162  3479  8198  3847 10807]\n",
            "Top recommendations for user 719: [ 8198 28028 15068 15203 17350 11062 28213 24160  2319 25982]\n",
            "Top recommendations for user 378: [28162  2805 10380  4615  2553 17350 29434 24835 11062  3479]\n",
            "Top recommendations for user 1369: [24907  7031 19665 17350 21134  6347  2003   640 23952 13685]\n",
            "Top recommendations for user 98: [15068 17350  6885  7031 11062  8198 22215 24907 18724 23952]\n",
            "Top recommendations for user 358: [17350 28028 24907  6347 20836   396  2003  7031 11533 24220]\n",
            "Top recommendations for user 988: [ 8198 15068 17350 11062  6347 29434 28028 24160 23997 25982]\n",
            "Top recommendations for user 701: [17350 24701 18960 21134 17524  6347 24745 11062 25334  3404]\n",
            "Top recommendations for user 481: [24074  4615  6492 29434 28162 18960  4097 16706 11062  4694]\n",
            "Top recommendations for user 785: [ 4615 28162 24074 29434  6492 18960 22755 22935 15937  4723]\n",
            "Top recommendations for user 526: [ 8198 15068  4097 23997 20487 13881 24160 20931  1451 11062]\n",
            "Top recommendations for user 1704: [11062 29434  8198 17350 15068 24701 25334 28162  4615 15203]\n",
            "Top recommendations for user 317: [ 8198 28213  1451 15068 20487 13055 29434 15203 20043  4368]\n",
            "Top recommendations for user 1296: [17350 24701 11062 17524 24745 18960 21134 25334  6347 29434]\n",
            "Top recommendations for user 482: [17350 21134 18960 22124  6347 17524 24701 24745  2003   396]\n",
            "Top recommendations for user 1785: [15068  8198 23952 17350  7031 24161 11062 21134 28028  6347]\n",
            "Top recommendations for user 649: [29434 17350  4615  2126 28162 11062 23825 10380 20836 28028]\n",
            "Top recommendations for user 1513: [17350 24701 17524 18960 24745 21134 11062  6347 25334  7890]\n",
            "Top recommendations for user 885: [15068 11062  8198 17350 24701 25334 25982 29434  4385 21134]\n",
            "Top recommendations for user 16: [ 3710  8975 14504 15068 24809  9392 10807  6885 21181 11062]\n",
            "Top recommendations for user 367: [ 4615 28162  6492 24074 18960 22935 22755 29434 15937  7987]\n",
            "Top recommendations for user 1754: [17350 28028  6903 10380  1059  7316 11062  2003  8198 20836]\n",
            "Top recommendations for user 262: [15068  8198 11062 13055 29434 15203  4385 20487  1449 10807]\n",
            "Top recommendations for user 1340: [17350 18960  6347 24701 21134 25334  4615 11062 16706 22124]\n",
            "Top recommendations for user 1718: [17350 24701 15068 11062  6347 21134 18960 25334 16706 25982]\n",
            "Top recommendations for user 142: [11062  8198 29434 15068 17350 22215 23825  3479 28162  7316]\n",
            "Top recommendations for user 584: [11062 17350 15068 24701  2245 21134 17524 10894 25334 29434]\n",
            "Top recommendations for user 1847: [15068 24701 21134 23952 11062 25334  2245 25982 17350  7150]\n",
            "Top recommendations for user 991: [ 6903 28028 26638 24907 17350  1059 18739 10380  9536 20985]\n",
            "Top recommendations for user 705: [ 8198 15068 11062 20487 29434  1451  4097 28213 15203 13055]\n",
            "Top recommendations for user 1352: [17350 28028 29434   396  4723 23825 11062 28162  4615  8198]\n",
            "Top recommendations for user 321: [15068  6885 11062 14504  7988 10807  4385  2245  9392 29434]\n",
            "Top recommendations for user 248: [28028  8198 13881  1451  9536 26638 25377 18655 28213 11549]\n",
            "Top recommendations for user 1088: [17350 28028 21134  7031  6347  6903 23952 24907  2003   640]\n",
            "Top recommendations for user 1643: [17350 21134  6903  7031 17524  2003 19665  7316  5486 22215]\n",
            "Top recommendations for user 803: [17350 21134 24701  6347 18960 22124 17524  7031  3404 23952]\n",
            "Top recommendations for user 1663: [28028 17350  6347 21134 25982 24160 23952  8198  7031  6903]\n",
            "Top recommendations for user 693: [17350  6903 28028 21134 17524  6347  2003 18960 24701 24745]\n",
            "Top recommendations for user 812: [11062 28162  4615 29434  3479 10894  2245 21058  5486  6885]\n",
            "Top recommendations for user 93: [ 6885 19682 24907 25401 27709  4651  3631  7031 10199 25765]\n",
            "Top recommendations for user 314: [19682 22124  7920 29178  3404 18960  6885   640 20579 21134]\n",
            "Top recommendations for user 1209: [15068  8198 20487 11062  6885 18724 23952  4385  4097 24161]\n",
            "Top recommendations for user 1424: [15068 11062  6885 17350  8198 23952  6347 24701  4385 16706]\n",
            "Top recommendations for user 501: [15068 11062  2245 17350 21134 14504  6885 10894  8975 17524]\n",
            "Top recommendations for user 495: [11062 29434  4615  8198 28162 15068 25334 24701 20931 20555]\n",
            "Top recommendations for user 1333: [11062  6885  4615  3479 22215 17350 10894  3847 29434 28162]\n",
            "Top recommendations for user 963: [11062 15068 17350  8198  6885 29434 10894  2245 22215  3479]\n",
            "Top recommendations for user 513: [28028 26638  8198 20985  1826 20722 24160 15068  6903 23952]\n",
            "Top recommendations for user 107: [ 4615 29434 11062  8198 28162 20931  2126  4097  3479  1277]\n",
            "Top recommendations for user 193: [24907  6903 28028 17350  7031 19665 26638  7316 20985 26398]\n",
            "Top recommendations for user 20: [11062 15068  4385  2245  6885 24701  8975 14504 10807  7988]\n",
            "Top recommendations for user 932: [17350  6903 28028 11062 17524 21134  7316 15068  2003 24701]\n",
            "Top recommendations for user 644: [15068 17350  8198 11062 23952 21134  7031  6347 28028 25982]\n",
            "Top recommendations for user 637: [17350 15068  7031  6347 24907 11062  8198 21134  2003 24161]\n",
            "Top recommendations for user 57: [17350 24701 11062 17524 24745 25334 21134 28028 18960  6347]\n",
            "Top recommendations for user 26: [ 8198 13881  4097 24160  1451 24243 15068 20487 23997 28028]\n",
            "Top recommendations for user 1582: [11062 29434 10807  4468  8198  1449 26529 28162 15068  3710]\n",
            "Top recommendations for user 313: [24701   396 18960 24745 25334  4723 28028 29907 27931 24243]\n",
            "Top recommendations for user 670: [17350 11062 28028  2003  6347 17524 29434  7316  3479  6903]\n",
            "Top recommendations for user 829: [17350 28028  6903  1059 24907  2003 10380  7316  6347 20836]\n",
            "Top recommendations for user 160: [17350  6347 28028 18960 23997 25982 24220 24701 22124   396]\n",
            "Top recommendations for user 1401: [17350 15068 11062 21134  7031  7316  2003  6903  2245 17524]\n",
            "Top recommendations for user 186: [28028   396 24243 17350 24160  8198 25982 23616 24745 13881]\n",
            "Top recommendations for user 1786: [ 8198 13881 24243  1451    45 28028 24160 23616 10931  4097]\n",
            "Top recommendations for user 999: [28028 21134 17350  6903 26638 23952 25982 24701  6347 15068]\n",
            "Top recommendations for user 1240: [ 8198 28213 23825 29434 11396 15290 11062  1451 17328 26902]\n",
            "Top recommendations for user 1170: [15068  8198  6347 24161 17350 23952  7031 23997 24160 25982]\n",
            "Top recommendations for user 772: [17350 24701 28028 21134 25334  6347 25982 24745 18960 17524]\n",
            "Top recommendations for user 911: [17350 11062 17524  5486  6903  2003 21134  7316  3479 22215]\n",
            "Top recommendations for user 276: [11062 29434 17350 28162  4615  3479 24701 25334  1449 24745]\n",
            "Top recommendations for user 1029: [ 2805  4723 28162 24745 28608  7890 29434 15346 17524  1634]\n",
            "Top recommendations for user 1662: [17350 24701 18960 21134 17524 24745  6347 25334 22124 11062]\n",
            "Top recommendations for user 1056: [ 4615  5472 11268  7453 10380  2553 17504 28162  3847 20836]\n",
            "Top recommendations for user 529: [17350 19665 24907  7031  2003 21134  6347 22544 17524  6885]\n",
            "Top recommendations for user 735: [ 8198 11062 15068 29434 28213 15203 20487  1451 23825 20931]\n",
            "Top recommendations for user 510: [15068  8198 11062 23952 24701 25982  4385 17350 21134 25334]\n",
            "Top recommendations for user 283: [11062 15068 29434  8198 10807  6885 22215  3479 10894  4615]\n",
            "Top recommendations for user 565: [17350 21134 17524  6903  2003  6347 24701 28028 18960  7031]\n",
            "Top recommendations for user 345: [17350 11062 24701  4615 17524  6347 18960  3479 29434 21134]\n",
            "Top recommendations for user 581: [17350 11062  6903 17524 29434  7316 28028  2003 22215  3479]\n",
            "Top recommendations for user 1008: [18960 22124 15463  3404 24701  7920 21134  6492  6347 17350]\n",
            "Top recommendations for user 226: [26638  6903 28028 23952 21134  1826 20722  5543 28987 25362]\n",
            "Top recommendations for user 1804: [11062 17350 22215  5486 17524  3479 10894  2245  2003  7316]\n",
            "Top recommendations for user 431: [11062 17350 17524 24701  8975  7890 21134  2245 24745 25334]\n",
            "Top recommendations for user 264: [17350  8198 28028 11062 29434 15068  1059  7316 20836 23825]\n",
            "Top recommendations for user 354: [15068  8198 11062 15203  4385 24701 25334 25982 13055 23952]\n",
            "Top recommendations for user 797: [ 4615 24074 18960  6347  6492 10199 16706  9086 28162  4694]\n",
            "Top recommendations for user 750: [24243 13881 28028 23616  5238 10931 11533   396 22124 23997]\n",
            "Top recommendations for user 655: [17350  7031 15068  6903 21134 28028  2003  7316 23952  6347]\n",
            "Top recommendations for user 819: [24701 25334 11062 29434 18960 24745 25982 17350  8198   396]\n",
            "Top recommendations for user 1165: [ 4097 19713  8198 20487  4694 24074 19680 28679  2339  9886]\n",
            "Top recommendations for user 667: [ 8198  4097  1451 29434 20931 20487 13881 28213    45 12533]\n",
            "Top recommendations for user 1827: [21134 17524 17350  7890 24701 27569  6903 22544 24745 18960]\n",
            "Top recommendations for user 978: [11062 17350 24701 17524  2245  3479 29434 21134 10894 25334]\n",
            "Top recommendations for user 1731: [ 8198 29434  4097 20931 11062  4615 15068  1451 23997 13881]\n",
            "Top recommendations for user 1235: [11062  4615 17350  3479 29434 28162 18960  6885 24701  6347]\n",
            "Top recommendations for user 938: [11062 17350  4615 29434  3479 28162  6347 24701 10894 15068]\n",
            "Top recommendations for user 561: [11062 29434 15068  8198 17350 24701 15203  1449 25334  2245]\n",
            "Top recommendations for user 1839: [ 8198 15068 24701 25982 15203 25334 11062 24160 27931 23952]\n",
            "Top recommendations for user 760: [17350 11062 15068  8198 24701 29434 28028  6347 21134 25334]\n",
            "Top recommendations for user 1348: [ 8198  1451 20487  4097 13881 28213  2126  9962 20931 16697]\n",
            "Top recommendations for user 1083: [15068  6885 18724 14504  7031 11062  8198 23952 27968 22215]\n",
            "Top recommendations for user 690: [11062 17350 15068  8198 29434 24701 25334 28028 15203 17524]\n",
            "Top recommendations for user 1801: [11396 23807  6903 17328  1634  7890 11062 28213 15290 23825]\n",
            "Top recommendations for user 1062: [23952  7031 15068 17350  6347 21134 24161   640 22124 25982]\n",
            "Top recommendations for user 1243: [24907 17350  4615  6347  7031 20836  2003 18960 24220  3479]\n",
            "Top recommendations for user 41: [ 2126  4615  9086  7205 24907 20836  7021  1820 13881 23997]\n",
            "Top recommendations for user 352: [ 6885  4615  6347 18960  3479 17350  7031 25401  3404  3847]\n",
            "Top recommendations for user 1774: [ 6903 17350 28028 26638 17524  7890  7316 18103  1059 12128]\n",
            "Top recommendations for user 1540: [ 4615  6885  3847  3479  6347 24907  9086 24074 18960  3477]\n",
            "Top recommendations for user 707: [26638 28028  6903  1826 20722   725 20985 24160 17350 23952]\n",
            "Top recommendations for user 1477: [28028 17350  6903  6347  1059 26638 21134 24907   396  2003]\n",
            "Top recommendations for user 1690: [11062 15068 17350 24701  2245 21134 29434 25334 17524 10894]\n",
            "Top recommendations for user 868: [29434  8198 11062 28162 20931  4615 23825 25334  4097  1449]\n",
            "Top recommendations for user 673: [ 8198 11062 17350 15068 29434 24701 25334 28028 25982  6347]\n",
            "Top recommendations for user 268: [ 6885  4694  7988 12581 13091 29430  3847  4508  5862 10807]\n",
            "Top recommendations for user 1651: [ 8198 11062 17350 22215 29434 20731  7316 20466 15068 23825]\n",
            "Top recommendations for user 949: [ 8198 15068 15203 24160 25982 11062 25334 24701 23952  4385]\n",
            "Top recommendations for user 192: [ 6885 19682  3847  7988 13352 25401  4615 14504  4694  3477]\n",
            "Top recommendations for user 1290: [ 8198 28028 15068 17350 20985 24160  7031 24907 23952 24161]\n",
            "Top recommendations for user 1280: [17350 11062 15068 24701 21134  6347  2245 25334 17524 18960]\n",
            "Top recommendations for user 203: [ 4615 18960  6347 17350 24074 24701  3479 24220 22124 10199]\n",
            "Top recommendations for user 880: [17350 11062  4615  3479 29434 24701  6347 17524 21134  2003]\n",
            "Top recommendations for user 1567: [11062 15068 17350  6885  2245 10894  8198  3479 22215 14504]\n",
            "Top recommendations for user 1332: [21058 11062  9392  8975  3710 10807 13462   871  2202  1449]\n",
            "Top recommendations for user 1158: [17350  4615 11062  6347 18960  3479 24701 21134 29434  2003]\n",
            "Top recommendations for user 558: [17350 11062  6903  7316 28028 17524 15068  1634 12128 22215]\n",
            "Top recommendations for user 304: [17350  6347 28028 18960 21134 24701   396 22124  7031 25982]\n",
            "Top recommendations for user 1650: [ 6885  6347 15068 24161  7031 23952 10199 20140   640 16706]\n",
            "Top recommendations for user 718: [ 6903 10380 22215 17350  7316 12128  2075 17764 11062  4566]\n",
            "Top recommendations for user 555: [24243 23616 24074 18960 22755 25982 22456  4572 22124 24701]\n",
            "Top recommendations for user 374: [ 4615 11062 29434 28162  3479  3847  1277  6885 17350 20931]\n",
            "Top recommendations for user 1169: [ 6903 17350 28028 26638 23952 21134 15068  7031 20722 19665]\n",
            "Top recommendations for user 516: [ 3477 15463  4615  6885 18960  3631  3404 19682  1380 10527]\n",
            "Top recommendations for user 837: [11062 29434 28162  4615  1449 17350 24701  3479 25334 15068]\n",
            "Top recommendations for user 1375: [28028 17350  8198 11062  6903  1059 29434 23825  1634  7316]\n",
            "Top recommendations for user 1798: [ 8198 15068 11062 28213 15203 29434  2319 13055  1451 25334]\n",
            "Top recommendations for user 407: [11062 17328  1449 29434 10708 28213 11396 23662 26902 21058]\n",
            "Top recommendations for user 1689: [17350 28028  8198 15068 25982 24701  6347 24160 25334 21134]\n",
            "Top recommendations for user 473: [11062 21058  8975  9783  7890 13462 28608 15346  2202  9392]\n",
            "Top recommendations for user 608: [ 8198 11062 15068 29434 20487  1449 15203 13055 28213  4385]\n",
            "Top recommendations for user 153: [11062 15068  8198 29434 13055 15203  8975 28213  2245  1449]\n",
            "Top recommendations for user 460: [11062 15068  8198 17350 29434  2245 15203 22215 10894  3321]\n",
            "Top recommendations for user 463: [10527 20314 10207 15463 24835  2103  7442  2805  8113 25762]\n",
            "Top recommendations for user 279: [27931 24701  8975 24745 27569 25334 27801  7890 28608 29907]\n",
            "Top recommendations for user 10: [11062 17350 17524 24701  2245 21134 29434  5486 10894 15068]\n",
            "Top recommendations for user 21: [17350 28028 10380  6903 20836  1059  2003   396  2126 11062]\n",
            "Top recommendations for user 675: [24701 11062 17350 25334 15068 21134 25982 24745 15203 27931]\n",
            "Top recommendations for user 1176: [15068  8198 11062 29434 20487  4385 15203 17350 13055  2245]\n",
            "Top recommendations for user 984: [15068 11062 17350  8198  6885 23952  2245 24701 21134  4385]\n",
            "Top recommendations for user 979: [ 8198 15068 20487  1451 18724  5732  4097  3321 13372  9962]\n",
            "Top recommendations for user 43: [24907  7031 13685  6885  4651 30258 19665 18724 17350 24161]\n",
            "Top recommendations for user 1558: [24701 18960 25334 16706 27931 25982  4572 24074  2128 29178]\n",
            "Top recommendations for user 6: [11062 15068 17350  6885  4615  6347 24701  3479  2245 16706]\n",
            "Top recommendations for user 1746: [17350  4615 11062 29434  6347 28162  3479 18960 20836   396]\n",
            "Top recommendations for user 29: [ 4615 17350  3479 11062 18960 28162  6347  2003 17524 29434]\n",
            "Top recommendations for user 1533: [24243   396 23616 28028 18960 24701 25982 25334  4723 24160]\n",
            "Top recommendations for user 1636: [17350 28028 24701 24745 11062 25334   396 17524  1634 29434]\n",
            "Top recommendations for user 623: [17350  6347 18960 24701 11062 21134  4615 25334 28028 25982]\n",
            "Top recommendations for user 1336: [17350 24701 21134 28028 25982 25334  6347 18960 24745 15068]\n",
            "Top recommendations for user 440: [15068 17350 28028 21134 23952 24701 25982 26638  6903  8198]\n",
            "Top recommendations for user 442: [ 8198 15068 11062 29434 20931  4097 24160 17350 25334 25982]\n",
            "Top recommendations for user 178: [11062  4615 29434 28162  8198  3479 20931 17350 15068  4097]\n",
            "Top recommendations for user 1119: [29434 17350 11062   396 28162  4615  4723 24745 25334 24701]\n",
            "Top recommendations for user 924: [ 4615  8198  4097 29434  2126 28162 20931 28679  7205  9086]\n",
            "Top recommendations for user 1768: [ 8198 15068 15203 24160 11062 25982 28028 23952 25334 24701]\n",
            "Top recommendations for user 744: [23952 25982 22124  6347 21134   640 28028 24160 24243 24701]\n",
            "Top recommendations for user 716: [ 6885 11062 22215 17350  4615  3479 10894  5486  2245 14504]\n",
            "Top recommendations for user 1322: [ 4615 11062 17350 29434 28162  3479 20836  6347  1277  2003]\n",
            "Top recommendations for user 1242: [18960 24701  6347 16706 17350 25334 24074 25982  4615 11062]\n",
            "Top recommendations for user 66: [11062 17350 29434  4615 24701 28162  6347  3479 25334 18960]\n",
            "Top recommendations for user 1109: [28162  2805  4723 17328 29434 11396  7442 15346  1449 21671]\n",
            "Top recommendations for user 739: [13091 12581  5862 19543  3847 10807  6885 27312 19713  7624]\n",
            "Top recommendations for user 1562: [ 8198 28028 15068 17350 15203 11062 24160 28213 25982 25334]\n",
            "Top recommendations for user 75: [17350  4615 11062 29434 18960 28162  6347 24701   396  3479]\n",
            "Top recommendations for user 1588: [ 8198 15068 11062 29434 17350 15203 20931 25334 20487  4097]\n",
            "Top recommendations for user 1699: [17350 15068 21134 23952  7031  6347 24701 11062  2245 28028]\n",
            "Top recommendations for user 472: [ 8198 17350 28028 11062 29434 23825  1059  7316 20466  6903]\n",
            "Top recommendations for user 1183: [ 6885 14504  2245 11158 24621  8975 24721 24272 19682 13352]\n",
            "Top recommendations for user 1350: [28028 26638   396  6903  9398  8198  1059 17350 20126 24745]\n",
            "Top recommendations for user 1147: [ 4615 17350  6347 20836 18960  3479 24074 29434 28162 23997]\n",
            "Top recommendations for user 453: [20731 17764  4121 22215 26529  2075   265 11187 11029  4472]\n",
            "Top recommendations for user 167: [ 8198 24243  4572 27931 25334 24160 25982 24701 16706  4097]\n",
            "Top recommendations for user 1316: [ 4615  2126 28162  7205 29434  3847  9086  1277  7021 20836]\n",
            "Top recommendations for user 135: [11062 17350  6885 15068  2245 10894 22215  3479 14504 21134]\n",
            "Top recommendations for user 1584: [28028 17350  8198 15068 25982 24701 24160 21134 25334 15203]\n",
            "Top recommendations for user 424: [15068 23952 21134 24701 25982  7150  8198 17350 25334 24160]\n",
            "Top recommendations for user 903: [11062 15068  8198 17350 24701 25334 29434 15203 25982  4385]\n",
            "Top recommendations for user 970: [17350  7031  6347 21134 24907 23952  2003 15068   640 28028]\n",
            "Top recommendations for user 1800: [ 8198 28028 24160 24243 26638 13881 25982  1451 28213 20985]\n",
            "Top recommendations for user 1245: [ 6903 19665 26638  7031 17350 24907 28028 18332 20527 28987]\n",
            "Top recommendations for user 856: [ 6885 24907 17350 22215  7031 11062  2003 10894 15068  3479]\n",
            "Top recommendations for user 1491: [26638 23952 28028  1826  7150 24160 24243   640 25982  5238]\n",
            "Top recommendations for user 295: [17350 15068  6885  7031 11062 21134  2245 22215 10894  2003]\n",
            "Top recommendations for user 1215: [15068  8198 23952 24160 24161 28028 25982 17350  6347  7031]\n",
            "Top recommendations for user 1193: [29434 28162  8198 11062  4615 25334 20931 24074 24701 20555]\n",
            "Top recommendations for user 1648: [19682 28368 27709 28823  6885   640 15998 22124 23395 10636]\n",
            "Top recommendations for user 1345: [ 4615  2126 11268  5472 20836 24907 10380 28162  7205 17800]\n",
            "Top recommendations for user 553: [24907 27871 20840 10380 19665 20527 16580 23920 26398 24582]\n",
            "Top recommendations for user 221: [ 6885 11062 17350 15068 22215 10894  2245  3479  7031 14504]\n",
            "Top recommendations for user 370: [ 6885  7031 14504 25401 24907 17350 24621 18724 22215 10894]\n",
            "Top recommendations for user 1495: [ 8198 15068 20487 11062  1451  4097 13372 20731 11187 29434]\n",
            "Top recommendations for user 397: [24907  5472 17504 11533 10495   583  8670 10710 22124 10931]\n",
            "Top recommendations for user 218: [ 4615  3477 11268  3631 24907 17504 18960 15463  7453  3847]\n",
            "Top recommendations for user 1239: [ 4615 11062  6885 28162  3847  3479 29434  6492  7988 22935]\n",
            "Top recommendations for user 34: [23952  7150   640 22124 21134 28368 19187 25982 24161 24701]\n",
            "Top recommendations for user 1372: [11062 15068  8198 17350 29434  2245 24701 15203 25334 10894]\n",
            "Top recommendations for user 782: [17350 11062  8198 29434 24701 25334 28028  6347  4615 15068]\n",
            "Top recommendations for user 787: [11062 17350 29434  8198 23825 28028 15203  6903 28213  1634]\n",
            "Top recommendations for user 1167: [17350  8198 15068 28028  6347  7031 24907 23997 24161 24160]\n",
            "Top recommendations for user 887: [28028  8198 17350 15068 24160 25982  6347 11062 24701 25334]\n",
            "Top recommendations for user 1547: [17350 11062  6347 24701 18960  4615 25334 21134 29434  3479]\n",
            "Top recommendations for user 724: [17350  4615  6347 20836 18960   396 28028  2003  3479 28162]\n",
            "Top recommendations for user 237: [29434 28162  4615 17350 11062  4723   396 23825 24745  2126]\n",
            "Top recommendations for user 105: [17350 11062  6347 24701  8198 28028 25334 18960 15068 29434]\n",
            "Top recommendations for user 845: [17350 28028  6347  2003  6903 21134   396  7031 17524 24907]\n",
            "Top recommendations for user 1335: [17350 17524 21134 18960  3404 15463 22124 24621  6347  5486]\n",
            "Top recommendations for user 159: [11062  8198 29434 15068  4615 28162 20487  4097  4385  3479]\n",
            "Top recommendations for user 832: [ 4723 12533 22755   396 23616 28162 24074 10931 29434  2126]\n",
            "Top recommendations for user 795: [17350 28028 21134  6903  6347 17524 24701  7031  2003 25982]\n",
            "Top recommendations for user 547: [ 3710 15068 11062 14504 10807  6885  9392  8975 13055 21229]\n",
            "Top recommendations for user 291: [17350 11062  4615 29434  8198  6347 24701 25334 18960 15068]\n",
            "Top recommendations for user 411: [24907  4651 24161 13685 25596  5864  7031  6915 23395 22425]\n",
            "Top recommendations for user 753: [17350  6347  4615 18960  2003 22124  7031  3479 21134 24907]\n",
            "Top recommendations for user 1358: [  396 24243 18960 28028 23616 22124 10931  6347 24074 24701]\n",
            "Top recommendations for user 1605: [17350  6903 28028 21134 17524  2003  7316  1059 26638  6347]\n",
            "Top recommendations for user 1379: [11062 29434 17350 24701  4615 25334 15068  8198 28162  3479]\n",
            "Top recommendations for user 1037: [ 6885  7031 17350 25401  6347 24907  4615  2003 21134 24621]\n",
            "Top recommendations for user 427: [ 4615 11062 28162  3847  3479 22215  6885 29434  1277  5486]\n",
            "Top recommendations for user 485: [ 8975 21058 11062  9392  1449 13462 17571  9783 23662  7613]\n",
            "Top recommendations for user 1715: [28028  8198 17350 23825 29434 11062 15290 28213 11396  1059]\n",
            "Top recommendations for user 859: [28028  8198 24160 20985 25982 24243 23952 13881 26638 17350]\n",
            "Top recommendations for user 65: [17350  6347  7031 21134 22124   640 23952 18960 24161 24220]\n",
            "Top recommendations for user 590: [11062 17350 15068 24701 21134 25334  8198 17524  2245  6347]\n",
            "Top recommendations for user 1722: [ 6347 17350 15068  7031 23952 21134   640 24161  6885 18960]\n",
            "Top recommendations for user 1141: [ 4097 19680  8198 20140 20487 15068  4651 18724 24161 18581]\n",
            "Top recommendations for user 493: [17350 28028 24907 20836  6347  2003  7031  4615 23997  1059]\n",
            "Top recommendations for user 144: [17350 11062 17524 15068 24701 21134 29434  3479  2003  6347]\n",
            "Top recommendations for user 1293: [17350 11062  4615  6347 15068  3479 24701 29434 21134  7031]\n",
            "Top recommendations for user 769: [17350 18960  6347   396 28028 24745 17524  4615 24701  2003]\n",
            "Top recommendations for user 36: [17350  4615 11062  3479  6347 18960  2003 29434 28162 17524]\n",
            "Top recommendations for user 1627: [11062 15068  2245 17350  8198 10894 29434 14504  6885  8975]\n",
            "Top recommendations for user 1674: [17350 11062 17524 24701 29434  3479 21134  6347  2245  2003]\n",
            "Top recommendations for user 1566: [11062 17350 15068 29434 24701  8198 25334 15203 17524  2245]\n",
            "Top recommendations for user 869: [15068 11062  8198  4385 16706 29434 24701  4615  6885  4097]\n",
            "Top recommendations for user 255: [ 6885 24907  4615  7031  2627 22215 30258 25401 11268 17350]\n",
            "Top recommendations for user 904: [ 4615 18960  6347 17350 22124 24074 24220 10199 20836  2003]\n",
            "Top recommendations for user 483: [ 8198 15068 11062 15203 28213 13055  8975  2319 23662 29434]\n",
            "Top recommendations for user 1089: [17350 11062 29434  4615 28028   396  6347 28162 24745 18960]\n",
            "Top recommendations for user 1607: [28028 17350  6347   396  6903 21134 22124  2003 24907  7031]\n",
            "Top recommendations for user 1120: [11062 17350 15068  8198 29434 24701  6347 25334 28028 21134]\n",
            "Top recommendations for user 298: [ 4615 17350 28162 29434 11062  3479 18960  6347 20836 24745]\n",
            "Top recommendations for user 1195: [17350 15068 11062  6885  2245 21134  7031 10894 22215 17524]\n",
            "Top recommendations for user 790: [17350  6347 18960 28028 24701   396 25334 24745 21134 25982]\n",
            "Top recommendations for user 855: [11062 22215 17350  6885 15068 10894  2245  5486 18332 14504]\n",
            "Top recommendations for user 116: [ 8198 15068 20487  7150 15203  4385 13055 27931  7267 21806]\n",
            "Top recommendations for user 1571: [11062 15068 10807 14504  6885 29434  3710  9392  2245  8198]\n",
            "Top recommendations for user 824: [24074 10199  4615  9086  6347 22124 18960 24907 24220  1380]\n",
            "Top recommendations for user 1005: [26638  8198  1826 24160 28028 24243 15068 23952 15203  7150]\n",
            "Top recommendations for user 1736: [23952 15068 26638 23395 18332 28987 27968  9223 12012  2858]\n",
            "Top recommendations for user 1765: [  396 18960 17350 22124  6347 24745 23616 24074 28028 24701]\n",
            "Top recommendations for user 783: [19682  7150 28172 29178 24280  7988 14852  7156  7869  2339]\n",
            "Top recommendations for user 1380: [ 8198 15068 28028 17350 24701 25982 25334 24160 11062  6347]\n",
            "Top recommendations for user 489: [15068 11062 17350 24701  6885  2245  4385 21134  8198  6347]\n",
            "Top recommendations for user 1750: [27931  8198 15068  7156  7869  4385 13055  7267 15203 20124]\n",
            "Top recommendations for user 1596: [ 6885 15068 22215 11062 17350  7031 14504 10894 24907  3479]\n",
            "Top recommendations for user 1542: [ 6885 17350 11062  3479  4615  6347  7031 21134 10894  2245]\n",
            "Top recommendations for user 578: [17350 11062  6347 28028  2003 24907  4615  7031 20836  3479]\n",
            "Top recommendations for user 1844: [17350 28028 15068 21134  6903  8198 23952  6347  7031 25982]\n",
            "Top recommendations for user 1727: [28028  8198 17350 15068 24160 25982  6347 23952 24701 21134]\n",
            "Top recommendations for user 1447: [17350  6347 18960 21134  4615  2003 22124 17524  7031  3479]\n",
            "Top recommendations for user 1815: [11062 29434 17350 24701 28162  4615  3479 25334 18960 15068]\n",
            "Top recommendations for user 1548: [ 8198 28028 15068 17350 24160 20985 25982 23997 13881  6347]\n",
            "Top recommendations for user 500: [29434 11062  4615 28162 17350  8198 20931  3479 23825 25334]\n",
            "Top recommendations for user 791: [23952 21134  7031 17350 15068   640  6347 22124  6885 24161]\n",
            "Top recommendations for user 1035: [17350  6347  4615  7031  2003 24907 11062 20836  3479 18960]\n",
            "Top recommendations for user 920: [17350 24907 20836  4615 10380 28028  2003  2126 26024  6347]\n",
            "Top recommendations for user 7: [17350 11062 24701 25334  6347 18960 29434 17524 21134 24745]\n",
            "Top recommendations for user 1469: [17350 11062 17524  3479 24701 29434 21134  6347  2003  2245]\n",
            "Top recommendations for user 28: [15068 11062 14504  6885  8198  2245 23952  4385 24682 10894]\n",
            "Top recommendations for user 873: [15068 17350  8198 24701 11062 28028 21134 25982 23952 25334]\n",
            "Top recommendations for user 560: [17350 10380 28028 20836  4615   396 24907  2003  6347  2126]\n",
            "Top recommendations for user 307: [26638 27931 15203  9398  8198 23835 28213 27773 28028  2319]\n",
            "Top recommendations for user 92: [ 6903 17350 17524 22544  7890 19665 21134 18103  2003  5486]\n",
            "Top recommendations for user 774: [15068 23952 21134  8198  7150 26638 28172 25982  2245 15203]\n",
            "Top recommendations for user 158: [17350  6347  4615 11062  7031 18960 21134  3479  2003 24701]\n",
            "Top recommendations for user 710: [ 6885  7031 14504 23952 21134 24621 15068  2245 25401   640]\n",
            "Top recommendations for user 400: [28028 17350 21134 23952 15068  6347 25982  7031  6903 26638]\n",
            "Top recommendations for user 1344: [11062 29434  4615 17350 28162  3479 22215 10894 15068  1277]\n",
            "Top recommendations for user 714: [ 7031 23952 21134 17350   640  6347 15068 24907 24161  6885]\n",
            "Top recommendations for user 573: [15068 23952  8198 17350 28028 24161 24160 25982 21134  7031]\n",
            "Top recommendations for user 443: [11062 17350 29434  4615 18960 24701 28162 25334 24745  3479]\n",
            "Top recommendations for user 1836: [ 7988 25568  4385 16706 29178  6492 23678 20630 15068  2002]\n",
            "Top recommendations for user 0: [16706 24701 15068  4385 25334 27931  7150 29178  4572 24074]\n",
            "Top recommendations for user 1734: [ 8198 17350 11062 15068 29434 28028  4615  6347 20931 23997]\n",
            "Top recommendations for user 79: [28028 17350  6347   396 24160 21134 20985 25982 24907  7031]\n",
            "Top recommendations for user 965: [11062 29434  4615 28162  3479 15068  6885 10807  3847  4385]\n",
            "Top recommendations for user 957: [11062 15068  6885  7988  4615  4385 16706 29434  4694  6492]\n",
            "Top recommendations for user 1602: [17350  6347 28028 21134 24701 18960 25982  7031 15068 11062]\n",
            "Top recommendations for user 632: [29434  8198 11062 28162  1449 20931 23825 20555  4615  4097]\n",
            "Top recommendations for user 1283: [28162  6492 22755 12300 29434  4723  4572 22935  1449 18960]\n",
            "Top recommendations for user 1771: [17524 17350 21134 22544  5486  7890  2003 10527 14644  6903]\n",
            "Top recommendations for user 1307: [ 8198  4723 28213 11396 27931 15203 25334  9398 29434 23825]\n",
            "Top recommendations for user 1228: [15068 17350  8198  7031  6347 11062 23952 24161  6885 21134]\n",
            "Top recommendations for user 70: [17350 11062 24701 18960 17524 25334 29434 24745 21134  6347]\n",
            "Top recommendations for user 1207: [28028  6903 26638 17350 21134 23952 24701 25982 17524 20722]\n",
            "Top recommendations for user 893: [17350 11062  6347  4615  2003  3479 28028 20836 29434 18960]\n",
            "Top recommendations for user 444: [17350  4615 11062  6347 29434  3479 18960 24701 28162 15068]\n",
            "Top recommendations for user 1043: [26638  6903 28987 19665 23952 29650 15925  1871 28028 21134]\n",
            "Top recommendations for user 1817: [23395 23952  7031   640 24907 28987  7105 19665 28368 24161]\n",
            "Top recommendations for user 1752: [ 6885 14504 11062 24721 15068 18332 11158  3710  2245 10807]\n",
            "Top recommendations for user 1377: [17350 11062  8198 15068  6347  4615  7031 29434 28028  3479]\n",
            "Top recommendations for user 1434: [11062 15068 17350 24701 21134  2245 17524 25334 10894  6347]\n",
            "Top recommendations for user 258: [11062  4615 17350 18960 24701 29434 28162  3479  6347 25334]\n",
            "Top recommendations for user 194: [24243 22124 14773 23616 18960 22456 28368  7150 19187 24701]\n",
            "Top recommendations for user 230: [11062  8975 15068  2245  9783 21058 17350 17524 12128 15203]\n",
            "Top recommendations for user 1783: [ 6903 17350 26638 28028 12128 17524  7316  7890 21134  1059]\n",
            "Top recommendations for user 339: [11062 24701 29434 25334 15068 27931  8975  1449  2245 15203]\n",
            "Top recommendations for user 1743: [ 7920 28986 10636 19682 29178 14773 18960 20579  6492 22124]\n",
            "Top recommendations for user 1222: [21134 22124   640  3404 18960 23952 24701  6347 29178  7150]\n",
            "Top recommendations for user 55: [17350  6347 21134  7031 28028 18960 11062 24701  2003  4615]\n",
            "Top recommendations for user 514: [15068  8198 11062  6885 17350 20487  2245 23952 18724 14504]\n",
            "Top recommendations for user 897: [ 8198 15068 11062 17350 25982 24160  6347 25334 24701  4097]\n",
            "Top recommendations for user 235: [17350 15068 11062  6347  6885  7031 21134  2245  3479  2003]\n",
            "Top recommendations for user 1476: [17350  7031 24907  2003  6903  7316 19665  6347 21134 22215]\n",
            "Top recommendations for user 419: [11062 17350 15068  4615 29434  3479 24701  6347  8198 25334]\n",
            "Top recommendations for user 1499: [29434  4615 11062 28162  8198 18960 25334 24074 17350 24701]\n",
            "Top recommendations for user 312: [28028 24907 24161  7031 20985 23952 17350 25362  8198 13881]\n",
            "Top recommendations for user 127: [15068  8198 11062  4385 29434 16706 24701 25334 20487  4097]\n",
            "Top recommendations for user 1268: [10807 11062  3710  4468 13091 12581 14510 28162  9392 29434]\n",
            "Top recommendations for user 523: [17350  6903 17524 21134 24701 11062 28028  7890 24745 25334]\n",
            "Top recommendations for user 712: [17350 11062 28028 24701  8198 15068 25334 29434 17524 21134]\n",
            "Top recommendations for user 1421: [17350  4615 29434 28162 11062 18960  6347  3479   396 20836]\n",
            "Top recommendations for user 1076: [ 6885 14504 11062  7988 13352 10807  2245  3847  4615 10894]\n",
            "Top recommendations for user 1838: [17350 24701 21134 28028 25982 25334  6347 23952 24745 15068]\n",
            "Top recommendations for user 889: [17350  6347 11062 28028 21134  7031  2003 15068 24701 17524]\n",
            "Top recommendations for user 1354: [18960 24074 24243 23616   396 22124 22755 24701  4704 10931]\n",
            "Top recommendations for user 717: [15068 23952  8198 11062  6885 14504  2245 21134 18724  4385]\n",
            "Top recommendations for user 818: [ 7890 17524  6903 27569 28608  8975 14644 22544 21134 17350]\n",
            "Top recommendations for user 741: [28028 17350  8198 25982 15068  6347 21134 24160 24701  6903]\n",
            "Top recommendations for user 771: [24701 11062 18960  3404 16706  6885  2245 15068 21134 29178]\n",
            "Top recommendations for user 572: [17350 28028 24701 11062 25334 24745   396  6347 29434 18960]\n",
            "Top recommendations for user 1599: [ 8198 15068 20487 11062 18724  3321  4097  1451  5732  8895]\n",
            "Top recommendations for user 1695: [ 8198 15068 20487  4097 11062 29434  4385 20931  1451 24160]\n",
            "Top recommendations for user 289: [17350 28028  6347 21134 24701 25982   396 25334 24160 18960]\n",
            "Top recommendations for user 738: [ 8198 28028 15068 15203 28213 24160 25982 26638  2319 25334]\n",
            "Top recommendations for user 905: [ 6903 17350  7890 12128 17524 26638  8975 11062  7316  9783]\n",
            "Top recommendations for user 669: [18960 24701 22124 24243 16706 24074 25334 25982  6347 21134]\n",
            "Top recommendations for user 827: [24907  4615  9086 20836 23997 17350  7031  6347 11533 13685]\n",
            "Top recommendations for user 269: [11062  8198 15068 29434 17350 22215  3321 26529 23825  7316]\n",
            "Top recommendations for user 1438: [24701 11062 25334 18960 17350 29434 16706  6347 25982 24745]\n",
            "Top recommendations for user 1166: [11062  8198 15068 29434 15203  1449 13055 17350 28213 25334]\n",
            "Top recommendations for user 1546: [17350 11062  6347 28028 24701  8198  4615 15068 29434 25334]\n",
            "Top recommendations for user 58: [18960 17350  6347 22124   396  4615 24074 24701 24220 21134]\n",
            "Top recommendations for user 1255: [11062 17350 24701 29434 25334 15068  6347 18960  4615 21134]\n",
            "Top recommendations for user 296: [28162 29434 17328  2805 20345 14032  2553  4615  2126  8663]\n",
            "Top recommendations for user 1460: [15068  4097  8198  4694 24074  4615 20487 16706 20140  6885]\n",
            "Top recommendations for user 74: [ 4615 17350 29434 28162 11062 18960  6347 20931   396 20836]\n",
            "Top recommendations for user 1728: [ 4615 29434 28162 11062 17350  3479 20931 18960 24074  8198]\n",
            "Top recommendations for user 1404: [24701 21134 22124 18960 27931 24745 17524 27569  3404   846]\n",
            "Top recommendations for user 211: [ 6885 14504 11158 18332 27968 24721 24272  2245 24621 15068]\n",
            "Top recommendations for user 674: [17350 24701 18960 24745 11062 25334 17524 21134  6347 29434]\n",
            "Top recommendations for user 50: [24907  9086  4615  7031  6347 23997 13685 10199 30258  4651]\n",
            "Top recommendations for user 933: [17350  4615 11062 29434  3479 20836 28162  2003 22215  6347]\n",
            "Top recommendations for user 87: [19682 28368  7920 22124 10636 20579 29178   640 27709 15998]\n",
            "Top recommendations for user 386: [ 6903 26638 28028 10380 18655 18103 16019  7890   396 16947]\n",
            "Top recommendations for user 109: [24701 18960 25334 11062 17350 29434 24745  6347  4615 16706]\n",
            "Top recommendations for user 1151: [17350 11062  6347 29434  4615 24701  3479 15068 17524  2003]\n",
            "Top recommendations for user 300: [15068  8198 11062  4097 20487 29434  4615  6885  4385 10807]\n",
            "Top recommendations for user 541: [23952 21134   640 15068  7150 22124  7031 24161  6347  6885]\n",
            "Top recommendations for user 665: [28028  6903 10380 17350   396  1059 12858 18655 12614 10495]\n",
            "Top recommendations for user 630: [17350 24907  2003 20836  7031 10380  6347 28028  4615  6903]\n",
            "Top recommendations for user 564: [15068 11062  8198 24701 23952 21134 25334 25982 17350  2245]\n",
            "Top recommendations for user 700: [28028 17350 15068 21134 26638 24701 25982  6903  8198 23952]\n",
            "Top recommendations for user 1232: [18960 24701  6347 17350 11062 16706  4615 25334 21134  3404]\n",
            "Top recommendations for user 42: [28028 17350 21134 23952 22124  6347 25982   640 24160 24243]\n",
            "Top recommendations for user 1313: [11062 29434  8198 28162 15068  4615  1449  3479  4468 20931]\n",
            "Top recommendations for user 188: [17350 11062  2003  6347  3479 17524 15068 21134  4615 22215]\n",
            "Top recommendations for user 85: [ 6885 14504 18332 24721 11158 22215 27968 18817  5486  2245]\n",
            "Top recommendations for user 842: [11062  8198 29434 17350 15068 24701 25334 15203  3479  2245]\n",
            "Top recommendations for user 697: [18960 24701  4615 17350 11062 29434 25334  6347 28162 24074]\n",
            "Top recommendations for user 983: [11062 17350  4615 29434 28162  3479 18960  6347 24701 17524]\n",
            "Top recommendations for user 908: [17350 18960  6347  4615 21134 24701  3479 22124  3404  2003]\n",
            "Top recommendations for user 866: [ 8198 15068 11062 17350 29434 28028 20931 23997 24160  4615]\n",
            "Top recommendations for user 1383: [ 6903 21134 17350 17524 22544  7890 19665 24701 22124  2987]\n",
            "Top recommendations for user 1673: [17350 11062 17524 29434 24701  3479  4615 24745 21134 28162]\n",
            "Top recommendations for user 1301: [17350 11062 15068  8198 28028 24701 25334 21134 15203 29434]\n",
            "Top recommendations for user 324: [18332 15068  6903  2858 12128 14504  9223 27968 11158  6885]\n",
            "Top recommendations for user 95: [17350 28028 11062 15068  8198 21134 24701  6903 17524  6347]\n",
            "Top recommendations for user 1410: [24243   396 23616 28028 18960 22124  6347 24074 10931 17350]\n",
            "Top recommendations for user 1587: [15068 23952  7031 17350 24161 28028  8198 21134  6347 24160]\n",
            "Top recommendations for user 645: [  871 10807  9392 13091  3710 11062  2476 21058 15535  4468]\n",
            "Top recommendations for user 1455: [15068  8975 11062 14504  3710  7156 10807 13055  9392  4385]\n",
            "Top recommendations for user 1504: [ 8198 15068 13055 20487 15203 28213 11062  3710 12607  4385]\n",
            "Top recommendations for user 1832: [29434 28162  4615 11062  2126 17350 23825 20836  3479 20931]\n",
            "Top recommendations for user 1231: [10380  6903 24907 19665 18103 27871 22821 17350 15180 25734]\n",
            "Top recommendations for user 1859: [ 8198 11062 17350 15068 28028 29434  3321  7316 23825 15203]\n",
            "Top recommendations for user 702: [17350 15068  6347 23952  7031 28028 24161 21134  8198 25982]\n",
            "Top recommendations for user 1312: [17350 24907  7031  2003  6903 28028  6347 19665  7316 20836]\n",
            "Top recommendations for user 990: [28028 17350  6903   396 10380  1059 12858  6347  4216  2003]\n",
            "Top recommendations for user 254: [11062 15068 17350 29434  8198  3479  6885  4615 10894 22215]\n",
            "Top recommendations for user 798: [17350 15068  6347 11062 21134 24701  7031  4615 18960  3479]\n",
            "Top recommendations for user 942: [17350  4615  6347 18960 11062  7031  3479 24220  2003 20836]\n",
            "Top recommendations for user 1427: [ 8198 15068 17350 28028 11062 25982 24701 24160 25334 15203]\n",
            "Top recommendations for user 1198: [ 8198 29434 11062  1449  4097 28162 20555 20931 28213 15068]\n",
            "Top recommendations for user 1772: [21134  6903 22124 17350 22544 17524 28028 12434  2987  7890]\n",
            "Top recommendations for user 169: [11062 29434  8198  4615 15068 24701 25334 17350 28162 20931]\n",
            "Top recommendations for user 696: [28028   396  8198 26638 24160 25982 24243 17350 25334 15203]\n",
            "Top recommendations for user 1484: [15068  8198 11062 17350 28028 23952 24160 25982 15203 24701]\n",
            "Top recommendations for user 683: [ 8975 24701 27931 11062  7156 27569 25334 21058 17524 21134]\n",
            "Top recommendations for user 499: [ 8975 21058 11062  1449  9392 23662 27931 27801 17571  7156]\n",
            "Top recommendations for user 434: [24243 28028 13881  8198 23616   396 10931    45  4723  1451]\n",
            "Top recommendations for user 498: [14510  4468 11062  5862  4121 12581 29434 10394 17764 28162]\n",
            "Top recommendations for user 1386: [15068  8198 23952  9962 18724 24161   720  5732 24907 23395]\n",
            "Top recommendations for user 100: [17350 11062 29434 28162  4615 24745 24701  3479 25334 17524]\n",
            "Top recommendations for user 1152: [15068 17350  6885 11062  7031  2245 21134 14504 10894 22215]\n",
            "Top recommendations for user 1064: [15068 24701 23952 16706 25982  8198 21134 25334 11062  4385]\n",
            "Top recommendations for user 1725: [11062  8198 29434 15068 17350  4615 20931  3479 28162  6347]\n",
            "Top recommendations for user 1343: [17350 28028 21134  6347 22124   396  6903 18960  2003 24701]\n",
            "Top recommendations for user 1685: [24074 15068  4097 16706  8198  4385 11062  6492  4694 29434]\n",
            "Top recommendations for user 1766: [16706 24701 29178 15068 18960  4385 24074  3404  7988  6492]\n",
            "Top recommendations for user 53: [17350  4615  6347  3479 18960 11062  2003 17524 20836 28162]\n",
            "Top recommendations for user 1577: [ 8198 15068 11062 28028 20487 17350  3321  1451 24160 15203]\n",
            "Top recommendations for user 216: [ 8198 15068 11062 25334 24701 29434 15203 25982 27931  4385]\n",
            "Top recommendations for user 1041: [17350  7031  6347 24907 21134  2003   640 23952 24220 28028]\n",
            "Top recommendations for user 1069: [ 8198 11062 17350 15068 29434 28028 25334 24701 15203 25982]\n",
            "Top recommendations for user 543: [ 6885 11062  5486 22215  3479 14504 10894  2476  2245 13352]\n",
            "Top recommendations for user 757: [ 8198 25334 24701 15068 27931 29434 11062 15203  4572 25982]\n",
            "Top recommendations for user 1063: [ 4615 24907 11268 20836 17350  3479  3847  2003  5472 17800]\n",
            "Top recommendations for user 509: [ 4615  8198 11062 29434  4097 15068  9086 20731 11187 20487]\n",
            "Top recommendations for user 1551: [ 6885 11062 14504 15068 10807  9392  2245  3710  7988  8975]\n",
            "Top recommendations for user 691: [24243 28368 22124 27039 24074  5238 10199 14773 23616  6729]\n",
            "Top recommendations for user 1755: [  396 28028 24243 23616 10931  4723 18960 24745 22124 25982]\n",
            "Top recommendations for user 624: [11062 29434  4615 28162  8198 15068  3479  4097  3847  1277]\n",
            "Top recommendations for user 1130: [ 8198 28679 29434  1451 12533 28162 14032 11597  2126 19713]\n",
            "Top recommendations for user 1182: [15068  8198 11062 17350  6347 23952  7031  6885 24701 29434]\n",
            "Top recommendations for user 387: [23952 15068 24161 20140 18724  6885  5974  7031   640  7150]\n",
            "Top recommendations for user 242: [11062 17350 29434  4615 15068  3479 10894 22215 28162  6885]\n",
            "Top recommendations for user 559: [28028 17350  8198 11062 29434   396 23825 25334 24745  1059]\n",
            "Top recommendations for user 808: [ 4615 28162 11062 29434  3847  6492  3479  6885 24074 22935]\n",
            "Top recommendations for user 286: [ 4615 28162  3847 24074  7205  6492  9086  4694 29434  2126]\n",
            "Top recommendations for user 1318: [24907  7031  6885 17350 13685  6347 30258  2003 24161 24220]\n",
            "Top recommendations for user 1171: [ 8198 15068  4097 24160 13881 20487 23997 20931  1451 24243]\n",
            "Top recommendations for user 659: [ 8198 15068 23952 24161 24160 25982 23997 28028  6347  4097]\n",
            "Top recommendations for user 452: [27968 24907 23395 12012   720 26196  9223 19665  2858 18332]\n",
            "Top recommendations for user 909: [24243 23616   396 18960 24074 10931 22755 24701  4723 28028]\n",
            "Top recommendations for user 936: [11062  1449 25568 29434  6492  4385 23678 12300  7988 20630]\n",
            "Top recommendations for user 148: [28028 17350  6903 11062  8198  1059 26638  7316  1634 15203]\n",
            "Top recommendations for user 1641: [24907 17350  7031  6347  2003 28028 19665 20836 21134 13685]\n",
            "Top recommendations for user 173: [15068 17350  8198 28028 11062 21134 25982 24701 23952  6347]\n",
            "Top recommendations for user 145: [17350 11062 15068 21134  6347  7031  2003 28028 17524 24701]\n",
            "Top recommendations for user 1686: [17350 11062 24701  6347  4615 15068 18960 29434 25334 21134]\n",
            "Top recommendations for user 27: [10207 19682  3007 19727 10527  9348  3477  6885 13064 11463]\n",
            "Top recommendations for user 1560: [15068  8198 11062 17350 20487 23952  4385 15203 24160 29434]\n",
            "Top recommendations for user 733: [11062 28162 17350 29434  4615 17524  3479 24745  5486  2805]\n",
            "Top recommendations for user 1668: [17350 11062  6347  4615  3479  7031  2003 15068 21134 10894]\n",
            "Top recommendations for user 527: [17350 21134 28028  6347  7031 15068 23952  6903  2003 17524]\n",
            "Top recommendations for user 451: [ 8198 15068 17350  6347 23997  4615 11062  7031  4097 24161]\n",
            "Top recommendations for user 882: [11062 15068  8198 17350 29434  3321 28028 22215  7316 15203]\n",
            "Top recommendations for user 524: [ 8198 15068 11062 17350 29434 25334 25982 24701 24160 15203]\n",
            "Top recommendations for user 977: [11062 17350  2245 17524 10894  5486  3479  6885 22215 21058]\n",
            "Top recommendations for user 1204: [29434 28162 11062  4615  6492 18960 25334 24701 24074 22755]\n",
            "Top recommendations for user 39: [24701 21134 18960 22124 28028 17350 25982  6347 25334   396]\n",
            "Top recommendations for user 446: [ 6903 28028 26638 21134 17350 23952 17524 24701 22124   725]\n",
            "Top recommendations for user 634: [17350  4615 24907  6347 20836 18960  2003 24220  7031  9086]\n",
            "Top recommendations for user 458: [15068  8198 11062 20487  3710 13055 10807 26529 14504  6181]\n",
            "Top recommendations for user 1494: [17350 28028 24701  6347 18960 21134 11062 25334 25982 24745]\n",
            "Top recommendations for user 1271: [  396 18960 23616 22124 24745  4723 22755 24243 24074 24701]\n",
            "Top recommendations for user 796: [ 8198 11062 15068 29434 20487  4097 20931  1451  4615 17350]\n",
            "Top recommendations for user 1060: [17350 18960 24745   396 17524 24701  7890 24835  4723  6347]\n",
            "Top recommendations for user 394: [17350 15068 11062  7031  6347  2003 28028 21134  8198 24907]\n",
            "Top recommendations for user 1011: [11062 17350  8198 15068 29434  4615  6347  3479 28028 20931]\n",
            "Top recommendations for user 1027: [17350 15068 11062 21134 17524  2245 24701  6903  7316 23952]\n",
            "Top recommendations for user 1642: [28028 26638  6903 17350   396 20985 24160 24243  1059 25982]\n",
            "Top recommendations for user 12: [28028 17350   396  6347 18960 24243 22124 24701 25982 21134]\n",
            "Top recommendations for user 777: [24907 17350  7031 19665 22215  2003 20836  7316  6885  5486]\n",
            "Top recommendations for user 1814: [17350  6347 21134 11062 24701  7031 18960  2003 15068 17524]\n",
            "Top recommendations for user 1792: [17350 11062 24701 18960  6347 17524 21134  4615 25334 29434]\n",
            "Top recommendations for user 1508: [ 6347  4615 10199  7031 24907 24074 18960 24220 22124 17350]\n",
            "Top recommendations for user 405: [ 8198 15068  4097 20487 11062  4385 16706  4615 20140 24074]\n",
            "Top recommendations for user 1672: [28028 17350  6903 21134  6347   396  2003  7031 22124 26638]\n",
            "Top recommendations for user 1370: [ 7150 23952 21134   640 22124 29178  3404 28172 24701 19682]\n",
            "Top recommendations for user 1400: [ 4615 24907 20836  2126 10380  5472  9086 17350 28162  2553]\n",
            "Top recommendations for user 1304: [28028  8198 17350 24160   396 25982 24243 20985  6347 25334]\n",
            "Top recommendations for user 245: [25982 24701 28028 23952 24160 21134 24243 15068 25334  6347]\n",
            "Top recommendations for user 1557: [ 8198 15068 17350 28028  6347 24160 11062 25982  7031 23952]\n",
            "Top recommendations for user 305: [11268  6885 19682  3477  3007 10527 20775 23211  8275  3847]\n",
            "Top recommendations for user 672: [26638 28028  6903 15068 15203 21134 23952 25982 24701 17350]\n",
            "Top recommendations for user 398: [ 8198 15068 11062 29434  4097 20487  4615 20931  1451 28162]\n",
            "Top recommendations for user 1681: [11062 17350 17524 29434  3479 28162  5486 24701  2245  4615]\n",
            "Top recommendations for user 1496: [ 8198 15068 11062 28028 17350 20487 24160  1451 20931 23997]\n",
            "Top recommendations for user 1396: [28028 26638 15068  6903  8198 17350 23952 20985 20722  3321]\n",
            "Top recommendations for user 417: [17350 11062  3479 24701  4615  6347 21134 17524  2245 18960]\n",
            "Top recommendations for user 505: [24701  8198 25334 17350 11062 15068 25982 18960  6347 29434]\n",
            "Top recommendations for user 1829: [ 8198 28028 15068 17350 15203 24160 11062 25982 28213 25334]\n",
            "Top recommendations for user 704: [ 8975 23662 21058 10708 15203 11062 27931 13055 27569  1032]\n",
            "Top recommendations for user 1025: [28028  6903 26638 24907 20985 17350 18739  1059  9536 20722]\n",
            "Top recommendations for user 538: [17350  6903 17524  2003 28028 10380  7316 21134  6347  5486]\n",
            "Top recommendations for user 1137: [24907  6885  7031  2627 30258 13685  4651 11187 22215 20731]\n",
            "Top recommendations for user 161: [ 6885 17350  6347  7031  4615 18960 21134  2003  3404 22124]\n",
            "Top recommendations for user 870: [17350 24907  6347  4615 20836 28028  2003   396 18960 24220]\n",
            "Top recommendations for user 1852: [24701 27931  7150  7156 25334 28172  8975 16706 25982 15068]\n",
            "Top recommendations for user 577: [ 6885 18960  6347  3404 16706 24701 21134 15068  4615 17350]\n",
            "Top recommendations for user 1779: [15068  8198 23952 20487 24160 18724 11062 15203  4385 24594]\n",
            "Top recommendations for user 1103: [17350 24907 22215  2003  7031  7316 11062 20836  6903 19665]\n",
            "Top recommendations for user 1758: [17350 11062 17524  4615  3479 29434  2003  6347 28162 24701]\n",
            "Top recommendations for user 1799: [ 8198  1451  2126 13881  9962 20466  8663 23825 28213 28796]\n",
            "Top recommendations for user 1364: [28162 29434  4615 11062 17350  3479  2805  4723  2553 22935]\n",
            "Top recommendations for user 176: [11062 17350  8198 15068 29434 28028  7316  3479  4615  6347]\n",
            "Top recommendations for user 927: [15068  6885 23952 21134 14504  7031   640  7150 24161  3404]\n",
            "Top recommendations for user 1314: [ 8198 15068 11062 29434  4097 20931 17350  4615 23997 20487]\n",
            "Top recommendations for user 162: [21134 17350  6347 22124   640 23952  7031 28028 24161 25982]\n",
            "Top recommendations for user 1146: [ 4615 28162  3847  2126 29434 20836  2553  1277  3479  7205]\n",
            "Top recommendations for user 261: [17350 11062  4615  6347  3479  2003 29434 17524 20836 28028]\n",
            "Top recommendations for user 1259: [11062 29434 28162 23825 17328 17350 11396  1634  1449 15290]\n",
            "Top recommendations for user 1853: [28028  8198 26638 20985 24160 15068 17350 20722  6903 15203]\n",
            "Top recommendations for user 1573: [17350 28028  8198 11062 29434  6903 23825  1059  7316 15290]\n",
            "Top recommendations for user 1825: [17350 28028 11062  8198 24701 29434 25334 15068  6347 25982]\n",
            "Top recommendations for user 395: [17350 28028  6347  7031  8198  2003 24907 21134 23997 11062]\n",
            "Top recommendations for user 1730: [11062 22215 17350  7316 12128 18332  5486 26529 15068 10894]\n",
            "Top recommendations for user 967: [17350 28028 24701 21134 25982  6347 25334 11062 15068 17524]\n",
            "Top recommendations for user 1432: [17350 15068 11062  6885  7031 21134  2245  6347 10894 22215]\n",
            "Top recommendations for user 219: [11062  8198 15068 24701 17350 25334 29434 25982  6347 16706]\n",
            "Top recommendations for user 1693: [11062 17350 29434  8198 28028 23825 28162  7316 20931  1059]\n",
            "Top recommendations for user 1537: [10380 10495   396 27345  5472 24907 17350 10710 17504   583]\n",
            "Top recommendations for user 1270: [11062 29434 17350 28162  3479  4615 17524  5486 22215 10894]\n",
            "Top recommendations for user 52: [ 6885 14504 15068 18724  2245 25401 24621  7031 11158 23952]\n",
            "Top recommendations for user 618: [29434  4572 25334 24701  8198 27931 22755  1449 24074 28162]\n",
            "Top recommendations for user 681: [ 8198 15068 11062 20487 13055 26529  3710 29434 28213 15203]\n",
            "Top recommendations for user 788: [15068  6885 23952  7031 17350 21134 14504  2245 24161 18724]\n",
            "Top recommendations for user 1816: [ 4615  3847 28162 11062  6885 29434  4694 10807 12581  3479]\n",
            "Top recommendations for user 441: [15068  8198 20487  6885  4385 20140  4097 18724  7988 23952]\n",
            "Top recommendations for user 636: [ 4615 28162 29434  6492 11062 24074  3847  4694 22935  4097]\n",
            "Top recommendations for user 359: [17350 21134 28028  6347  7031  6903  2003 17524 24701 23952]\n",
            "Top recommendations for user 1378: [24074 23616 10931  3666 22755  2267  9886 24243 29634 18960]\n",
            "Top recommendations for user 600: [17350  4615 10380  2003 20836  3479 11062 17524  6347 28162]\n",
            "Top recommendations for user 1187: [ 4615 17350 29434 11062 28162 20836 20931  6347  3479  2126]\n",
            "Top recommendations for user 847: [28028 13881 24907 26638  9536 20985 11533 11389  6903   396]\n",
            "Top recommendations for user 1724: [11062 15068 22215  6885 10894 14504  2245 17350 18332 26529]\n",
            "Top recommendations for user 517: [15068 11062  8198 17350 24701 21134 15203 25334 25982 23952]\n",
            "Top recommendations for user 1564: [28028 17350   396 24907  6347 22124 10495 11533 24220 18960]\n",
            "Top recommendations for user 1337: [15068  8198 11062 17350  7031  6347  6885 20487 23952  3321]\n",
            "Top recommendations for user 1360: [24907  9962 11187  4121 20731 14397  2627   265  7553 30258]\n",
            "Top recommendations for user 1489: [28028  8198 24160 13881 24243 25982 20985 23997 24161 15068]\n",
            "Top recommendations for user 224: [17350  6347 28028 18960 21134 24701 22124 25982  7031 24220]\n",
            "Top recommendations for user 9: [15068 11062  8198 17350 23952 24701 21134  2245 25982 15203]\n",
            "Top recommendations for user 764: [17350 11062 28028  8198  6903 15203 15068 29434  7316  1634]\n",
            "Top recommendations for user 512: [11062 29434  4615 28162 15068  8198  3479 10807  6885  1449]\n",
            "Top recommendations for user 25: [28368 16053  4651 26227 28823 20360 24907  5864 27709  5974]\n",
            "Top recommendations for user 1305: [24907  4615 17350 20836  6347  7031  2003  9086 24220  3479]\n",
            "Top recommendations for user 651: [23952  7031 21134   640 17350 19665 24907 24161  6347 23395]\n",
            "Top recommendations for user 650: [15068 11062 17350  8198  6347 21134 23952 24701  2245 25982]\n",
            "Top recommendations for user 1841: [ 6903 12128 18332 29288  4805 19691  9783  8975  4566 18103]\n",
            "Top recommendations for user 1163: [11062 29434 28162  8198  4615  1449 23825 20931  3479 15068]\n",
            "Top recommendations for user 729: [ 6903 28028 10380 17350  1634 15290 23825 11396  1059 17887]\n",
            "Top recommendations for user 257: [11062 17350 29434  4615  3479 28162 24701 17524  2245 10894]\n",
            "Top recommendations for user 1697: [18960 22124 17350 21134  6347 15463 24701 17524 12434  3404]\n",
            "Top recommendations for user 533: [17350 28028  8198 11062 29434  6347  4615 20931 23825 25334]\n",
            "Top recommendations for user 551: [17350  8198 15068 28028  6347 11062 25982 24160 21134  7031]\n",
            "Top recommendations for user 758: [ 4615 24074 18960 28162   396 23616 22755  6492  4723 29434]\n",
            "Top recommendations for user 1023: [24907  6885 22215 19665  2627  7031 11268 17350 18332 30362]\n",
            "Top recommendations for user 1436: [24074 24243 16706 18960  4097  6492 23616  8198 22755 25334]\n",
            "Top recommendations for user 922: [11062 17350 29434  8198 23825 28162 28028  3479  4615  7316]\n",
            "Top recommendations for user 508: [ 8975 11062 24701 21058 27569 27931 17524  7890 28608 24745]\n",
            "Top recommendations for user 13: [ 8198 28028 11062 29434 17350 23825 28213 15068 20931  1451]\n",
            "Top recommendations for user 1857: [15068 11062 17350  8198 23952  2245  6885 21134 14504 10894]\n",
            "Top recommendations for user 1339: [ 8198 15203 11062 27931 25334 24701 28213 15068 29434 23662]\n",
            "Top recommendations for user 1234: [11062 17350 24701 29434  4615  3479 18960 25334 28162  6347]\n",
            "Top recommendations for user 1104: [22124   396 18960 24701 24745 28028 21134 19187 17350 24243]\n",
            "Top recommendations for user 992: [ 4615 24907  9086  6885  6347  7031 23997 20836  3847  3479]\n",
            "Top recommendations for user 1260: [11062 17350 15068 22215  6885  3479 10894 29434  2245  4615]\n",
            "Top recommendations for user 1014: [ 4723   396 29434 24745 28162 25334 11396 28028 23825 24701]\n",
            "Top recommendations for user 103: [ 6903 17350 19665 22544 24907 21134  2003 28028  7031 17524]\n",
            "Top recommendations for user 954: [15068 11062 24701 17350 21134  2245 25334 23952 25982 17524]\n",
            "Top recommendations for user 1310: [15068  8198 11062 22215 26529  3321  3016 18332 20487  7316]\n",
            "Top recommendations for user 973: [22124 18960   846  6655 24701  7920   299 29312 13546 27931]\n",
            "Top recommendations for user 338: [17350 28028  4615 20836 10380  6347  2003   396 24907 11062]\n",
            "Top recommendations for user 661: [24907 17350  7031  2003 19665  6347 20836 28028  6903 10380]\n",
            "Top recommendations for user 770: [ 8198 11062 15068 29434 20487 17350 20931  4097  4615 15203]\n",
            "Top recommendations for user 1125: [24701 25334 18960 28028 25982 17350 24243   396  6347  8198]\n",
            "Top recommendations for user 2: [24243 22124  6347 24161 10199   640 23952 24074 18960 24220]\n",
            "Top recommendations for user 1611: [ 8198 20487  1451 16697 14397 11978  4097 17867  9962 13771]\n",
            "Top recommendations for user 625: [17350  7031  2003 24907 21134  6347 19665  7316  6903 22215]\n",
            "Top recommendations for user 76: [ 8198 28028 15068 24160 25982 17350 23952 24701 25334  6347]\n",
            "Top recommendations for user 895: [24161 23952 15068  7031  8198  6347 24160 28028 24907 23997]\n",
            "Top recommendations for user 1735: [21134 22124 17350 17524 18960  3404   640  6347 22544 24701]\n",
            "Top recommendations for user 84: [11062 15068  8198 24701 17350 25334 29434 25982  4385  6347]\n",
            "Top recommendations for user 1806: [15068  8198 11062 29434  4385 24701 25334 16706 25982 17350]\n",
            "Top recommendations for user 931: [28162 29434  4615 11062  1449  3479  4723 23825 20931 17350]\n",
            "Top recommendations for user 1524: [17350 11062 15068  4615  7031  6885 24907  8198  6347  3479]\n",
            "Top recommendations for user 849: [11062  8198 29434 15068 15203  1449 28213 25334 24701 13055]\n",
            "Top recommendations for user 1349: [22124 18960  6347 23616 24243   396 24220 24907 10199 10495]\n",
            "Top recommendations for user 1629: [17350  4615 18960  6347  3479 21134 11062 24701  3404  6885]\n",
            "Top recommendations for user 199: [21058  8975 17571 12300 27801  2128 15463  5861  9392  6492]\n",
            "Top recommendations for user 752: [24701 15068 23952 16706 18960 25982 21134  6347 22124   640]\n",
            "Top recommendations for user 301: [17350 21134 15068 17524  6903  7031  2245 23952  2003  6347]\n",
            "Top recommendations for user 1757: [29434 28162 11062  4615  1449 20555 25334 20931  6492  3479]\n",
            "Top recommendations for user 1637: [29434 11062 24701 18960  4615 25334 28162 17350 24074 16706]\n",
            "Top recommendations for user 946: [13881  4651  9962  8198 24907  2404 25596 24161 19680 24320]\n",
            "Top recommendations for user 678: [11062 29434 17350 15068 24701 25334  8198  4615  3479 28162]\n",
            "Top recommendations for user 1028: [ 8198 17350 24907  2126 28028 11062 20466 20836 20731 23997]\n",
            "Top recommendations for user 63: [27931 12300 23678 24701  6492 25568  1449  4572  4385  2128]\n",
            "Top recommendations for user 1269: [17350 29434 11062 28162  4615  3479 24745  4723   396 18960]\n",
            "Top recommendations for user 1614: [17350  4615  6347  3479  2003  7031 11062 24907 20836 18960]\n",
            "Top recommendations for user 1073: [17350 28028 15068  8198 21134 25982  6347 23952 24160 24701]\n",
            "Top recommendations for user 900: [11062 15068 17350 24701  8198 21134  2245 25334 29434  6347]\n",
            "Top recommendations for user 666: [ 8198  4097  1451 20487 13881 20931 29434 19713 24074 15068]\n",
            "Top recommendations for user 401: [28028 24701 21134 17350 26638 25982 25334 24745 27931  6903]\n",
            "Top recommendations for user 1617: [28028 24243   396  8198 24160 25982 13881 23616 25334 17350]\n",
            "Top recommendations for user 952: [15068 23952 21134 17350  7031  6347   640 24701  6885 24161]\n",
            "Top recommendations for user 677: [17350  6903 21134 28028 17524  2003 24701  7890  6347 24745]\n",
            "Top recommendations for user 54: [ 4615 28162 29434 11062  2126  3847  1277  3479 20931  7205]\n",
            "Top recommendations for user 392: [18960 24701 17350 25334  6347   396  4615 29434 24074 25982]\n",
            "Top recommendations for user 807: [24907 17350 10380 20836  4615  6347  2003 28028 11533  7031]\n",
            "Top recommendations for user 1351: [11062 29434  4615  8198 17350 15068 24701 25334 28162  3479]\n",
            "Top recommendations for user 836: [28162 29434 11062  4615  1449  6492 22935  3479 20555  3847]\n",
            "Top recommendations for user 454: [28028 17350  6347 18960 22124 24243 24701 25982   396 21134]\n",
            "Top recommendations for user 1849: [15068 26638 23952 18332  9223  6903  2858 23395 27968 14504]\n",
            "Top recommendations for user 994: [ 6885 11062 15068 14504  2245 10894 17350  3479  4615 22215]\n",
            "Top recommendations for user 1374: [11062 17350 17524  6903  8975  2245  7890 15068 12128 21134]\n",
            "Top recommendations for user 611: [15068 11062 24701  4385 16706  2245  8198 25334  6885 21134]\n",
            "Top recommendations for user 846: [24701 11062 15068 17350 18960  6347 25334 16706  4615 21134]\n",
            "Top recommendations for user 1585: [17350 28028  6347  6903 11062  2003  1059  8198   396 21134]\n",
            "Top recommendations for user 1780: [ 5472  7453 17504  4615 11268 10710  1820 10380  2553  6440]\n",
            "Top recommendations for user 773: [17350 28028  6347  2003 24907  7031 21134  6903 11062 20836]\n",
            "Top recommendations for user 1000: [ 4615 24074 17350  6347 29434 11062 18960  3479 28162 23997]\n",
            "Top recommendations for user 1: [24074  4615  4097  8198 20931 29434 24243 23997 18960 13881]\n",
            "Top recommendations for user 256: [17350 28162 29434  4615   396  4723  2126 10380 23825 28028]\n",
            "Top recommendations for user 1145: [17350  4615 11062 29434  6347  3479 28162 18960 20836 24701]\n",
            "Top recommendations for user 1559: [17350 11062 28028 21134  6347 24701 15068 17524  2003  7031]\n",
            "Top recommendations for user 1488: [17350  6347 15068 28028 24701  8198 25982 21134 23952 24160]\n",
            "Top recommendations for user 539: [17350 28028 10380 29434   396 11062  4615 20836 23825 28162]\n",
            "Top recommendations for user 33: [17350  7031 11062  2003  6347 21134 15068 24907 17524  7316]\n",
            "Top recommendations for user 1623: [11062 17350 15068  8198 24701 29434  2245 21134 25334  3479]\n",
            "Top recommendations for user 372: [15068  8198 11062 17350 28028 15203 29434 24701  3321 25334]\n",
            "Top recommendations for user 1591: [11062 17350 29434 24701 25334 28162  4615 17524  3479 24745]\n",
            "Top recommendations for user 1453: [28028  8198 17350   396 24160 15203 25334 28213 25982 23825]\n",
            "Top recommendations for user 1223: [ 6885 25401  4615  3847 19682  3631  3477  4694  2627 11268]\n",
            "Top recommendations for user 1502: [17350  6347 15068 21134 18960 24701  7031 11062  4615 25982]\n",
            "Top recommendations for user 1837: [28028 24243  8198 24160 13881 25982 26638  1826 23616   396]\n",
            "Top recommendations for user 466: [24907  7031 17350  6347 13685 19665 24161  2003 24220 23952]\n",
            "Top recommendations for user 1810: [11062 15068 17350 29434  2245  3479 10894  8198  4615 24701]\n",
            "Top recommendations for user 1860: [15068 23952 17350  6885 21134  7031  2245 11062 14504  6347]\n",
            "Top recommendations for user 620: [15068 23952 21134 17350  2245  7031 18332 14504  6903  6885]\n",
            "Top recommendations for user 1705: [11062 17350 24701 29434 17524 25334 24745 18960 28162  3479]\n",
            "Top recommendations for user 1741: [17350  6903 19665 18332 17524 21134 22215  7031  6885  5486]\n",
            "Top recommendations for user 1221: [ 8198 11062 29434 23825 17350 28213 28028 15203 15290 15068]\n",
            "Top recommendations for user 1781: [11062 15068 17350  8198  2245  7316 29434 22215 15203 17524]\n",
            "Top recommendations for user 96: [23952 24161 15068  6347  7031   640 21134 22124 10199 17350]\n",
            "Top recommendations for user 748: [15068  8198 23952  6885 18724 20487 14504 11062 24161  4385]\n",
            "Top recommendations for user 320: [15068  8198 18724 20487  6885 24161 24907  7031  4651 23952]\n",
            "Top recommendations for user 635: [24701 25334 29434  8198 18960   396 24745  4723 25982 11062]\n",
            "Top recommendations for user 468: [17350 24907 28028  7031  8198  6347 15068 23997 24161 20985]\n",
            "Top recommendations for user 1334: [25978 22215 26529  3016 18332 29123 15552  2858 15068  7090]\n",
            "Top recommendations for user 1819: [ 8198 15068  1451 24160 28213 28028 15203  4097 20931 29434]\n",
            "Top recommendations for user 898: [ 7031   640 23952 22124 24161  6885 24907 28368  6347 10199]\n",
            "Top recommendations for user 1273: [15068 11062  3710 14504 10807  8975  6885 13055 26529  8198]\n",
            "Top recommendations for user 1543: [24907 10380 11268 27871  5472 16728  1820 11029 20836 16108]\n",
            "Top recommendations for user 1787: [14397 11978  8198 20487 19680  4097 11187 16697  4651 28782]\n",
            "Top recommendations for user 890: [28028   396 17350 12858  6903  1059  6347 24243 20985  4216]\n",
            "Top recommendations for user 825: [29434 23825  2126 17350 28162 11062 10380  4615 28028 11396]\n",
            "Top recommendations for user 1733: [ 4723 12533 11396 23825  2126   396 10437 28162 29434  8663]\n",
            "Top recommendations for user 1856: [27931 24701  7150 25334   846 18960  4572 27801 23818 25982]\n",
            "Top recommendations for user 328: [19665 23395  7031 28987 23952   640 24907  7105 21134 28368]\n",
            "Top recommendations for user 81: [ 6347 24161 23952   640 22124  7031 10199 24220 18960 20140]\n",
            "Top recommendations for user 1225: [ 4615 17350 10380 28162 20836  2126 29434  2553 11062  3479]\n",
            "Top recommendations for user 940: [17350  4615 11062  6347 15068  3479  7031 29434  8198  6885]\n",
            "Top recommendations for user 1045: [15068 24701  7150 23952 16706 25982 29178  4385 25334 18960]\n",
            "Top recommendations for user 641: [24243 24074 22124 23616 10199  5238 27039 18960  6729 25295]\n",
            "Top recommendations for user 23: [17350 28028   396 10380 20836  4615  2126  6347 12858 26024]\n",
            "Top recommendations for user 349: [26638  8198 28028 15203 28213  2319  6903  9398 27931 15068]\n",
            "Top recommendations for user 1058: [24701  4723 24745 29434 25334 11062 27931 28162 29907 18960]\n",
            "Top recommendations for user 1770: [28028 17350 24907   396 22124  6347  6903 10495 11533 10049]\n",
            "Top recommendations for user 1654: [17350 11062 15068 28028 21134 17524  2003  6347  7316  6903]\n",
            "Top recommendations for user 731: [ 8198 17350 15068 11062 28028  6347 25982 24701 24160 25334]\n",
            "Top recommendations for user 629: [ 2805  4723 28162  7442   380 20751  2553 20345 24835 17328]\n",
            "Top recommendations for user 332: [ 8198 17350 11062 15068 28028 29434 24701 25334  6347 25982]\n",
            "Top recommendations for user 353: [17350 28028 24701 21134 15068  6347 25982 11062 25334  8198]\n",
            "Top recommendations for user 1052: [18960 22124  6347   396 24074 15463 25324  2812 17350  7920]\n",
            "Top recommendations for user 703: [24701 21134 18960 17350 25982 25334 22124  6347 23952 16706]\n",
            "Top recommendations for user 130: [11062 28028  8198 24701 17350 25334 15203 24745 29434 25982]\n",
            "Top recommendations for user 179: [ 8198 25982 24701 24243 25334 24160 28028 15068 16706 15203]\n",
            "Top recommendations for user 834: [15068  8198 17350 11062  6347 24701 25982 25334 28028 23952]\n",
            "Top recommendations for user 1389: [ 8975 21058 11062 27569  7890 28608 27931 23662  7613 27801]\n",
            "Top recommendations for user 809: [17350 28028  6347 21134  2003  6903  7031 11062 17524 24701]\n",
            "Top recommendations for user 789: [17350  6347 28028 18960 24701   396 21134 22124 25982 25334]\n",
            "Top recommendations for user 1026: [11062  4615 17350 29434  8198 15068  6347  3479 24701 28162]\n",
            "Top recommendations for user 588: [11062 17350  3479  4615  6885 10894  2245 29434 15068 22215]\n",
            "Top recommendations for user 1403: [18960 15463 17350 17524 24701 24745 10527  4615  2103  3479]\n",
            "Top recommendations for user 69: [17350  4615  6347 23997 18960 24074 24220 20836  8198  9086]\n",
            "Top recommendations for user 207: [15068 23952  8198 24161  7031 28028 26638 18724 24160 21134]\n",
            "Top recommendations for user 1726: [28028  6903 17350 26638  1059 21134  7316  2003  7031 20985]\n",
            "Top recommendations for user 1688: [11062  8198 15068 17350 29434 24701 25334 15203 28028 25982]\n",
            "Top recommendations for user 548: [17350 15068  6347 28028  8198 25982 23952 21134 24701 24160]\n",
            "Top recommendations for user 657: [17350 22124  6347 28028 18960 21134   640  7031   396 24220]\n",
            "Top recommendations for user 1138: [17350  6347 28028 24701 11062  8198 25334 25982 18960 15068]\n",
            "Top recommendations for user 679: [17350  2003  7031  6347  3479  4615 24907 11062 17524 21134]\n",
            "Top recommendations for user 1474: [18960 22124   396 24701 24243 23616 24745  6347 24074 25334]\n",
            "Top recommendations for user 402: [17350  4615  6347 11062 18960 24701 29434 25334  3479 16706]\n",
            "Top recommendations for user 660: [ 7031 19665 24907  6885 21134   640 17350 28987  7105 24621]\n",
            "Top recommendations for user 1503: [11062 24701 17350 25334 18960 17524 29434 21134 24745  2245]\n",
            "Top recommendations for user 1439: [24243  2842 27303  9886 24074 13881 25295 25468 10738 24055]\n",
            "Top recommendations for user 1622: [17350  2003 19665  6347  7031 24907 21134 17524 22544 22124]\n",
            "Top recommendations for user 917: [24907  4615  6347 20836  7031 17350  9086 11268 24220 17504]\n",
            "Top recommendations for user 1759: [ 8198 23825  2126 28028  1451 28213 29434 20466 15290 20931]\n",
            "Top recommendations for user 570: [15068  8198 11062 17350 15203 24701 25982 25334 23952  2245]\n",
            "Top recommendations for user 1287: [ 8198 24907 15068  4651 23997 30258 11187 20487  4097  9086]\n",
            "Top recommendations for user 1581: [ 8198 28028 17350 15068 11062 24160 25982 25334 24701 29434]\n",
            "Top recommendations for user 941: [17350  6347 11062 24701 28028 18960 21134 25334  4615  2003]\n",
            "Top recommendations for user 217: [ 4615 17350 10380 24835  2553  5486 28162 10527  3479  5472]\n",
            "Top recommendations for user 871: [17350  6903 10380  2003 24907 28028  6347 20836 17524 19665]\n",
            "Top recommendations for user 1545: [ 6903  7890 17524 12128  5486 28608 18103 17350 27569  8975]\n",
            "Top recommendations for user 1098: [ 8198 15068  1451 20487  4097 11062 29434 20931 13881 23997]\n",
            "Top recommendations for user 90: [11062 17350 17524  3479  5486 28162 29434  4615 24701 10894]\n",
            "Top recommendations for user 355: [28028   396 17350 24907 13881  6903  1059 20985 10380 11533]\n",
            "Top recommendations for user 326: [15068 11062  8198  6885 20487 29434 10807  4385 14504 19336]\n",
            "Top recommendations for user 1067: [17350  6903  2003 21134  7031  6347 24907 28028 19665 17524]\n",
            "Top recommendations for user 1054: [28028 23952 15068 25982 21134 24701 24160 17350  8198 26638]\n",
            "Top recommendations for user 1515: [28028 17350 26638  6903  8198 20985   396 25982 24160  1059]\n",
            "Top recommendations for user 1346: [17350  8198 11062 15068 28028 29434  7316  3321 15203  6347]\n",
            "Top recommendations for user 1472: [11062  4615 29434 17350 28162  8198  3479 20931 15068  6347]\n",
            "Top recommendations for user 448: [15068 17350 11062 21134  6347  8198 24701  7031  2245 23952]\n",
            "Top recommendations for user 423: [ 8198 11062 15068 29434 25334 24701 15203  4385 27931  1449]\n",
            "Top recommendations for user 1289: [23952 21134 15068   640 28028  7150 25982 24161  7031 17350]\n",
            "Top recommendations for user 639: [24701 18960 25334 25982 24243  6347 17350 16706   396 24745]\n",
            "Top recommendations for user 621: [10527  5486 24835 17524  2805  2103 15463 17350  3479 20314]\n",
            "Top recommendations for user 247: [24701 17350 21134 15068 25982 25334 28028 11062 23952  6347]\n",
            "Top recommendations for user 1185: [ 8198  9962  2404  1451 13881 20487 15068  1678 16697 24022]\n",
            "Top recommendations for user 416: [ 4615 11187  4097  3847  9086  7205  4561 14397 29434  8198]\n",
            "Top recommendations for user 1042: [ 8198 15068 17350 24160 28028  6347 25982 23997 24161 23952]\n",
            "Top recommendations for user 1021: [28028  8198   396 24160 17350 25982 24243 25334 24701 26638]\n",
            "Top recommendations for user 956: [17350  6903 17524 21134  2003 19665  5486 22544  7316  7031]\n",
            "Top recommendations for user 1175: [11062  8198 15068 17350 29434 28028 15203 24701 25334 25982]\n",
            "Top recommendations for user 1355: [26638 28028  1826 24243 23952  5238 29342 28368 22124   640]\n",
            "Top recommendations for user 627: [18960  4615 17350  6347 24074   396 22124 24701 28162 20836]\n",
            "Top recommendations for user 599: [17350 28028  6903 10380 24907  2003  1059 20836  7316  6347]\n",
            "Top recommendations for user 626: [17350 24907  8198  4615 11062 20836 23997 15068  6347  7031]\n",
            "Top recommendations for user 638: [17350 15068 11062 24701  6347  8198 21134 25334 25982 18960]\n",
            "Top recommendations for user 1266: [11062 17350  8198 29434 15068 28028 24701 25334  6347  4615]\n",
            "Top recommendations for user 1180: [17350 28028 21134  6347 24701 25982 22124  7031 23952 18960]\n",
            "Top recommendations for user 955: [28028 24907 17350 20985  5238  6347 24160 13881 24161  7031]\n",
            "Top recommendations for user 37: [17350 11062  6347  4615  8198 15068 28028 23997  7031 29434]\n",
            "Top recommendations for user 1093: [17350 15068  8198 11062  6347  7031 28028 21134  2003  4615]\n",
            "Top recommendations for user 1102: [19665  6903 28987 18332 20527  7031 23395 15925 12128  2858]\n",
            "Top recommendations for user 883: [22544 17524 21134 14644 10207  7890 12434  6903 12250 27569]\n",
            "Top recommendations for user 1739: [  396 24243 23616  4723 10931 22755 28028 24074 18960  4572]\n",
            "Top recommendations for user 243: [24243 28368 22124 23616 14773 10931  5238   396  5478 27039]\n",
            "Top recommendations for user 1687: [11062 17350 29434 28162 24745 25334 28028 24701 23825  4723]\n",
            "Top recommendations for user 1729: [ 8975  8198 13055  7869 15068  3710 23662  7156  1449 12607]\n",
            "Top recommendations for user 745: [ 2126 29434  8198  4615 28162 20931 23825 11062  1451  4097]\n",
            "Top recommendations for user 1813: [ 6903 26638  7890 12128  8975 29650  1634 11062 17350 23807]\n",
            "Top recommendations for user 594: [ 8198 15068 17350 11062 28028 25982 24701 25334 24160  6347]\n",
            "Top recommendations for user 1177: [ 6903 21134 26638 17350 28028 23952 17524 24701   640  7890]\n",
            "Top recommendations for user 694: [15068  8198 11062 17350 24701 25982 25334 23952 28028 24160]\n",
            "Top recommendations for user 734: [ 8198 11062 17350 15068 28028 29434 15203 25334 24701 25982]\n",
            "Top recommendations for user 1535: [24907 17350  7031  8198 15068 28028 13685 20731 23997 20836]\n",
            "Top recommendations for user 1696: [ 8198 20487  1451  4097 14397 16697 15068 13881  9962 17867]\n",
            "Top recommendations for user 1709: [17350 21134 15068  6347  7031 24701 23952 11062  2003 17524]\n",
            "Top recommendations for user 1443: [28028 24243 24160  8198 25982 13881   396 26638 23616 20985]\n",
            "Top recommendations for user 1019: [24907 19665  6903 20527 26398  7031 17350 20840 25734 13888]\n",
            "Top recommendations for user 1319: [11062 17350  6885  3479  4615 10894 22215  2245  5486 29434]\n",
            "Top recommendations for user 205: [17350 11062 15068 21134  2245  7031  2003  6347 10894 22215]\n",
            "Top recommendations for user 1490: [11062 15068 17350 29434  8198  4615  3479 24701  2245  6347]\n",
            "Top recommendations for user 838: [ 8198 29434 11062 20931 23825  4615 17350 28162  2126 28028]\n",
            "Top recommendations for user 113: [19665 17350 24907 22544  2003  7031  6903 21134 17524  6347]\n",
            "Top recommendations for user 1659: [ 7205  2126  9886  9086  4615 29120 24074  7021 14397 13881]\n",
            "Top recommendations for user 595: [ 8198 15068  1451 20487  9962 28213  3321  5732 13881 20985]\n",
            "Top recommendations for user 1224: [17350 28028  8198 11062 15068 24701  6347 25334 25982 21134]\n",
            "Top recommendations for user 668: [28028 10495 24907   396 11533  5478   583 11389 22124 10064]\n",
            "Top recommendations for user 1437: [ 8198 15068 11062 20487  3321 15203  1451 28213 13055  8895]\n",
            "Top recommendations for user 303: [11062 17350 15068  2245  6885 10894  3479 21134  6347 24701]\n",
            "Top recommendations for user 1664: [ 8198 29434 17350 11062 28028 20931 25334  4615 24701  6347]\n",
            "Top recommendations for user 1112: [29434 28162  4615 11062 17350  4723 23825  3479 20931  2126]\n",
            "Top recommendations for user 337: [15068 24701 11062 25334 27931  8198 15203 25982  8975 21134]\n",
            "Top recommendations for user 579: [17350  6903 22215 11062  7316  5486 17524  2003 12128 10894]\n",
            "Top recommendations for user 518: [17350 24701 21134  6347 18960 25334 25982 11062 15068 16706]\n",
            "Top recommendations for user 49: [24907  7031 23952 24161 15068 23395  4651 18724 13685 27968]\n",
            "Top recommendations for user 1740: [17350 28028   396  6347 18960  4615 24701 24745 20836 25334]\n",
            "Top recommendations for user 1467: [11062  4615 17350  3479 29434 28162  6347 10894 18960  2003]\n",
            "Top recommendations for user 960: [29434 11062 28162  1449  4723 24745 17328 11396 25334 24701]\n",
            "Top recommendations for user 1442: [17350 21134  6347  2003  7031 22124 18960 17524 22544  3404]\n",
            "Top recommendations for user 1479: [11062 15068  8975  2245 14504  4385 24701  6885 21058  9392]\n",
            "Top recommendations for user 1483: [17350 28028 11062 29434 20836  4615  8198 10380  2003 23825]\n",
            "Top recommendations for user 122: [28028 17350  6347   396 22124 18960 21134 24220 24907 24243]\n",
            "Top recommendations for user 294: [ 8198  4723 28213 11396 28028  9398 27931 15203   396 24243]\n",
            "Top recommendations for user 1516: [28028 17350  6903  8198  1059 11062 26638  7316 20985 15068]\n",
            "Top recommendations for user 767: [17350 28028  6347 15068 25982 21134 24701 23952  8198 24160]\n",
            "Top recommendations for user 73: [28028 17350 26638 15068 21134  6903 25982 24701  8198 23952]\n",
            "Top recommendations for user 835: [17350  8198 11062 15068 24701 28028 25334 29434 25982  6347]\n",
            "Top recommendations for user 1580: [17350 28028  6347   396 18960 25982 24701 21134 23997 24220]\n",
            "Top recommendations for user 429: [ 8198  4097 13881 23997 20931  2126  9086  4615  1451 24074]\n",
            "Top recommendations for user 236: [ 4615 29434 28162  9086 11062  4097 24074  3847  3479 20931]\n",
            "Top recommendations for user 1861: [18960 24745 22124 24701   396 21134 17524  7890 17350  2987]\n",
            "Top recommendations for user 1012: [10380  6903 28028 18655   396 17350 16019 22821 12858 12614]\n",
            "Top recommendations for user 692: [17350 28028  8198 15068 11062 24701 25982  6347 25334 21134]\n",
            "Top recommendations for user 1518: [ 8198 11062 15068 29434 17350 15203 25334 24701 28028 28213]\n",
            "Top recommendations for user 308: [ 4615 11062 17350  3479  6885 29434 28162 10894 22215  6347]\n",
            "Top recommendations for user 259: [17350  7031  6347  2003 11062 21134 24907 15068  6885  3479]\n",
            "Top recommendations for user 1603: [28028 24243   396 26638 24160 20985  5238 13881 17350 25982]\n",
            "Top recommendations for user 1597: [24701 27931 25334 24745 25982 29907   396  4572  4723 15203]\n",
            "Top recommendations for user 1295: [29434 11062 28162  4723 23825 11396  8198 25334  1449 24745]\n",
            "Top recommendations for user 779: [24907  4615  8198  9086 23997 20836 17350  2126 20731  4097]\n",
            "Top recommendations for user 433: [ 8198 15068 26638 28028 15203 24160 25982 23952  2319 27931]\n",
            "Top recommendations for user 530: [17350 11062 29434  4615 24701 25334  8198 18960  6347 28162]\n",
            "Top recommendations for user 210: [17350  6903 15068 11062 21134 17524  7316 28028  2245 12128]\n",
            "Top recommendations for user 38: [11062 17350  4615 29434 15068  6347  8198  3479 24701 25334]\n",
            "Top recommendations for user 507: [15068  8198  3710 13055 10807  7869 11062  1449 23678 19713]\n",
            "Top recommendations for user 1203: [17350 15068  8198 28028 11062  6347  7031 21134 25982 24701]\n",
            "Top recommendations for user 421: [24835 10380 22544  2805 27345 17350 10527  9468 18103 17524]\n",
            "Top recommendations for user 986: [11062 17350 17524 29434  6903  5486 22215  7316  7890  1634]\n",
            "Top recommendations for user 1384: [24701 25334 16706 18960 27931 25982  4572 11062  4385 15068]\n",
            "Top recommendations for user 563: [23952 21134   640 22124 28028 25982 15068  7150 17350  6347]\n",
            "Top recommendations for user 418: [28028   396 24243  8198 13881 23616 10931  4723 24160  2700]\n",
            "Top recommendations for user 1828: [  396 18960 17350 24745  4723 28028 24701 22124  6347 17524]\n",
            "Top recommendations for user 1530: [11062  6885  4615 15068 29434  3479 10894 17350 28162  2245]\n",
            "Top recommendations for user 598: [17350 15068 11062  7031  6885  2245 21134 10894  6347 22215]\n",
            "Top recommendations for user 950: [17350 11062 17524  2003 21134  7031  6347 22215  5486  3479]\n",
            "Top recommendations for user 439: [17350 11062 29434  8198 28028 25334 24701  4615 28162 24745]\n",
            "Top recommendations for user 132: [ 8198 15068 11062 15203 25982 25334 24701 24160 28028 17350]\n",
            "Top recommendations for user 1777: [ 6903 19665 28987 18332 12128 26638 29288 15925 19691  2086]\n",
            "Top recommendations for user 227: [ 8975 15203  8198 26638 28213 23662 13055 15068  2319 11062]\n",
            "Top recommendations for user 962: [11062  2245 29434 21058  8975 17524 17350 10894 24701  3479]\n",
            "Top recommendations for user 435: [28028 17350 24701 21134  6347 25982   396 25334 24745  6903]\n",
            "Top recommendations for user 329: [ 8198 15068 11062 29434  4385 25334 24701 20487  4097 25982]\n",
            "Top recommendations for user 762: [26638  7150 23835 27773 28172 27931 23952  2796 28028  6903]\n",
            "Top recommendations for user 1264: [11062  8198 17350 15068 29434  4615  3479  6347 28028 20931]\n",
            "Top recommendations for user 1291: [17350  6347 18960  4615 24701 21134  2003 11062  3479 22124]\n",
            "Top recommendations for user 747: [17350 11062 17524 29434  5486  3479  2245 22215  7316 10894]\n",
            "Top recommendations for user 157: [ 4615 17350 24907 11062  3479  6885 20836  6347  7031  9086]\n",
            "Top recommendations for user 1385: [15068 17350  6903  7031  7316 28028 18332 23952 21134 11062]\n",
            "Top recommendations for user 1297: [11062 29434  8198 17350  4615 28162 15068 20931 25334  3479]\n",
            "Top recommendations for user 1475: [11062  8198 29434 15068 17350  4615 28162  3479 20931 22215]\n",
            "Top recommendations for user 1048: [28028   396 17350  4615  2126  6347 20836 29434 18960  4723]\n",
            "Top recommendations for user 1261: [24701 18960 17350 25982 25334  6347 21134 16706 15068 28028]\n",
            "Top recommendations for user 743: [17350 11062  4615  6347 29434  3479  2003 28162 18960 20836]\n",
            "Top recommendations for user 1749: [15068  8198 21806 20487  7150  7156  3710 28172 23952  7869]\n",
            "Top recommendations for user 1520: [11062 29434  8975  1449 21058 15068  2245  9392 23662 28162]\n",
            "Top recommendations for user 1613: [ 6885 19682  7031 25401 24907 19665 27709  7105  7510 24621]\n",
            "Top recommendations for user 1219: [17350 28028 11062  8198 29434 24701 25334  6347 15068 25982]\n",
            "Top recommendations for user 233: [17350 11062 29434  4615 17524  3479 24701  6347  2003 28162]\n",
            "Top recommendations for user 531: [23952 15068 24161  6347   640  7031 25982 24160 21134 22124]\n",
            "Top recommendations for user 1420: [15068  8198 11062 23952 17350  2245 15203  3321 20487  4385]\n",
            "Top recommendations for user 1716: [23952 15068 28028 25982  8198 24160 21134 24701  7150 17350]\n",
            "Top recommendations for user 171: [ 8198 24243  4723 23616    45 12533   396  1451 13881 20931]\n",
            "Top recommendations for user 22: [18332 12128 22215 11062 15068 25978  6903 19691  2858  7316]\n",
            "Top recommendations for user 1531: [11062 22215 15068 17350  6885 10894  2245  5486 14504  3479]\n",
            "Top recommendations for user 622: [11062 29434  8198  4615 28162 15068  4468 22215 10807  1277]\n",
            "Top recommendations for user 574: [28028  8198 17350 15068 24160 25982 23952  6347 24701 20985]\n",
            "Top recommendations for user 605: [17350 22544 22124 19665  2003 12434 21134 18960 17524  6347]\n",
            "Top recommendations for user 362: [ 4615 28162 29434  3847 24074  3479  2126 11062  1277 20836]\n",
            "Top recommendations for user 506: [24074  9886  6492  4615 23616 25468  3666 24240 22755  4694]\n",
            "Top recommendations for user 631: [  640 23952  7031 21134 22124  6885  6347 24161 24621 28368]\n",
            "Top recommendations for user 449: [ 6885 14504 11062  8975  2245  9392 10807 21058 15068 24721]\n",
            "Top recommendations for user 1574: [ 8198  4097 20487 29434  1451 15068 11062 20931  4615  2126]\n",
            "Top recommendations for user 412: [18960  4615 24701 24074 17350 29434 25334   396 28162  6347]\n",
            "Top recommendations for user 494: [ 8198 15068 20487 24161 13881 23997 24907 28028 24160 20985]\n",
            "Top recommendations for user 1101: [ 6903 12128 11062 17350  7316  4566 22215  2202  5486  7890]\n",
            "Top recommendations for user 1417: [ 4615 24907 20836 17350 10380  2126 28162  3479 17800  2553]\n",
            "Top recommendations for user 1639: [17350  6347  7031 24907  2003 21134  4615 24220 28028 18960]\n",
            "Top recommendations for user 1006: [11062 17350  6885 22215  4615  3479 15068 10894  2245  7031]\n",
            "Top recommendations for user 801: [28162 29434  4615 11062 17350  4723 18960   396 23825  3479]\n",
            "Top recommendations for user 478: [24074 10199  9086 23997 24243 13881  6347  4097  4615 24220]\n",
            "Top recommendations for user 195: [17350  6347  4615 11062 18960 21134  7031  3479 24701  2003]\n",
            "Top recommendations for user 1367: [ 4615 17350 11062  6347  3479 18960 29434 28162 24701  7031]\n",
            "Top recommendations for user 5: [15068 23952 21134 17350 24701 11062 25982  6347  2245 25334]\n",
            "Top recommendations for user 124: [11062  4615 29434 17350 28162  3479  8198  6347 15068 20931]\n",
            "Top recommendations for user 534: [28679  6889 11597 19713  9886 28162  2303  7205 17637 12533]\n",
            "Top recommendations for user 755: [15068 11062 17350  2245  8198 22215 10894  7316 21134  6885]\n",
            "Top recommendations for user 476: [17350 28028   396 29434 11062 24745  6347 25334 24701  4615]\n",
            "Top recommendations for user 1100: [17350 17524  2003 11062  4615  3479  5486 22215  6347 20836]\n",
            "Top recommendations for user 1205: [28028 26638 24243 29342  1826   396  9536 20985  6903 24160]\n",
            "Top recommendations for user 959: [28028 23952 17350  7031 15068 24161 24907 20985 21134  8198]\n",
            "Top recommendations for user 1387: [17350 11062 15068  6347 21134 24701  4615  7031  3479  6885]\n",
            "Top recommendations for user 1761: [28028 26638 17350  6903  8198 15068 15203 25982 21134 24160]\n",
            "Top recommendations for user 552: [ 8198 15068 11062 24701 25334 15203 25982 24160 17350 28028]\n",
            "Top recommendations for user 1423: [ 6885 15068 14504 23952  2245  3404  7031 25401 21134 29178]\n",
            "Top recommendations for user 860: [17350 11062 15068 24701  6347 29434  4615  3479 21134 25334]\n",
            "Top recommendations for user 1795: [ 8198 29434 11062 23825 28028 20931 25334  4723 28162 17350]\n",
            "Top recommendations for user 585: [28028  6903 17350 26638  1059   396 21134   725  2003  6347]\n",
            "Top recommendations for user 853: [ 8198 15068 15203 11062 27931 24701 25334 25982 24160 28213]\n",
            "Top recommendations for user 1274: [11062 17350  4615  8198 29434 24907  3479 22215 20836 15068]\n",
            "Top recommendations for user 1003: [11062 29434 17350  4615 15068  3479  8198 28162 10894 22215]\n",
            "Top recommendations for user 642: [17350 11062 28028 24701 17524 15068 29434 21134 25334  6347]\n",
            "Top recommendations for user 1359: [ 8198 15068 24161  4097 13881 20487 19680  4651 20140 23997]\n",
            "Top recommendations for user 851: [ 8198 28028   396 24243 25334 24160 25982 24701 17350 20931]\n",
            "Top recommendations for user 1797: [24701 25334 17350 24745 18960   396 28028 11062  4723 29434]\n",
            "Top recommendations for user 239: [17350 15068  6347 11062 21134 24701  7031  8198 28028 25982]\n",
            "Top recommendations for user 1365: [ 4615 11062 15068  6347 16706 18960 17350  6885 24074  3479]\n",
            "Top recommendations for user 1250: [28028 26638  6903   725  9536 11389 20985  1059 17350 20722]\n",
            "Top recommendations for user 315: [ 8198 28028 13881 24243 24160  1826 20985 26638  7087  5238]\n",
            "Top recommendations for user 793: [ 4615 11062 17350  3479 29434 28162  6885  3847  6347  1277]\n",
            "Top recommendations for user 102: [17350 28028 21134 15068 24701  6347 25982 23952 11062  6903]\n",
            "Top recommendations for user 1698: [17350 11062 15068  8198  6347 28028 29434 24701 25334 21134]\n",
            "Top recommendations for user 125: [24907  4615 20836  2126  9086 11268 20731 17800 17350 10380]\n",
            "Top recommendations for user 1466: [11062  4615 29434 28162 17350  3479  8198 15068 24701  6347]\n",
            "Top recommendations for user 1160: [15068  6885 23952 21134   640  7031  3404  6347  2245 24701]\n",
            "Top recommendations for user 215: [17350 11062 24701 15068 25334  8198  6347 25982 21134 18960]\n",
            "Top recommendations for user 252: [11062 10807 29434  3710  1449 28162  4468  9392  6885 12581]\n",
            "Top recommendations for user 804: [17350  7031 24907  2003  6347 11062 15068 21134  7316 22215]\n",
            "Top recommendations for user 1412: [15068 11062  6885  8198  4385 14504  2245 10807 23952 20487]\n",
            "Top recommendations for user 1210: [21134 23952 15068 26638  6903 17350 28172  7150  8975  2245]\n",
            "Top recommendations for user 1426: [ 8198 15068 15203 28028 26638 11062 28213  2319  6903 17350]\n",
            "Top recommendations for user 876: [17350 15068 24701 28028 21134  8198 11062 25982 23952 25334]\n",
            "Top recommendations for user 737: [11062  4468 24721 22215 10807 14510  3710  2476  9392  6885]\n",
            "Top recommendations for user 1263: [17350  6885 11062  4615  3479 22215  2003 10894  7031  5486]\n",
            "Top recommendations for user 778: [23952 26638 15068  1826  8198  7150 23395 24161 20722 28028]\n",
            "Top recommendations for user 1712: [17524  7890  6903 17350 27569 21134 28608  8975 22544 24701]\n",
            "Top recommendations for user 168: [ 4615  6885 11062 15068 16706 24074  4694  7988  4385 18960]\n",
            "Top recommendations for user 1458: [11062  6885 17350 22215  4615  3479 15068 10894  2245 29434]\n",
            "Top recommendations for user 1821: [24907 11533  9086 11268 17504 20836  1820  8670 13685 16728]\n",
            "Top recommendations for user 1633: [17350  8198 28028 11062 15068 15203 24701 25334 25982 29434]\n",
            "Top recommendations for user 1747: [28028 17350 21134 24701 25982 15068 23952  6903 26638  6347]\n",
            "Top recommendations for user 432: [17350 15068 11062  6347  7031  8198  2003 21134  4615  3479]\n",
            "Top recommendations for user 1050: [23616 24243   396 18960 22124 24074 10931  6347 28028 10495]\n",
            "Top recommendations for user 1745: [17350 28028  6903 24907  7031  2003  6347 21134  7316  1059]\n",
            "Top recommendations for user 1192: [11062 15068  8198 17350 29434  4615  6347 24701 25334  3479]\n",
            "Top recommendations for user 1065: [28028 17350 24907  6347  8198  7031 23997 20985 24160 24161]\n",
            "Top recommendations for user 180: [ 8198 15068 28213  1451 15203 20487  3321 11062  2319 28028]\n",
            "Top recommendations for user 290: [17350 18960  6347 24701  4615 21134 17524 11062  3479 22124]\n",
            "Top recommendations for user 557: [17350 11062 29434 28028 24745 24701 17524 25334  1634   396]\n",
            "Top recommendations for user 1422: [ 6885  4615  3479 17350  5486 22215 10894 11062  2245  3847]\n",
            "Top recommendations for user 722: [17350  6903 21134 17524 28028  2003 22544  7890 24701  6347]\n",
            "Top recommendations for user 913: [ 8198 15068 11062 20487 24160 15203 23952 25982  4385  4097]\n",
            "Top recommendations for user 371: [15068 23952  6885  7031 24161 21134   640 18724  6347 17350]\n",
            "Top recommendations for user 995: [24907  7031 17350  6347 19665  2003 13685  6885 21134 24161]\n",
            "Top recommendations for user 1694: [  871 21058 24853  9392 28162 13462  1449  2805 15535 19030]\n",
            "Top recommendations for user 1200: [11062 17350  8198 29434 28028 15068  4615 25334 24701  3479]\n",
            "Top recommendations for user 1323: [11062 15068  8198 24701 29434 25334 16706  4385 17350 25982]\n",
            "Top recommendations for user 484: [15068 18332 14504 23952 11158  6885 27968 23395  9223  2858]\n",
            "Top recommendations for user 343: [ 6903 26638 28028 29650  1059 17350 18739  8198 10380  7316]\n",
            "Top recommendations for user 1133: [ 6885  7988  4694 25568  3847 10807  6492 13352  4615 19682]\n",
            "Top recommendations for user 1155: [24074  9886    45 23616 11597 24243 22755  6889 12533 22963]\n",
            "Top recommendations for user 1626: [ 6885 19682 20775 27968 14504 16053 12897 29430 25401   733]\n",
            "Top recommendations for user 1775: [13881 28028 24243 10931 11533 23616 23997 24907   396  5238]\n",
            "Top recommendations for user 77: [11062 17350 15068 29434  4615  3479  6347  8198 24701  2245]\n",
            "Top recommendations for user 272: [17350  6347 28028  7031  2003 24907 21134 11062 20836 17524]\n",
            "Top recommendations for user 1134: [17350 24701 11062 25334 18960  6347 28028 29434 25982 24745]\n",
            "Top recommendations for user 1680: [ 8198 29434 20931 25334  4097 11062 24074 28162  4615 24243]\n",
            "Top recommendations for user 687: [ 8198 15068 20487 15203  7150 23952 24160 13055  4385 24594]\n",
            "Top recommendations for user 1843: [11396  7890 23807  9398  1634 28213 11746 10708 26638 15203]\n",
            "Top recommendations for user 430: [15068 11062  8198 17350  6347 29434 23952  7031  2245  6885]\n",
            "Top recommendations for user 536: [10380  5472 27345   396 24835  7453 10495  6440  2553 17504]\n",
            "Top recommendations for user 844: [ 4615 28162 24074  6492 18960 29434 22935 22755  7987  3847]\n",
            "Top recommendations for user 582: [11062 29434 28162  1449 17350 21058  3479 17328 15068  8198]\n",
            "Top recommendations for user 591: [ 4615 29434 24074 11062 28162 18960  8198 25334 16706 20931]\n",
            "Top recommendations for user 187: [17350 11062 21134 15068  6347  7031  6885 24701  2245  2003]\n",
            "Top recommendations for user 980: [11062 15068  6885  2245 17350 14504 22215 10894  3479 18332]\n",
            "Top recommendations for user 275: [11062 15068 29434  8198 22215 17350  2245  1449 10894 13055]\n",
            "Top recommendations for user 1085: [ 8975 11062 15068  9783 12128  6903  7890 21058 15203  2245]\n",
            "Top recommendations for user 958: [11062 22215  6885 17350 10894 15068  2245  3479  5486 29434]\n",
            "Top recommendations for user 1031: [ 8198 28028 15068 24160  1451 13881 17350 20985 28213 20931]\n",
            "Top recommendations for user 521: [ 8198 25334 24701 25982 29434 27931 15203  4572 24243 11062]\n",
            "Top recommendations for user 1285: [28028 17350  6903 26638 21134  7031 23952  6347 15068  2003]\n",
            "Top recommendations for user 229: [28028  8198 26638  6903 17350 15203  1059 15068 20985  2319]\n",
            "Top recommendations for user 1848: [24907 20836 10380 17350 11268 20731  4615 11533 30258  7031]\n",
            "Top recommendations for user 80: [24701 18960 21134 16706 25334 22124  3404 29178 27931  2128]\n",
            "Top recommendations for user 280: [15068  8198 17350 11062 25982 23952  6347 24160 24701 28028]\n",
            "Top recommendations for user 1756: [ 8198 28028 15203 28213 11062 15068 26638  2319 17350 25334]\n",
            "Top recommendations for user 364: [15068 23952  8198 24161  6885 25982  4385 24160 11062 18724]\n",
            "Top recommendations for user 490: [11062 17350  4615 29434  3479 28162 22215 10894  5486  1277]\n",
            "Top recommendations for user 925: [17350  6885 11062  3479  2003  7031 10894  4615 22215  2245]\n",
            "Top recommendations for user 121: [17350  4615 11062  6347  3479 29434 18960  2003 20836 24701]\n",
            "Top recommendations for user 1742: [ 8198 28028  1451 13881 24160 28213 20931 23825  4097 15068]\n",
            "Top recommendations for user 1645: [ 8198 15068 11062 15203 29434 25334 24701  4385 28213 27931]\n",
            "Top recommendations for user 1523: [17350 28028 24907  7031  6903 15068  8198  6347  2003  7316]\n",
            "Top recommendations for user 525: [15068 23952  6885  8198 18724 11062  7031 14504 24161 17350]\n",
            "Top recommendations for user 602: [ 8198 11062 29434 17350 15068 15203 28028 28213 23825  2319]\n",
            "Top recommendations for user 299: [24907 19665 11268  7031 13685  7510 11533  4651 16580    48]\n",
            "Top recommendations for user 1132: [17350 21134  6347  2003 17524 18960  7031 24701  3479 11062]\n",
            "Top recommendations for user 1451: [24701 18960 17350 21134  6347 25334 22124 24745 25982 16706]\n",
            "Top recommendations for user 1682: [11062 29434 21058  1449 28162  8975  9392 13462  2245  7613]\n",
            "Top recommendations for user 727: [22124   640 21134 23952  6347  7031 28368 18960 17350 24161]\n",
            "Top recommendations for user 1189: [ 8198  1451 13881  4097 15068 20487 20931 28028 23997 28213]\n",
            "Top recommendations for user 689: [24907 28028 17350  6903 10380 26398 11533 19665  7031  1059]\n",
            "Top recommendations for user 930: [28162 11062 29434 24701  1449 24745 18960  4615 22935 25334]\n",
            "Top recommendations for user 664: [ 6885 14504 15068  7988 19682 18724 29178 25401  4385 14233]\n",
            "Top recommendations for user 1782: [  396 18960  4615 17350  4723 23616  5472 24074 10927 10495]\n",
            "Top recommendations for user 1017: [ 8198  4097 15068 29434 11062 20931 20487  1451  4615 24160]\n",
            "Top recommendations for user 858: [28028 22124 24243 24701   396 18960  6347 25982 17350 21134]\n",
            "Top recommendations for user 1767: [11062 24701 17350 15068 21134 25334 18960  2245 17524  3404]\n",
            "Top recommendations for user 220: [17350  6347 21134 18960 24701 22124  7031   640 23952 25982]\n",
            "Top recommendations for user 1744: [11062 29434  8198 17350 15068 24701 25334 15203 28028 28162]\n",
            "Top recommendations for user 742: [17350 22215 11062  7316  6903  5486 12128  2003 24907  3842]\n",
            "Top recommendations for user 1855: [11062 29434  8198 28162 23825  4468  1449 26529 28213 15068]\n",
            "Top recommendations for user 1168: [11062 17350 17524 24701 21134  2245  3479 10894  3404 18960]\n",
            "Top recommendations for user 776: [ 6903 29288 12128  8975 18332 19691 29650 26638  4805  9783]\n",
            "Top recommendations for user 1184: [28028 17350  6903  1059   396 24907 10380  6347  2003 20985]\n",
            "Top recommendations for user 971: [24907 11268 19665  2627  7031 30258 30362  4121    70 13685]\n",
            "Top recommendations for user 1046: [ 4615 28162 17350 10380  2553 29434   396 20836  5472 18960]\n",
            "Top recommendations for user 923: [28028 17350  6347 23952 21134 25982 24160 22124   640  7031]\n",
            "Top recommendations for user 528: [ 6885 11062  4615 22215  3847  2627 12581 10807  4468 14504]\n",
            "Top recommendations for user 1583: [11062 17350 15068  8198 29434 28028 24701 25334 17524 15203]\n",
            "Top recommendations for user 202: [19682 24074 22124 18960  3631 10199  1380  7920 20579  4694]\n",
            "Top recommendations for user 1831: [ 6885  7031 17350 24907 19665 24621 14504  2003 21134 25401]\n",
            "Top recommendations for user 32: [ 8198 15068  4097  1451 20487 20931 13881 23997 29434 11062]\n",
            "Top recommendations for user 244: [21134 24701 17350 23952 17524  3404 25334 18960 15068 25982]\n",
            "Top recommendations for user 134: [ 8198 15068 28028 24160 25982 23997 13881 17350 24243  4097]\n",
            "Top recommendations for user 108: [ 8198 28028 13881 24161 23997 24160 24907 15068  6347 24243]\n",
            "Top recommendations for user 1803: [11062  8198 15068 17350 29434 24701 25334 15203  3479  6347]\n",
            "Top recommendations for user 60: [17350 28028   396  6347 18960  4615 20836 24745 12858 10380]\n",
            "Top recommendations for user 1700: [17350  6347 11062  7031 21134  2003  4615  3479 15068 24701]\n",
            "Top recommendations for user 1864: [24907 20840  4121 11029 27871  2075 29123 22215 20731   265]\n",
            "Top recommendations for user 1510: [24701 18960 21134 17350 22124  6347 24745 28028 25334 25982]\n",
            "Top recommendations for user 571: [15068 17350  8198 23952 11062  6347 21134  7031 25982 24161]\n",
            "Top recommendations for user 1114: [28028 26638  8198 17350 25982 24160 15203 24701  6903 25334]\n",
            "Top recommendations for user 1576: [28028 17350 24701 25982  6347  8198 25334   396 24160 21134]\n",
            "Top recommendations for user 492: [11062  8198 29434 15068  4615 22215 28162  4468 20731 26529]\n",
            "Top recommendations for user 213: [26638 28028  8198 15068  6903 17350 23952 15203 24160 25982]\n",
            "Top recommendations for user 1361: [ 4615 11062 29434 28162  3479  3847  1277 17350 20931  1449]\n",
            "Top recommendations for user 474: [ 7031 24907 17350 23952  6347 21134 24161   640 15068 28028]\n",
            "Top recommendations for user 408: [28028 26638 17350  8198  6903 15203 24701 25982 25334  2319]\n",
            "Top recommendations for user 1057: [17350 11062 17524 21134  2245  5486 10894  6903  2003 24701]\n",
            "Top recommendations for user 1791: [18960 17350  4615  6347 24701 24074   396 22124 25334  3479]\n",
            "Top recommendations for user 1541: [28162  6492  4615 29434 11062 22935 15463  1449 18960  2103]\n",
            "Top recommendations for user 1253: [17350 11062 15068  6347  3479  4615  2003  7031 21134 10894]\n",
            "Top recommendations for user 875: [24701 17350 18960 21134  6347 25334 22124 25982 11062 24745]\n",
            "Top recommendations for user 140: [28162  4615  3847 29434 11062 10807  4468 13091 15535 12581]\n",
            "Top recommendations for user 520: [15068 11062  6885  2245 14504 17350 10894  4385  4615  3479]\n",
            "Top recommendations for user 1501: [11062 29434 17350 28162  4615  3479 15068 10894 17524 24701]\n",
            "Top recommendations for user 1136: [17350 24907  7031  6347  4615  2003 20836  3479 24220 21134]\n",
            "Top recommendations for user 350: [11062 17350 29434 17524 24701  8975  2245  7890  1449 21058]\n",
            "Top recommendations for user 208: [28028 17350  6903  6347   396 21134  2003 24907  1059  7031]\n",
            "Top recommendations for user 1254: [21134 23952 15068 24701 17350  7150  2245   640 28172 25982]\n",
            "Top recommendations for user 1092: [28028  8198 17350 29434   396 23825 20931 11062 25334  4723]\n",
            "Top recommendations for user 1233: [ 2126  8663 28162 10380 20345  2553  4615  5472 15303  8696]\n",
            "Top recommendations for user 1760: [24701 25982 16706 24243 25334 15068 18960 24160 23952  7150]\n",
            "Top recommendations for user 1431: [11062  8198 29434 15068  4615 17350 28162 20931  3479 22215]\n",
            "Top recommendations for user 1647: [ 7150 23952 15068 28172  5974 24161  4105 19218 18513 24160]\n",
            "Top recommendations for user 1701: [11062 15068  7988  6885  4385 10807 14504 25568 16706 20630]\n",
            "Top recommendations for user 1226: [15068 24701 11062 16706  6347 17350 21134  4385 23952  6885]\n",
            "Top recommendations for user 850: [17350 11062  6347  2003 21134  4615  3479  7031 28028 24701]\n",
            "Top recommendations for user 1022: [14397 19713  4097 11978 28679  7205 15029 20487 21173  6889]\n",
            "Top recommendations for user 238: [15068  8198 11062 24701 25334 25982  4385 15203 23952 17350]\n",
            "Top recommendations for user 1512: [22124 18960 28368  7920 24701   640 21134 19187 10636  9774]\n",
            "Top recommendations for user 1655: [ 6885 17350 22215 11062  3479  4615 10894  7031  2003  5486]\n",
            "Top recommendations for user 461: [15068 11062  6885  8198 14504  2245  4385 23952 10894 20487]\n",
            "Top recommendations for user 1002: [23952 15068 21134   640  7031 24161  7150 28028 25982 23395]\n",
            "Top recommendations for user 1461: [ 8198 11062 15068 29434 15203 17350 28213 20487  3321 20931]\n",
            "Top recommendations for user 139: [17350 11062 21134  6347 17524  2003  7031 15068  2245  3479]\n",
            "Top recommendations for user 568: [ 8975 24701 21058 27931 11062  2128 17571  7156 27801 25334]\n",
            "Top recommendations for user 330: [ 6885 11062 22215 17350 14504 10894 15068 18332  2245  5486]\n",
            "Top recommendations for user 1529: [17350 11062 29434 24701  4615 25334  6347  8198 18960 28162]\n",
            "Top recommendations for user 1618: [11062 22215 29434 17350  7316 15068 12128 26529  5486  8198]\n",
            "Top recommendations for user 273: [11062 29434  4615 17350  8198 28162  3479 15068 20931  6347]\n",
            "Top recommendations for user 185: [15068 11062  8975 15203 24701  8198 27931 25334  7156 25982]\n",
            "Top recommendations for user 961: [28162 29434  4615 11062  3847  2126  1277 28679  4468  1449]\n",
            "Top recommendations for user 1071: [17350 28028 21134  6347  7031  2003  6903 17524 24907 24701]\n",
            "Top recommendations for user 1624: [ 8198 28028 15203 28213 11062 17350 15068 26638  2319 23825]\n",
            "Top recommendations for user 380: [22124 24243 18960   396 23616 28028 10931  6347 24074 24701]\n",
            "Top recommendations for user 341: [13881 23616 24243 10931 24074    45  9886 25295  2842 25468]\n",
            "Top recommendations for user 969: [ 8198 15068 11062 28028 17350 15203 24160 29434 28213 25982]\n",
            "Top recommendations for user 612: [15068 11062 14504  6885  2245 18332  8975 11158 10894 22215]\n",
            "Top recommendations for user 1105: [17350 28028 21134 24701  6347 15068 25982 17524 23952  6903]\n",
            "Top recommendations for user 1230: [11062 15068  8198 17350 29434 24701 25334  4385  6347  4615]\n",
            "Top recommendations for user 879: [11062 17350 15068  8198 29434  3479  4615 22215  6347  7316]\n",
            "Top recommendations for user 1811: [17350 10380 17524  4615  2003   396 24745 18960  3479 28162]\n",
            "Top recommendations for user 761: [ 6347  6885 23952   640  7031 22124 21134 15068 18960 16706]\n",
            "Top recommendations for user 1188: [ 8198 15068 20487 15203 28213 13055 12607 24594 21806 24160]\n",
            "Top recommendations for user 841: [  396 25334  4723 28028 24701 29434  8198 24745 18960 17350]\n",
            "Top recommendations for user 147: [15068  8198 23952 11062  4385  7150 15203 24701  7156 25982]\n",
            "Top recommendations for user 271: [28028 17350 24907  7031  6347 24161 23952 24160 21134 23997]\n",
            "Top recommendations for user 1470: [ 8198 15068 11062 28028 17350 29434 24160 15203 25334 25982]\n",
            "Top recommendations for user 117: [22124 21134  6347 18960   640 17350 24701 23952  7031 25982]\n",
            "Top recommendations for user 1395: [17350 28028  6903 11062  2003  7316 17524  6347  1059 21134]\n",
            "Top recommendations for user 646: [24907 17350 28028  8198 20836 20466 23997  2126 26024 20731]\n",
            "Top recommendations for user 937: [17350 24701  6347 21134 18960 15068 11062 25334 25982 16706]\n",
            "Top recommendations for user 231: [11062 15068  8198 17350 29434 24701 25334  3479  2245  4385]\n",
            "Top recommendations for user 1612: [17350 10380  2126 20836  4615 28028 20466 29434 24907 23825]\n",
            "Top recommendations for user 1433: [17350 17524 11062 24701 21134 18960  3479  3404  2245  5486]\n",
            "Top recommendations for user 684: [ 7988  4694  2339 25568  6492 29178 19682  6885 16706 24240]\n",
            "Top recommendations for user 72: [11062 15068 29434  4615  4385  8198 16706  6885 24701 28162]\n",
            "Top recommendations for user 1511: [11062  4615 29434 28162  3479  3847  1277  1449 10807  4468]\n",
            "Top recommendations for user 680: [ 8198 11062 15068 17350 29434 28028 25334 24701 25982 24160]\n",
            "Top recommendations for user 170: [17350  6903 28028 17524 21134 11062  2003 24701 24745  7316]\n",
            "Top recommendations for user 914: [ 8198 28028 15068 24160 17350 25982 15203 20985 28213 22702]\n",
            "Top recommendations for user 165: [15068  8198 23952 11062 25982 24701 21134 24160 17350 15203]\n",
            "Top recommendations for user 1465: [11062 17350  6885  3479 22215 10894  4615  2245 29434  5486]\n",
            "Top recommendations for user 656: [28028 23952 26638 25982 24160 15068  8198 21134 24701 17350]\n",
            "Top recommendations for user 206: [28028  8198  6903 17350  1059 26638 20985 10380   396  9536]\n",
            "Top recommendations for user 1199: [ 6903 19665 24907  7031 18332 17350 26638 28987  2858 20527]\n",
            "Top recommendations for user 766: [17350 10380 24907 20836  4615  2003  5472  6347  2316  2553]\n",
            "Top recommendations for user 331: [28162 29434 24074  4615  6492 22755  4723 22935  1449 18960]\n",
            "Top recommendations for user 915: [17350 15068 11062 21134 24701  2245  6347 17524 23952  7031]\n",
            "Top recommendations for user 368: [17350  8198 11062  6347 24701 15068 25334 28028 25982 29434]\n",
            "Top recommendations for user 1082: [17350 28028  6903 10380   396  2003  1059 17524  6347 24745]\n",
            "Top recommendations for user 260: [28028 15068 23952 21134 17350 26638 25982 24701  8198 24160]\n",
            "Top recommendations for user 385: [ 4615 29434  8198 28162 11062  4097 20931  2126 24074  3847]\n",
            "Top recommendations for user 1446: [11062 29434 15068  8198 17350  4615  3479 28162 10894  2245]\n",
            "Top recommendations for user 15: [17350 28028 29434 11062  8198 23825   396 11396  1634 24745]\n",
            "Top recommendations for user 14: [ 4615  9086 24907  3847  2126 20836  7205 24074 23997 28162]\n",
            "Top recommendations for user 1030: [ 8198 15068 17350 28028 11062 25982 24160  6347 23952 21134]\n",
            "Top recommendations for user 4: [11062  4615 29434 28162  3479 17350  8198 15068 20931 25334]\n",
            "Top recommendations for user 1038: [ 8198 28028 23825  6903 17350 20466 28213 15290 11062  1059]\n",
            "Top recommendations for user 943: [ 6903 12128 29650 15552 19691 29288 18332 26638  9783  4566]\n",
            "Top recommendations for user 175: [ 6903 17350 21134 19665 26638 28028 17524  7031 22544  2003]\n",
            "Top recommendations for user 267: [ 8198 11062 29434  4615 15068 20931 17350 28162  4097  3479]\n",
            "Top recommendations for user 1721: [24243 27931  4572 25334  8198 24701 25982 24160  8560 23818]\n",
            "Top recommendations for user 1366: [29434  4615 28162 11062  8198  3847  4097 20931 28679  1277]\n",
            "Top recommendations for user 1094: [28028 17350 21134  6347 23952  7031 25982 15068 24160 24701]\n",
            "Top recommendations for user 1846: [15068 23952  7031 17350  8198 21134 24161  6885  6347 25982]\n",
            "Top recommendations for user 1353: [15068  8198 11062 24701 23952 17350  4385 25982 25334  2245]\n",
            "Top recommendations for user 1393: [11062 29434 17350  4615 24701 28162  3479 18960 25334 17524]\n",
            "Top recommendations for user 1858: [11062 21058 29434 17524  2245 28162  3479 17350  8975 10894]\n",
            "Top recommendations for user 459: [11062 17350  6885 15068 22215  2245 10894  3479 17524  5486]\n",
            "Top recommendations for user 1549: [17350 28028  6347 11062  8198 21134 24701 15068  2003 25982]\n",
            "Top recommendations for user 1620: [11062  8198 17350 29434 15068 28028 15203 25334 24701 23825]\n",
            "Top recommendations for user 1356: [13091 28162   871  3847 14510  2476  4468 19193 10807 11062]\n",
            "Top recommendations for user 944: [15068 11062 24701 25334  8198 17350 16706  4385 25982 29434]\n",
            "Top recommendations for user 266: [17350 15068 11062  8198  7031 24907 28028  6347  2003  7316]\n",
            "Top recommendations for user 676: [17350  6903 10380 19665 22544  2003 18103  2316 17524 24907]\n",
            "Top recommendations for user 1670: [11062 17350 15068 29434 17524  2245  3479 24701 10894 22215]\n",
            "Top recommendations for user 685: [11062 17350 22215  3479 29434  4615 10894 15068  6885  5486]\n",
            "Top recommendations for user 413: [11062 17350 29434 17524 24701  1634  3479 28162 24745  5486]\n",
            "Top recommendations for user 348: [15068  8198 17350 24701 11062 25982 28028 25334 24160  6347]\n",
            "Top recommendations for user 1481: [17350 11062 15068 24701  8198 21134 25334 28028 29434 17524]\n",
            "Top recommendations for user 1462: [17350 15068 11062 24701 28028  8198 25334 25982 21134 15203]\n",
            "Top recommendations for user 1248: [11062  8198 17350  4615 15068 29434  3479 24907 20836 23997]\n",
            "Top recommendations for user 318: [28028   396 17350 24243 24745 25982 24701 25334  4723  6347]\n",
            "Top recommendations for user 274: [17350 11062 24701 25334 24745 29434 17524 21134 18960 28028]\n",
            "Top recommendations for user 1140: [15068 11062 14504  2245  6885  8198  8975 10807 13055 10894]\n",
            "Top recommendations for user 241: [ 8198 15068 20487  4097 24160 18513 24243  7150  4385 23952]\n",
            "Top recommendations for user 425: [18960 22124 24074 23616 24243   396  6347  7920 24701  4704]\n",
            "Top recommendations for user 253: [15068  8198 11062 15203 17350  2245  4385 29434 20487 23952]\n",
            "Top recommendations for user 1538: [17350  6347 28028 18960 24701 21134   396 22124 25982 25334]\n",
            "Top recommendations for user 537: [17350 11062  6347 24701 17524 28028 21134  2003 18960 24745]\n",
            "Top recommendations for user 149: [11062 15068  8198 17350 29434 24701 25334  2245  4385  6347]\n",
            "Top recommendations for user 1594: [ 6885 14504 17350  7031 22215  2245 10894 24621 11062  5486]\n",
            "Top recommendations for user 83: [15068  8198 11062 20487  6885 14504 10807 13055  4385  2245]\n",
            "Top recommendations for user 1220: [15068  8198 11062 17350 24701 25982 25334  6347 23952 21134]\n",
            "Top recommendations for user 1020: [17350 17524 11062  5486  2245  3479 21134 10894  2003 22215]\n",
            "Top recommendations for user 1868: [24701 28608 27569 24745  7890 27931 27801  8975 29907 18960]\n",
            "Top recommendations for user 830: [15068 24701 11062 17350 25334  8198 25982 21134  6347 16706]\n",
            "Top recommendations for user 706: [24907 28028 13881 23997  8198 24161 17350  7031  4147  6347]\n",
            "Top recommendations for user 596: [17350 11062  4615  6347  8198 28028 20836 24907 29434 23997]\n",
            "Top recommendations for user 1692: [11062  6885 15068 14504  4615 29434  2245 10807  3479 10894]\n",
            "Top recommendations for user 1683: [15068  8198 24161 23952 24160 20140  4097 23997 20487 24243]\n",
            "Top recommendations for user 759: [ 2805 10527  7442 28162 20314   871  8113  2103 21058 24835]\n",
            "Top recommendations for user 822: [ 4615 17350 28162 20836  3479  6347 18960 29434 24074 11062]\n",
            "Top recommendations for user 284: [15068  8198 23952 18332 14504 18724 27968  9223  2858 11158]\n",
            "Top recommendations for user 410: [17350 28028  6903 21134  6347 17524  2003 24701 26638  1059]\n",
            "Top recommendations for user 601: [17350 15068 21134 24701  6347 11062 23952 28028  7031 25982]\n",
            "Top recommendations for user 491: [11062 15068 17350 29434  8198  4615 24701  6347  3479 25334]\n",
            "Top recommendations for user 1778: [ 4615  3847 11062  6885 12581 29434 11187 28162 10394  4468]\n",
            "Top recommendations for user 1657: [ 4615 11062 28162 29434  3847  3479  1277  4468 22215 10807]\n",
            "Top recommendations for user 1646: [15068 17350 24701 21134 23952  6347 25982 25334 11062 16706]\n",
            "Top recommendations for user 1865: [ 2842  4651 13881  9886  9086 27303 25295 24074  7921 25468]\n",
            "Top recommendations for user 648: [11062 29434  1449 21058 28162 17328 23662  8975  7613 11396]\n",
            "Top recommendations for user 399: [ 4615  6347 23997 17350 24074  8198  9086  4097 18960 24220]\n",
            "Top recommendations for user 1631: [ 8198 11062 15068 26529 20487 29434  4468 13372 10807 22215]\n",
            "Top recommendations for user 550: [ 8198 11062 29434 15203 28028 11396 28213 25334 24745 23825]\n",
            "Top recommendations for user 663: [ 8198 28028 28213 24243 24160  1451 15203   396  4723 25334]\n",
            "Top recommendations for user 929: [17350 22544  6903 19665 17524  2003 21134  6347 22124 12434]\n",
            "Top recommendations for user 1016: [15068  6885 21134  6347 17350 23952  7031 24701   640  3404]\n",
            "Top recommendations for user 899: [ 6885 17350 11062  2245  7031 15068 21134 10894 14504 22215]\n",
            "Top recommendations for user 1244: [ 4615 17350  3479 20836  2003  6347 11062 24907 28162  7031]\n",
            "Top recommendations for user 881: [17350 11062  4615  6347 29434 24701 18960  3479 25334 28028]\n",
            "Top recommendations for user 1173: [17350 28028  6347  6903   396  2003 21134 24907  7031 18960]\n",
            "Top recommendations for user 989: [17350 11062 24701 21134 15068  6347 17524 25334 18960  2245]\n",
            "Top recommendations for user 1411: [22124 28368   640 18960 24243 19187 21134 14773   396  5238]\n",
            "Top recommendations for user 1707: [17350  6903  7031 21134 19665  2003 17524  7316 24907  6347]\n",
            "Top recommendations for user 143: [17350  6347 21134 11062 15068 24701 28028  7031  2003 25982]\n",
            "Top recommendations for user 1487: [17350  6347  7031 21134 15068 18960 24701 28028 25982 23952]\n",
            "Top recommendations for user 1830: [11062 29434  4615  8198 15068 17350 28162 24701 25334  3479]\n",
            "Top recommendations for user 156: [17350 24701 28028 21134 24745 25334 18960   396 25982  6347]\n",
            "Top recommendations for user 1362: [ 8198 11062 17350 29434 28028 23825 15068 20931 20466  7316]\n",
            "Top recommendations for user 223: [15068 17350 21134 24701 23952 11062 25982  6347 25334 28028]\n",
            "Top recommendations for user 150: [15068 14504  8975 23952  2245 18332 11062 21134  6885 28172]\n",
            "Top recommendations for user 763: [11062  2245 17350  3479 10894  6885 29434 17524 15068 22215]\n",
            "Top recommendations for user 1842: [15068 22215  6885 18332 11062 14504 17350  2858  7316 10894]\n",
            "Top recommendations for user 966: [ 6885 24907  7031 15068 18724 14504 27968 22215  2627 17350]\n",
            "Top recommendations for user 322: [28028   396 17350 24907 10495 11533 10931 10380  6347 20836]\n",
            "Top recommendations for user 246: [11062 17350  4615  3479 29434 15068 24701  2245 10894  6347]\n",
            "Top recommendations for user 184: [15068 24701 23952 21134  7150 25982 25334 11062  2245 27931]\n",
            "Top recommendations for user 86: [11062 15068 17350 24701  2245 21134 25334  4385 10894  6885]\n",
            "Top recommendations for user 1710: [24907 17350 28028 20836  7031 10380  2003  8198  7316 20731]\n",
            "Top recommendations for user 1237: [18332 25978 15068 15552  3016 27968 26529  2858  9223 29123]\n",
            "Top recommendations for user 1820: [24907 11268  6885  3631  4615  7031  3477 25401  1380 17504]\n",
            "Top recommendations for user 1526: [15068  6885 18724 14504  8198 11062 23952  7031  2245 20487]\n",
            "Top recommendations for user 1561: [ 2126 23825 29434  8663 28162 10380  8696 17328 11062 20466]\n",
            "Top recommendations for user 906: [17350 28028 24907  6347  7031  2003 21134 24220 22124 20836]\n",
            "Top recommendations for user 1720: [27931 24701 24745 27801 25334 29907  4723 27569  4572 28608]\n",
            "Top recommendations for user 1625: [17350 21134 22124 24701  6347 17524 18960   640  2003  3404]\n",
            "Top recommendations for user 711: [ 6347 18960  4615 17350 24074 22124 10199 24220  7031 23997]\n",
            "Top recommendations for user 1074: [18960 24074 24701  4615 25334  6347 17350 29434   396 16706]\n",
            "Top recommendations for user 1793: [  396 18960 24074 23616 24243  4723 22755  4615 24701 25334]\n",
            "Top recommendations for user 201: [ 7150 23952 19682 29178 15068  5974   640 28172  6885 20140]\n",
            "Top recommendations for user 437: [ 6492 11062 29434  4615 24074 16706 28162  4385  7988 25568]\n",
            "Top recommendations for user 708: [28028 26638 17350 23952 21134 25982 15068 24160  6903 24701]\n",
            "Top recommendations for user 1394: [22124   640 21134 23952  6347 28028 17350  7031 25982 18960]\n",
            "Top recommendations for user 1717: [29434 25334 24701 28028  8198   396 11062  4723 24745 17350]\n",
            "Top recommendations for user 918: [15068 23952 24161  6885 21134  7031   640  6347  7150 25982]\n",
            "Top recommendations for user 1677: [19682 28368 27709 22124 27039 20579  7920 10636  3631  9511]\n",
            "Top recommendations for user 71: [17350  4615 28162 10380 29434  2553   396 20836  4723 11062]\n",
            "Top recommendations for user 88: [15068  8198 23952 14504  8975  7156 28172 20487 13055  7150]\n",
            "Top recommendations for user 976: [ 6885 22215  4615 11268 24907  2627  3479  5486  3847 10894]\n",
            "Top recommendations for user 1532: [ 6885 14504  8975  9392  7988  2245 11062 21058 13352 18215]\n",
            "Top recommendations for user 379: [ 8198 28028 17350 24160 13881 20985 23997 25982 15068 24243]\n",
            "Top recommendations for user 369: [11062  4615 29434 17350  8198 28162  3479  6347 20931 25334]\n",
            "Top recommendations for user 912: [ 8198 24701 25334 15068 25982 11062 28028 24160 17350 29434]\n",
            "Top recommendations for user 1277: [11062 17350  4615  3479 22215 15068  6885 29434 24907  7031]\n",
            "Top recommendations for user 1415: [18960   396 24701 22124 24074 24243 23616 24745  6347 25334]\n",
            "Top recommendations for user 848: [28028 17350  6347  8198 24160 25982  7031 20985 21134 23952]\n",
            "Top recommendations for user 833: [ 8198 15068 11062 17350 15203 29434 28028 25982 25334 24701]\n",
            "Top recommendations for user 1550: [28028 17350  8198 15068  6347 24160 25982 20985  7031 11062]\n",
            "Top recommendations for user 1013: [ 6885 23952 15068  7150   640 29178 16706 20140 24161 19682]\n",
            "Top recommendations for user 390: [17350 11062 15068  4615  6347  3479 29434  8198 24701 21134]\n",
            "Top recommendations for user 939: [ 4615 28162  2126 29434 11062  3847  1277 17800  7205 20731]\n",
            "Top recommendations for user 888: [ 6885 14504 15068 19682 27968 24272 18724 11158  5974 23395]\n",
            "Top recommendations for user 1382: [28028  6903 17350 24907 26638   396 10049 20985  6347 11389]\n",
            "Top recommendations for user 263: [11062 17350  4615 29434 28162  3479 24701 18960 17524  6347]\n",
            "Top recommendations for user 1805: [28028 26638  6903 17350 20985  8198 24907 20722 15068 23952]\n",
            "Top recommendations for user 1509: [17350 28028  6347   396 18960 24701 25982 25334  4615 23997]\n",
            "Top recommendations for user 1669: [28028 17350  8198  6903 11062 15068 26638 15203  1059  7316]\n",
            "Top recommendations for user 1863: [10207 22544 19665 14644 12434 13064 10527  6751 26878 11463]\n",
            "Top recommendations for user 1862: [ 8198 28028 15068 24160 28213 15203  1451 25982  2319 20985]\n",
            "Top recommendations for user 1630: [11062 29434 17350  8198 23825  1634 15203 28162 17524  1449]\n",
            "Top recommendations for user 947: [ 6885 23395 27968 14504 28987 24272 11158 18332 23952  5974]\n",
            "Top recommendations for user 389: [27931  7156  7150 23835 27801  8975 28172 24701 23818   846]\n",
            "Top recommendations for user 1278: [11062  6903 12128 17350 22215  7316  9783  5486 17524 15068]\n",
            "Top recommendations for user 89: [23952  7150 21134 15068 24701 28172 25982   640 22124 19187]\n",
            "Top recommendations for user 155: [11062 29434 17350  8198 15068 28162  3479  4615 25334 24701]\n",
            "Top recommendations for user 545: [17350 28028  6347 11062 21134  2003 24701  7031 18960  4615]\n",
            "Top recommendations for user 1129: [ 8198 15068 11062 24701 25334 29434 25982 16706 17350 24160]\n",
            "Top recommendations for user 1615: [ 8198 26638 15068 15203 28028 24160 28213 25982  2319 27931]\n",
            "Top recommendations for user 781: [28162 29434  4615  4723 12533  2126 24074 22755 20931  6492]\n",
            "Top recommendations for user 270: [ 6885  7031 21134 17350 24621   640  6347 19665  3404  2003]\n",
            "Top recommendations for user 1440: [ 6885 14504 22215 18332  7031 10894 15068  2245 17350 11158]\n",
            "Top recommendations for user 1539: [28028  8198 24160 24243 25982   396 25334 13881 17350 24701]\n",
            "Top recommendations for user 867: [ 8198 15068 20487 21806  7869 12607 18513  7150 19713 22728]\n",
            "Top recommendations for user 198: [28028 24243   396 23616 20126 18605  9398 26638 10931 29342]\n",
            "Top recommendations for user 732: [28028 17350 26638  6903 15203 11062 15068 24701  8198 25334]\n",
            "Top recommendations for user 658: [21134 23952 24701 22124 25982   640  6347  7150 18960 17350]\n",
            "Top recommendations for user 1018: [ 6885 14504  3710 15068 10807 24721 11158  9392  5452  8975]\n",
            "Top recommendations for user 1794: [24701 27931 24745 21134 25334 27569  7890 18960 29907  8975]\n",
            "Top recommendations for user 120: [24907  4651  7031 13685 24161 11533 22425  5238   640 30258]\n",
            "Top recommendations for user 1191: [21134 22124   640  7031 19665 24621 17350  6347 28368  3404]\n",
            "Top recommendations for user 1835: [15068  6885  8198 18724 14504 23952 11062 18332 27968  7031]\n",
            "Top recommendations for user 1324: [15068 11062  4615  8198 16706  6885  4097  4385 24074  6347]\n",
            "Top recommendations for user 532: [28368 10636 14773 22124  7920  7150 19137   846 28986 15378]\n",
            "Top recommendations for user 1108: [29434  4615 28162 11062 24074  8198 20931  4097 25334  6492]\n",
            "Top recommendations for user 1676: [29434 28162 11062 23825  1449  4615  8198 17328  4723 20931]\n",
            "Top recommendations for user 486: [28028  8198 26638 17350 20985  6903 15068 24160  1059 20722]\n",
            "Top recommendations for user 181: [10380 22821 27871  6903 18103 17954 24907 11029  6054 16108]\n",
            "Top recommendations for user 1302: [18960 21134 17350 24701 22124  3404 17524  6347 15463 24745]\n",
            "Top recommendations for user 19: [24907 24161  8198  7031 23952 28028 13881 15068  4651 13685]\n",
            "Top recommendations for user 982: [28028  8198   396  4723 23825 11396 28213 15290  2700 10437]\n",
            "Top recommendations for user 1493: [28028 13881  8198 24243 24160  1826 20985 26638  5238  7087]\n",
            "Top recommendations for user 46: [15068  8198 24161  4097 20140 23997 19680  4651 20487 18724]\n",
            "Top recommendations for user 357: [15068  8198 11062 17350 23952  6347  7031  6885 25982 21134]\n",
            "Top recommendations for user 852: [17350 11062 28028 24701  8198 25334 29434  6347 25982 15068]\n",
            "Top recommendations for user 800: [24907 11533 11268 16728 10380  7553 27871  1820  9086 20836]\n",
            "Top recommendations for user 926: [15068 23952 21134  7150 25982 24161   640 24701 24160  6347]\n",
            "Top recommendations for user 686: [11062 15068  8198 15203 29434 24701 25334  8975  1449 17350]\n",
            "Top recommendations for user 1407: [11062 17350 29434  8198 28028 23825 25334 28162 24701 24745]\n",
            "Top recommendations for user 723: [ 4615 28162 29434 11062  3479  3847  1277  2126 20836 17350]\n",
            "Top recommendations for user 616: [11062 29434  1449 28162  8198 20555 25334 15068 24701 13055]\n",
            "Top recommendations for user 133: [24701 18960 25334 24745   396 17350 16706 11062 29434 25982]\n",
            "Top recommendations for user 1589: [17350 11062 17524  6903  7316  2003 22215  5486  3479 10894]\n",
            "Top recommendations for user 1521: [28028  6347 24907 24161 23997  7031 17350 24243 24160 24220]\n",
            "Top recommendations for user 583: [11062 24701 17350 15068 29434 25334  8198 18960  6347  4615]\n",
            "Top recommendations for user 471: [17350 11062 28028  6347  2003  7031 24907 15068  7316 21134]\n",
            "Top recommendations for user 1206: [15068 24701  8198 17350 25982 21134 23952 11062 28028 25334]\n",
            "Top recommendations for user 896: [ 4615 17350 20836 18960  6347   396 28162 24907 24074  5472]\n",
            "Top recommendations for user 1610: [ 4615  6492 24074 28162  9886 28679  6889  3847  7205  4694]\n",
            "Top recommendations for user 1111: [15068  8198 23952 16706  4385  7150 25982 20140  4097 24160]\n",
            "Top recommendations for user 287: [11062 15068 22215  6885 14504  2245 10894 29434 10807 18332]\n",
            "Top recommendations for user 993: [ 4615 17350 11062  6347 29434  3479 15068  8198 28162 18960]\n",
            "Top recommendations for user 1033: [18960 15463 22124 24701 24745 25324   396  6492 20270 22755]\n",
            "Top recommendations for user 1608: [11062 29434 21058 22215  5486 13462  2202 28162  1449  8975]\n",
            "Top recommendations for user 1711: [17350 15068 11062  8198  6347 21134 24701  7031  3479  4615]\n",
            "Top recommendations for user 365: [19713  8198  4097 20487 19680 14397 21173  2339 11978 28679]\n",
            "Top recommendations for user 1249: [22124 18960   640 21134 19682  3404  7920 29178 28368  6347]\n",
            "Top recommendations for user 1086: [17350 11062  6347  4615 15068 29434 24701  8198  3479 25334]\n",
            "Top recommendations for user 1773: [15068 24701 25982  8198 25334 23952 24160 17350  6347 21134]\n",
            "Top recommendations for user 469: [15068 24701  8198 17350 25334 25982 11062  6347 24160 18960]\n",
            "Top recommendations for user 740: [23952 15068  7150 23395 24161   640  7031 21134  5974 18724]\n",
            "Top recommendations for user 1609: [11062  8198 15203 28213 15068  2319 26638  6903 29434  8975]\n",
            "Top recommendations for user 18: [18960 24243 24701  6347 24074 25334 25982  8198 17350 28028]\n",
            "Top recommendations for user 519: [28028  8198 26638 17350  6903 15068 20985 15203  1059 24160]\n",
            "Top recommendations for user 1080: [11062 29434 15068 10807 28162  4615  8198  6885  1449  3479]\n",
            "Top recommendations for user 633: [ 6903 28028 17350 10380  1059 26638  1634 18103  7316 17524]\n",
            "Top recommendations for user 11: [17350 21134 15068 23952 24701  2245 17524 11062  6347  7031]\n",
            "Top recommendations for user 1036: [ 4615 11062 17350  6347 15068  8198 29434 18960 16706 24074]\n",
            "Top recommendations for user 1068: [24701 21134 24745 18960 25334 17524 27931 17350 27569  8975]\n",
            "Top recommendations for user 862: [ 6903 28028 19665 26638 21134 17350 22124 22544 25362   640]\n",
            "Top recommendations for user 1001: [ 8198 29434 11062 25334 24701 17350 25982 28028 20931 15068]\n",
            "Top recommendations for user 1808: [11062  8198 15068 29434 26529 13055 28213 22215 15203  1449]\n",
            "Top recommendations for user 1325: [17350 18960  6347 21134 24701 17524 22124  2003 24745   396]\n",
            "Top recommendations for user 1381: [17350 11062  6903 17524 28028 15068  7316 24701 21134 15203]\n",
            "Top recommendations for user 1309: [11062 15068  8198 29434 13055  2245 15203 22215 10807 20487]\n",
            "Top recommendations for user 857: [28028 17350  8198 24701  6347 25982 25334 11062 24160 15068]\n",
            "Top recommendations for user 682: [17350  6903 28028 21134  2003 17524  6347  7031  7316 11062]\n",
            "Top recommendations for user 1606: [17350 28028 24701 11062 21134  6347 25334 17524 25982 15068]\n",
            "Top recommendations for user 1164: [11062 29434 28162 17350  4615  3479 17524  1449 24701 24745]\n",
            "Top recommendations for user 556: [11062 29434 17350 24701 25334  1449 24745 28162 17524 15203]\n",
            "Top recommendations for user 902: [15068  6885 23952 11062  7031 14504 18724 17350  8198  2245]\n",
            "Top recommendations for user 488: [17350 24907  7031  6347  2003 28028 20836  4615 23997 24220]\n",
            "Top recommendations for user 653: [12128  8975  6903 18332 17524  5486  9783  2245  4805 14644]\n",
            "Top recommendations for user 730: [ 6492 24701 22755  4572 18960 24074 12300 25334 28162 29434]\n",
            "Top recommendations for user 821: [ 6885 11062  4615 15068  3479 22215 17350  8198 29434 10894]\n",
            "Top recommendations for user 502: [17350 11062 15068 22215  7316  2003  8198  7031  3479 24907]\n",
            "Top recommendations for user 1812: [24243  8198 24160 25982 15068 24074  4097 16706 24161  6347]\n",
            "Top recommendations for user 61: [ 8975 15068  7156 11062 13055  7869  9392 23662  3710 23678]\n",
            "Top recommendations for user 802: [24907  7031  6885  4651 18724 24161 23395 23952 13685 15068]\n",
            "Top recommendations for user 1341: [11062 17350 29434  8198  4615 15068  3479 28162 22215 20931]\n",
            "Top recommendations for user 1621: [17350 11062 15068  6347  8198  4615  7031  3479 23997 28028]\n",
            "Top recommendations for user 951: [ 8198  4097 13881  1451 20487 24243 24160 20931 15068 23997]\n",
            "Top recommendations for user 479: [ 6885 22215 14504 11062 24721 10894  3479  4615  3847  2627]\n",
            "Top recommendations for user 878: [15068 23952 17350 21134  8198  7031 28028 26638  6903  2245]\n",
            "Top recommendations for user 464: [21134 28028 22124 23952   640 26638 19187  7150 25982 24701]\n",
            "Top recommendations for user 191: [ 6885 22215  4615 14504  3847  2627 25401 11062  3479 10894]\n",
            "Top recommendations for user 1667: [28028 17350  8198 11062  1059  6903 20985 29434  6347 23825]\n",
            "Top recommendations for user 1118: [ 6885 14504 24621  2245  5486 18332 17524 10894 22215 17350]\n",
            "Top recommendations for user 136: [24907  4615 17350 20836  9086  7031  6347  3479 23997  2003]\n",
            "Top recommendations for user 396: [ 6885  7031 25401 14504 19682 24621  3404 21134  6347   640]\n",
            "Top recommendations for user 1149: [11062 28162 29434  4615  6492 22935  3479  1449  3847  2103]\n",
            "Top recommendations for user 1464: [ 7150 28172 24701 21134 29178  3404  7156 23952 16706 22124]\n",
            "Top recommendations for user 1425: [ 9392   871 10807 21058  3710 29909 25568  1372  3130 13091]\n",
            "Top recommendations for user 784: [24243 23616 10931 22124  5238   396 28368 28028 14773 22456]\n",
            "Top recommendations for user 1107: [ 8198  4097 20487 15068 19680 13881 20140 24074 23997 24243]\n",
            "Top recommendations for user 1448: [24701 28028 21134 25982 25334 17350 15068 27931 15203 24745]\n",
            "Top recommendations for user 436: [19682 28368  7150 28172   640 22124 23952 10636 21134 29178]\n",
            "Top recommendations for user 823: [11062 29434 24701  2245 21058 28162  1449  3479 17524 17350]\n",
            "Top recommendations for user 366: [17350 11062 17524  2245 21134  3479 24701 10894  5486  6347]\n",
            "Top recommendations for user 1392: [17350 24907  7031  8198 15068 28028  6347 11062 23997 20836]\n",
            "Top recommendations for user 515: [11062 29434 17350 28162 23825  3479  4615  1449  8198 17524]\n",
            "Top recommendations for user 1368: [11062 15068  2245 17350 29434 24701  6885 10894  4385  3479]\n",
            "Top recommendations for user 872: [ 6885  4615  3847  7988 14504 13352 25401 11062  3479  4694]\n",
            "Top recommendations for user 1679: [17350  4615  6347 11062  8198 29434 18960 28028 23997 20931]\n",
            "Top recommendations for user 934: [28028   396 17350 13881 24907 23997 10931 11533  2126  6347]\n",
            "Top recommendations for user 1128: [ 6903 18103 22544 19665  7890 17350 17524 15180 12128  9042]\n",
            "Top recommendations for user 775: [24243 23616   396 10931 24074 18960 22755 28028  4723 25982]\n",
            "Top recommendations for user 688: [27931 24701  7156  8975  7150 28172 25334 15068 27801  7267]\n",
            "Top recommendations for user 1632: [17350 11062  4615 29434 28162  3479 20836 22215  2003  1277]\n",
            "Top recommendations for user 1497: [11062 15068 10807 29434  1449  9392  4385  7988  3710 21058]\n",
            "Top recommendations for user 1553: [17350  6347 18960 21134  4615 24701  2003 17524  3479 22124]\n",
            "Top recommendations for user 975: [17350 11062 15068  4615  3479 29434  6347  2003  8198 10894]\n",
            "Top recommendations for user 1661: [22124 18960 24243   396 23616 24701  6347 28028   640 24074]\n",
            "Top recommendations for user 794: [28028 17350   396  6903  1059 24907 20985  6347 12858  4216]\n",
            "Top recommendations for user 610: [ 7031 17350  6885 24907 15068 19665 21134 23952  2003 18332]\n",
            "Top recommendations for user 1265: [11062  1449 27931  4385 29434 15068 24701 25334 23678  4572]\n",
            "Top recommendations for user 342: [24907 11268 19665  2627 16580 30362 10380 22215  4121 18109]\n",
            "Top recommendations for user 67: [15068  8198 11062 23952  6885  4385 16706 24701  2245 25982]\n",
            "Top recommendations for user 225: [ 8975  7890 11062 21058 28608  6903 27569  9783 17524 12128]\n",
            "Top recommendations for user 99: [ 8198 28028 11062 17350 15203 29434 28213 15068 23825  6903]\n",
            "Top recommendations for user 450: [ 6885 17350 24621  7031 14504 21134  2245 10894  5486 17524]\n",
            "Top recommendations for user 1196: [ 2339  7988  4694 25568  6492 29178 24240  5452  2002 19713]\n",
            "Top recommendations for user 197: [17350 17524  6903 21134  7890 24701 11062  2003  5486 24745]\n",
            "Top recommendations for user 1150: [17350  6903 28028 24907  2003  7031 19665  7316  6347  1059]\n",
            "Top recommendations for user 475: [28028 26638  1826 20985  8198 23952 24160 13881 20722 28788]\n",
            "Top recommendations for user 1485: [28028  6903 17350 26638 21134 23952  7031  6347   640 22124]\n",
            "Top recommendations for user 480: [ 6885 22215 11062  3479 10894  4615 14504  5486 17350  2245]\n",
            "Top recommendations for user 1072: [ 8198 15068 11062 15203 24701 25334 29434 17350 25982 13055]\n",
            "Top recommendations for user 907: [ 6347 18960 24701 16706 17350 15068 24074  4615 25982 25334]\n",
            "Top recommendations for user 654: [ 8198 28213 15203  1869 20043 27931  1451 13055  4368 12607]\n",
            "Top recommendations for user 606: [ 8198 15068 11062 29434 20931 25334  4097 15203 17350 24160]\n",
            "Top recommendations for user 1534: [17350 28028 24701  8198  6347 25982 11062 21134 15068 25334]\n",
            "Top recommendations for user 1262: [24074  4097 15068  8198 16706  4615 18960  6347  4385  4694]\n",
            "Top recommendations for user 1024: [17350  6347  4615 23997 28028 24907  8198 24220 24074  9086]\n",
            "Top recommendations for user 204: [15068  8198 11062  6885 20487 18724 17350 14504  7031 23952]\n",
            "Top recommendations for user 1660: [28028 26638  9398  8198 28213 15203 11396   396  6903  2319]\n",
            "Top recommendations for user 799: [28028 25982 23952 24160 21134 24243 24701 26638 17350 22124]\n",
            "Top recommendations for user 1213: [ 6885 11062 17350  3479 22215 10894  2245  4615  5486  2003]\n",
            "Top recommendations for user 1702: [ 4615 11062 17350 29434 28162  3479  6347 20836 18960  1277]\n",
            "Top recommendations for user 615: [ 8198  4097 15068 24074 29434 16706 11062  4385 25334 20931]\n",
            "Top recommendations for user 288: [ 6885  3404 18960 19682 22124 21134 24621  6347 15463 29178]\n",
            "Top recommendations for user 131: [17350  6347 15068  8198 11062 24701  7031 28028  4615 25982]\n",
            "Top recommendations for user 1208: [23952  7150  5974 15068  1826 19218 24161 23395 10419  4105]\n",
            "Top recommendations for user 1619: [24701 17350 18960 24745   396 25334 28028  6347 25982  4723]\n",
            "Top recommendations for user 1652: [22124 18960 21134 28368   640 12434 19187   396  6347 17350]\n",
            "Top recommendations for user 1601: [ 6885  4615  3847 11062 14504  3479 13352 22215  7988 10894]\n",
            "Top recommendations for user 671: [17350  4615 29434 11062 18960  6347 28162 24701  3479 25334]\n",
            "Top recommendations for user 1095: [ 4615  2126  5472  7205  7453  1820 28162  2553  3666  3847]\n",
            "Top recommendations for user 863: [17350 11062 15068  6903 21134 17524 28028  7316  2245 24701]\n",
            "Top recommendations for user 1616: [ 8198 17350 15068 11062 28028 24701 25334 25982  6347 29434]\n",
            "Top recommendations for user 647: [28162 29434  4615 11062  6492 22935  1449  4723 18960 22755]\n",
            "Top recommendations for user 1675: [17350  6903 19665  7031  2003 21134 24907 17524  7316  5486]\n",
            "Top recommendations for user 892: [11062 29434 15068  1449 10807 28162  9392  4385  8198 13055]\n",
            "Top recommendations for user 250: [11062 17350 29434  4615 28162  8198  3479  6347 20931 25334]\n",
            "Top recommendations for user 814: [24907  7031  9086  4615  6347 13685 17350 23997 20836 24220]\n",
            "Top recommendations for user 816: [28028 13881 20985 26638   396 17350 24907  9536 24160  6903]\n",
            "Top recommendations for user 209: [24074 23616 24243 10931 18960 29634  9886 22755 25295  2267]\n",
            "Top recommendations for user 562: [24074  4615 28162 22755 23616 12533  3666  2126  6492  4723]\n",
            "Top recommendations for user 1201: [11062 17350 29434  4615  3479 28162 22215 10894  1277  5486]\n",
            "Top recommendations for user 1329: [22124   396 18960 24243 28028 23616 24701 24745  6347 21134]\n",
            "Top recommendations for user 1179: [ 6885 24907  7031 17350 22215 19665  2003  2627 18332 10894]\n",
            "Top recommendations for user 1507: [17350  6347 24701 18960 11062 15068 25334 21134 25982  4615]\n",
            "Top recommendations for user 613: [ 6885 14504 18332 22215  2245 11158 10894 15068 11062 24721]\n",
            "Top recommendations for user 177: [11062 29434 28162 24745  1449 17350  4723 24701 11396  1634]\n",
            "Top recommendations for user 1554: [17350  6903 17524 28028 21134  2003  7890 22544  7316  6347]\n",
            "Top recommendations for user 189: [23952  7150 15068 24160 25982 21134 26638 24161   640 28028]\n",
            "Top recommendations for user 420: [19682  7988 29178 25568 24280  7150  2339 28172  6885  5452]\n",
            "Top recommendations for user 445: [28028  8198 25982 15068 24160 17350 24701 25334 23952 15203]\n",
            "Top recommendations for user 282: [15068 17350 11062  6885  6347 21134  7031 24701  2245 23952]\n",
            "Top recommendations for user 1480: [24243 23616 22124 14773 10931 18960 24074 28368   396 29634]\n",
            "Top recommendations for user 1430: [15068  6885 21134 17350 23952  6347  7031 11062 24701  2245]\n",
            "Top recommendations for user 487: [22124 18960 21134 24701   640  6347 19187 28368 17350  3404]\n",
            "Top recommendations for user 119: [17350 11062 15068 22215  2245 10894  7316  6885 17524  7031]\n",
            "Top recommendations for user 336: [ 6885 15068 23952  7031   640  6347 21134 24161  3404 16706]\n",
            "Top recommendations for user 126: [17350 21134  6347  7031  2003 11062 17524 15068  2245  3479]\n",
            "Top recommendations for user 1514: [ 8198 13881 28028  1451 24160 15068 20985  9962 23997 20487]\n",
            "Top recommendations for user 1051: [11062 29434  8198 28162  4615 15068 20931  1449 23825 17350]\n",
            "Top recommendations for user 1845: [ 4615 28162  2553  3847  7453  3479  5472 10527 20751 15463]\n",
            "Top recommendations for user 1644: [18960   396  4615 17350 24074  6347 22124 24745 23616  4723]\n",
            "Top recommendations for user 987: [28028 17350 23825 10380 29434 15290 11062  6903  8198  1059]\n",
            "Top recommendations for user 699: [11062 29434  8198 17350 25334 24701 15203  1449 15068 28162]\n",
            "Top recommendations for user 97: [24074 22755 29434  4572 22963  8198 15504 28162 12533  6492]\n",
            "Top recommendations for user 1678: [ 8198 11062 29434 17350 15068 25334 24701 28028 25982 20931]\n",
            "Top recommendations for user 1398: [ 4615 17350  6347 18960 29434 11062 24074 23997 20931  8198]\n",
            "Top recommendations for user 1593: [11062 15068  6885 29434 17350  4615  3479  2245 10894  4385]\n",
            "Top recommendations for user 1406: [ 4615  8198 24074  4097 20931 23997 29434  6347  9086 13881]\n",
            "Top recommendations for user 1565: [ 7150  7156 27931 28172 19137  7869 23835 27801  8975 28596]\n",
            "Top recommendations for user 1834: [11062  4615  6885 29434  3479 15068 28162  3847 10894  2245]\n",
            "Top recommendations for user 1156: [28028  8198 17350 25334 24701   396 25982 29434 11062 24160]\n",
            "Top recommendations for user 391: [11062 15068 14504  8975  2245 18332 22215  6885 10894 12128]\n",
            "Top recommendations for user 713: [28028 24243 24160 17350   396 25982  6347  8198 20985 24701]\n",
            "Top recommendations for user 110: [23952 24243  7150 25982 22124 24160   640 28028 19187 21134]\n",
            "Top recommendations for user 1850: [27931 24701  1449  4572 25334 12300 29434  4723 22755 11062]\n",
            "Top recommendations for user 968: [17350 11062 29434  4615  8198 28028  3479  6347 28162  2003]\n",
            "Top recommendations for user 820: [21134 24701 17524 17350 24745  7890 18960 22124 27569  6903]\n",
            "Top recommendations for user 335: [28028   396 24745 17350  4723 24701  1634 25334  6903 18960]\n",
            "Top recommendations for user 1288: [ 4615  3847  4694 28162  6492  6885 24074 11062 29434  7988]\n",
            "Top recommendations for user 549: [ 7150 15068 23952 28172 24701  7156 25982 16706  4385 29178]\n",
            "Top recommendations for user 1181: [ 8198 15068 24161 23952 24907  7031 28028 17350 24160 23997]\n",
            "Top recommendations for user 447: [17350 15068  8198 11062  7031 28028  7316 24907 22215  3321]\n",
            "Top recommendations for user 1212: [19665 17350 24907  6903 10380 22544 18103  2003  2316  9042]\n",
            "Top recommendations for user 751: [ 4615 28162 24074  3847  7205  6492 29434  4694 28679  4097]\n",
            "Top recommendations for user 695: [11062 29434  8198 28162 15068 17350  4615 25334  1449 24701]\n",
            "Top recommendations for user 285: [28028  8198 17350 25982 24160 24243   396 24701 25334  6347]\n",
            "Top recommendations for user 1066: [11062 15068  6885 17350  2245 10894  3479 22215 29434  4615]\n",
            "Top recommendations for user 511: [15068 17350 11062 24701  6347 18960 21134 16706  4615 25334]\n",
            "Top recommendations for user 1569: [17350 28028  6347 21134  7031  2003 24907 24701 18960 25982]\n",
            "Top recommendations for user 1753: [15068 17350 11062 21134  8198 23952  7031  6347 28028 24701]\n",
            "Top recommendations for user 1227: [ 4615 28162 29434 11062  3847  3479  1277  6492 22935  1449]\n",
            "Top recommendations for user 1007: [11062 15068 17350  2245  6885 10894 14504 22215  3479 29434]\n",
            "Top recommendations for user 183: [15068  8198 11062 29434 20487  4385  4097 10807 19336  6885]\n",
            "Top recommendations for user 422: [ 6347 23952 15068 22124   640 21134 25982 24161 17350 24701]\n",
            "Top recommendations for user 470: [11062 29434  4615  8198 17350 22215 28162  3479 15068  1277]\n",
            "Top recommendations for user 1040: [17350  6347 28028  7031 21134 24907  2003 22124 18960 24220]\n",
            "Top recommendations for user 1851: [19682  6885 25401  3631 27709  3477  1380 20579 15998  3007]\n",
            "Top recommendations for user 1391: [11062 15068  6885 14504  2245  8198 10894 22215 29434 10807]\n",
            "Top recommendations for user 1388: [17350 10380 24907   396  6347 20836 28028  2003  4615 18960]\n",
            "Top recommendations for user 928: [15068 23952 21134  7150  7031 23395 24161  6885 14504 18724]\n",
            "Top recommendations for user 865: [28028 24243 24160   396 13881 25982 17350 20985  6347  5238]\n",
            "Top recommendations for user 1416: [15068 17350  7031 11062 23952  6885  8198 21134  2245  7316]\n",
            "Top recommendations for user 1402: [ 3404 19682  6885 21134 24621 29178 22124  7920 18960 24701]\n",
            "Top recommendations for user 754: [17350 15068  7031 11062 21134  6347 23952  2003  2245  6885]\n",
            "Top recommendations for user 1592: [28028  8198 17350 15068  6347 24160 25982 23952 24161  7031]\n",
            "Top recommendations for user 309: [28028  8198 17350  2126 20466 20931 23825 29434 13881 20836]\n",
            "Top recommendations for user 1113: [ 8198 29434  4097 28679 28162 19713 20931  1451  4615 20487]\n",
            "Top recommendations for user 1059: [17350 18960  6347 24701 21134 17524 24745  2003 28028   396]\n",
            "Top recommendations for user 146: [10380   396 17350  5472 10495 18960 20836  4615 12858 24835]\n",
            "Top recommendations for user 426: [17350  6903 22215  7316  2003 11062 24907 19665  7031  5486]\n",
            "Top recommendations for user 1552: [ 8198  1451 13881 28213 24160  4097 28028 24243 20931 20487]\n",
            "Top recommendations for user 720: [24907  9086 11533 24074 25295  8670  8829 10199  1820  2842]\n",
            "Top recommendations for user 356: [ 6885 15068 16706 29178  4385  7988  3404 11062 24701 23952]\n",
            "Top recommendations for user 1751: [17350 11062 17524 15068  2245 21134 10894  3479 24701  2003]\n",
            "Top recommendations for user 806: [ 4615 24074 18960  6347  9086 23997 20836 23616 10199 24220]\n",
            "Top recommendations for user 1525: [11062 17350 29434  8198 15068  4615  3479 28162 22215 10894]\n",
            "Top recommendations for user 1595: [17350 11062  4615  6347 28028 29434  2003  3479 20836 24701]\n",
            "Top recommendations for user 383: [15068  8198 11062 29434 17350  4385 25334 24701 15203 20487]\n",
            "Top recommendations for user 916: [15068  8198 11062 17350 15203 23952  2245  3321 25982 24701]\n",
            "Top recommendations for user 1600: [17350 28028  8198 11062 15068  6347 25982 24160 24701 25334]\n",
            "Top recommendations for user 141: [11062  6885 24701  3404 18960 16706  2245 15068  4615 21134]\n",
            "Top recommendations for user 1575: [17350  6347  7031 21134 23952 15068   640 22124 24161 18960]\n",
            "Top recommendations for user 1556: [17350 11062  2003 21134  6347 17524  7031  3479  7316 22215]\n",
            "Top recommendations for user 1122: [ 6903 28028 17350 26638 21134 17524  1059  7316  7890 24701]\n",
            "Top recommendations for user 891: [17524 21134 17350  3404 24701 18960 14644 24621 22544 22124]\n",
            "Top recommendations for user 1275: [28028 13881 24243   396 26638 20985  9536  8198 24160 10931]\n",
            "Top recommendations for user 1500: [28028   396 24243 26638 10931 20126 13881 23616 18605 24160]\n",
            "Top recommendations for user 749: [11062 24701 17350 15068 25334 21134 17524  2245 29434 25982]\n",
            "Top recommendations for user 1570: [28028  8198 17350 15068 24160 25982 20985 15203 26638 24701]\n",
            "Top recommendations for user 1405: [11062 15068  2245 17350 10894  8198  6885 29434 14504 22215]\n",
            "Top recommendations for user 115: [17350 11062  6347  4615 29434 24701  3479 18960 28028  2003]\n",
            "Top recommendations for user 409: [24701 17350 21134 18960 25334 11062  6347 15068 16706 25982]\n",
            "Top recommendations for user 1544: [17350 11062  8198 29434 28028 23825  4615 20836  7316 20466]\n",
            "Top recommendations for user 1172: [17350 11062  6347  4615  2003  3479 17524 21134 24701 18960]\n",
            "Top recommendations for user 1822: [ 8198 29434 11062 28213 28028 25334 15203 23825 20931 15068]\n",
            "Top recommendations for user 1070: [15068 11062  8198  4615  4097 29434  4385 16706 24074  4694]\n",
            "Top recommendations for user 414: [11062 17350  3479  4615  6885 15068 10894 22215  2245  2003]\n",
            "Top recommendations for user 1691: [15068 23952  6885  7031 17350 14504 21134 18332  2245 18724]\n",
            "Top recommendations for user 477: [17350 11062 15068  6347 24701 21134  4615  8198  7031  3479]\n",
            "Top recommendations for user 886: [11062 17350 17524  2245 21134  8975 15068 10894  5486 24701]\n",
            "Top recommendations for user 535: [ 8198 28028 13881 24160 24243 23997 25982 17350 20931  4097]\n",
            "Top recommendations for user 587: [ 6885  4615 11062 14504  3847  3479 10894 13352  7988  2245]\n",
            "Top recommendations for user 1666: [ 8198  4615 29434 20931  4097 24074 28162 11062 23997  2126]\n",
            "Top recommendations for user 166: [28028  6903 10380 18655 26638   396  9536  1059 10064 11389]\n",
            "Top recommendations for user 163: [11062  8975 17350 15068  6903 17524 12128  2245  7890  9783]\n",
            "Top recommendations for user 1824: [ 8198 15068 24160 28028  1451  4097 13881 20487 25982 20931]\n",
            "Top recommendations for user 1468: [24074  9086 13881 24907 23997 10199  4615 24243 23616  2842]\n",
            "Top recommendations for user 919: [24907  7031 11268  6885 19665 17350  2003  4615  6347  3477]\n",
            "Top recommendations for user 736: [17350 11062  6903 15068 17524 28028  7316 21134  2003  8198]\n",
            "Top recommendations for user 1399: [ 4615 11062 18960 29434 28162 17350  3479 24701  6347 24074]\n",
            "Top recommendations for user 1034: [17350  4615  6347 11062 20836 28028 24907  2003 29434  3479]\n",
            "Top recommendations for user 1703: [ 8198 15068 11062 15203 28213 29434 25334 24160  1451 25982]\n",
            "Top recommendations for user 1413: [ 6885 15068  7031 17350 24907 11062 22215 18724  6347 10894]\n",
            "Top recommendations for user 1049: [ 8198 17350 11062  4615 29434 20931 23997 15068 28028  6347]\n",
            "Top recommendations for user 333: [ 6885 15068 11062  4615 16706  4385  7988 20140  4694 18724]\n",
            "Top recommendations for user 1586: [17350 11062  4615  3479 22215 29434  2003  6885 15068 10894]\n",
            "Top recommendations for user 1390: [ 8198 11062 29434 28028 17350 25334 15068 20931 24701 15203]\n",
            "Top recommendations for user 1238: [27931  7150  7869  7156 19137  4572  7267 23678 14889 20124]\n",
            "Top recommendations for user 1527: [11062 17350  3479  4615  2245 10894  6885 17524 22215 29434]\n",
            "Top recommendations for user 212: [11062 17350 15068 17524  2245 29434 10894  3479 24701 21134]\n",
            "Top recommendations for user 1634: [17350 28028  6347 24907  7031  2003 11062 20836 21134  8198]\n",
            "Top recommendations for user 1010: [ 8975 21058 11062  9783  9392 13462 28608  5486 17524  7890]\n",
            "Top recommendations for user 1075: [28028 17350 26638  6903 21134 25982 24160  6347 23952  8198]\n",
            "Top recommendations for user 597: [17350 28028 11062  8198  6903 15203 15068 24701 29434 25334]\n",
            "Top recommendations for user 709: [23616   396 24243 24074 18960 10931 22755 22124  4723 29634]\n",
            "Top recommendations for user 1123: [ 4615 24074 11062 29434  4097  6347  8198 18960 28162 16706]\n",
            "Top recommendations for user 8: [ 6885 10807 11062 14504  7988  3710 15068  5452  9392 25568]\n",
            "Top recommendations for user 455: [ 4615 17350 10380 20836 24907   396  5472  2126  6347 18960]\n",
            "Top recommendations for user 1087: [11062 17350 29434  4615  3479 15068  6347  8198 10894  2003]\n",
            "Top recommendations for user 1478: [ 4615 24074 18960  6347 10199 22124 24907  9086  3631 24220]\n",
            "Top recommendations for user 985: [28028  8198 17350   396 23825 29434  4723 25334 11062 28213]\n",
            "Top recommendations for user 1590: [23952   640 22124 16706  6347 15068 21134 18960 29178 24161]\n",
            "Top recommendations for user 1032: [ 4615 28162  3479  3847 11062 29434  6885 17350 22215  5486]\n",
            "Top recommendations for user 1738: [  396  4615 17350 18960 28162  4723 20836 28028 24745 29434]\n",
            "Top recommendations for user 1124: [ 8198 15068 15552 20487  3710 13055 11062 29037 28213 26529]\n",
            "Top recommendations for user 251: [ 4615 17350 24907  6347  7031 20836  9086 24220 23997  2003]\n",
            "Top recommendations for user 1091: [ 6885 17350  7031  2003  3479 24907 22215  4615  5486 10894]\n",
            "Top recommendations for user 438: [ 6885 22215 14504  3847  4615 24721 25401  2627  3479 10894]\n",
            "Top recommendations for user 554: [22124 24243 28368 14773 23616 22456 18960 10636 19187 27039]\n",
            "Top recommendations for user 1653: [ 6903  7890 17350 11062 17524  1634 12128  8975 27569 28608]\n",
            "Top recommendations for user 1174: [17350 24701  6347 28028 21134 15068 25982 25334 18960 23952]\n",
            "Top recommendations for user 1217: [ 8198 20487 11062 15068 13372 14397 29434 11187 26529 20731]\n",
            "Top recommendations for user 456: [15068 17350  8198 28028 11062 21134 23952 24701 25982  6347]\n",
            "Top recommendations for user 40: [ 6885  7988  4694  2339 19682  5452 15068 14504 25568 10807]\n",
            "Top recommendations for user 1449: [24907  9962  7553 26398  2075 14923 23521 17954 13833 20840]\n",
            "Top recommendations for user 415: [ 6903 17350 17524 19665  2003  5486  7316 18103 10380 22544]\n",
            "Top recommendations for user 948: [ 8198  4097 15068 23997 24160 13881 20931 20487 24243 24074]\n",
            "Top recommendations for user 222: [17350 24907  6903 10380  2003 28028  7316 20836 19665  7031]\n",
            "Top recommendations for user 297: [11062 29434 22215  2202  4468 17764 28162 26529 14510 13462]\n",
            "Top recommendations for user 1044: [11062 29434 17350  4615 24701 28162  3479 15068 25334 18960]\n",
            "Top recommendations for user 1039: [ 8198 15068 11062 15203 29434 28213 25334 13055 24701 25982]\n",
            "Top recommendations for user 652: [11062  8198 29434 15068 17350  4615 25334 24701 28162  3479]\n",
            "Top recommendations for user 1247: [17350 21134  6347  7031 28028  2003 17524 23952  6903 24701]\n",
            "Top recommendations for user 1671: [28028  8198 17350 15068 11062 24160 25982 20985  6347 15203]\n",
            "Top recommendations for user 569: [17350 21134  7031  6347  2003  6903 22124 28028 19665   640]\n",
            "Top recommendations for user 792: [15068  7031  6885 23952 24907 18724 24161 23395 27968 13685]\n",
            "Top recommendations for user 1251: [11062  4615 29434 28162  3479  6885 15068  3847 10894  4385]\n",
            "Top recommendations for user 884: [ 6885 11062 14504  2245 15068 10894 24621 21134 17350  3404]\n",
            "Top recommendations for user 609: [28028  8198 24160 17350 23952 25982 20985 15068 26638 24161]\n",
            "Top recommendations for user 1818: [ 4615 17350  3479 20836 28162 11062  3847 24907 29434  1277]\n",
            "Top recommendations for user 1732: [11062 29434 28162 17350  8198  4615 25334 24701 20931  3479]\n",
            "Top recommendations for user 1471: [29434  4615 11062 28162  8198 20931 17350  3479 23825  2126]\n",
            "Top recommendations for user 1784: [ 6903 19665 17350 22544 18103 17524 21134 12128 15180 28987]\n",
            "Top recommendations for user 30: [23952 15068 24160 28028 25982  8198  7150 24161 21134 26638]\n",
            "Top recommendations for user 1628: [17350 15068 11062  8198 28028  6347 21134  7031 24701  2003]\n",
            "Top recommendations for user 1419: [24074 18960  4615 24243  8198  6347  4097 24701 16706 23997]\n",
            "Top recommendations for user 403: [ 6903 26638 19665 17350 28028 21134 22544 18103 17524 12128]\n",
            "Top recommendations for user 1826: [17350 11062 29434  4615  8198 15068  3479  6347 24701 25334]\n",
            "Top recommendations for user 614: [28028 26638 23952  6903 21134  1826 25982 24160   640 20722]\n",
            "Top recommendations for user 1723: [15068 23395 27968 23952 18724  6885  7031 24907 24161 18332]\n",
            "Top recommendations for user 1257: [15068  6885 24907 18724  8198  7031 30258 11187 20487  4651]\n",
            "Top recommendations for user 1267: [11062 15068  2245  8975 17350 24701 17524 21134  8198 10894]\n",
            "Top recommendations for user 278: [11062 15068  6885  8198 14504 22215  2245 10807 10894 29434]\n",
            "Top recommendations for user 1809: [ 8198 11062 29434 15068 20487  1449 28162 28213 20931  1451]\n",
            "Top recommendations for user 1126: [ 6903 17350 11062 28028  7316  1634 17524  7890 10380 12128]\n",
            "Top recommendations for user 190: [ 4615 28162 29434 11062  3847  1277  4468  3479 10394  2126]\n",
            "Top recommendations for user 129: [28028   396 17350 18960  6347 24701 22124 24745 24243 25982]\n",
            "Top recommendations for user 1763: [28028   396  4723  8198 23825 29434 11396 17350 24745 20931]\n",
            "Top recommendations for user 1292: [28028  6347 17350 24243 22124 24220 23997 24161 24907 24160]\n",
            "Top recommendations for user 1486: [17350 11062 15068  8198  6347 24701 29434 21134 28028 25334]\n",
            "Top recommendations for user 1563: [17350  6347 18960 21134  4615 24701 11062  2003  3479  7031]\n",
            "Top recommendations for user 1789: [24907 17350  7031  6885 22215  2003 19665  6347 20836  7316]\n",
            "Top recommendations for user 106: [17350 11062  8198 28028 15068  7316  6903 29434  1059  3321]\n",
            "Top recommendations for user 503: [ 8198 13881 24243  1451    45  4097 24160 10738 23616 20931]\n",
            "Top recommendations for user 726: [ 6885 17350 15068 11062  2245 21134 14504 10894  7031  3404]\n",
            "Top recommendations for user 182: [ 5486  6885 17350 17524 11062  3479 22215 10894  2245  4615]\n",
            "Top recommendations for user 1157: [11062 17350 15068  8198 29434 24701  6347 25334  4615  3479]\n",
            "Top recommendations for user 1084: [17350  4615 11062  2003  6347  3479 17524 20836 28028 29434]\n",
            "Top recommendations for user 45: [15068  8198  6885  4385 11062 23952 20487 14504 16706 18724]\n",
            "Top recommendations for user 1178: [24907  4615 17350  7031  9086  6347 20836 23997  6885 30258]\n",
            "Top recommendations for user 1330: [18960 22124 24701 21134 17350  6347 24745 25334   396 25982]\n",
            "Top recommendations for user 1009: [11062 29434 15068 24701  8198 25334  1449 17350 15203  4385]\n",
            "Top recommendations for user 1162: [  396 28028 18960 22124 17350  6347 23616 24243 10495 24745]\n",
            "Top recommendations for user 1536: [ 8198 28028 15068 17350 11062 15203 24160 25982 20985  2319]\n",
            "Top recommendations for user 44: [ 6903 17350 17524  7890 12128  5486 11062  7316 18103 21134]\n",
            "Top recommendations for user 334: [ 8198 28028 15068 24160 17350 25982 23952 20985 15203 22702]\n",
            "Top recommendations for user 604: [24907 10380  5472  4615 17504 17350 20836 11268 10710  7453]\n",
            "Top recommendations for user 1719: [ 6885 14504 24621  2245 21134 24272 11158  3404 18332 28172]\n",
            "Top recommendations for user 1867: [ 4615  3847 28162  7205 27312 28679 13091 12581 19543  4084]\n",
            "Top recommendations for user 1452: [17350 17524  2003 11062 21134  5486  3479  6347  7031 22215]\n",
            "Top recommendations for user 1598: [21134 23952  7150 28172   640 22124 26638 19187 28368 24701]\n",
            "Top recommendations for user 1320: [ 4615 29434 11062 28162 24074  6492  4097  4694  7988  4385]\n",
            "Top recommendations for user 1429: [15068 23952 26638  7150  8198 25982 28172 24160 21134 28028]\n",
            "Top recommendations for user 1317: [17350 28028  4615   396 20836  6347 29434 10380 11062  2003]\n",
            "Top recommendations for user 1706: [15068  8198 24701 25982 23952 15203 25334 24160 27931  7150]\n",
            "Top recommendations for user 1078: [15068  6885 14504 23952  2245  7150 28172 21134 11062  4385]\n",
            "Top recommendations for user 1441: [ 4615 28162 11062  3479 29434 22935 18960 15463  6492  3847]\n",
            "Top recommendations for user 1444: [17350  7031 21134  2003  6347 24907 15068  6903 17524  7316]\n",
            "Top recommendations for user 1258: [24243   396 24074 18960 23616 28028 25334  4723 20931 24701]\n",
            "Top recommendations for user 381: [28162 15303  2126  4615  7205 14510  4084  8663 11029  3847]\n",
            "Top recommendations for user 388: [18960 24074 22124  6347 16706  4615 24701 10199  6492  4704]\n",
            "Top recommendations for user 1840: [17350  6347 28028 18960 22124 24220 24907 23997  7031   396]\n",
            "Top recommendations for user 617: [28028   396 10495 11533 24907 11389 10064 10931   583  6903]\n",
            "Top recommendations for user 1342: [17350 21134 17524 11062  2245 15068 24701  2003  6347  7031]\n",
            "Top recommendations for user 1081: [ 7031 23952 24161  6347   640 17350 21134 24907 15068 22124]\n",
            "Top recommendations for user 540: [17350  8198 28028 11062 24701 15068 25334 29434  6347 25982]\n",
            "Top recommendations for user 111: [28028 17350  8198  6347 24701 21134 25982   396 25334  6903]\n",
            "Top recommendations for user 376: [15068 23952 26638  8198 21134 28028 17350  7031 20722 24161]\n",
            "Top recommendations for user 575: [17350 15068 24701 21134 11062 28028 25982 25334  8198  6347]\n",
            "Top recommendations for user 1153: [10380  5472 17350 27345 10495 24835  6903 24907   396 10710]\n",
            "Top recommendations for user 1252: [17350 21134  6347 24701 17524  2003 11062 18960  7031 28028]\n",
            "Top recommendations for user 497: [15068 17350 11062  8198 21134 24701  2245 28028 17524  6347]\n",
            "Top recommendations for user 232: [26638 23952 15068 23395  6903 28987 18332  9223 12012 29650]\n",
            "Top recommendations for user 861: [28028  8198 25982 24160 24701 15068 17350 25334 15203 24243]\n",
            "Top recommendations for user 1326: [23952  7031 21134 17350 15068   640 24161 19665  6347 24907]\n",
            "Top recommendations for user 1321: [17350 11062  4615  6885  3479  6347 10894  7031 22215  2003]\n",
            "Top recommendations for user 1482: [11062 17350  4615 29434 28162  3479 17524 22215  2003  5486]\n",
            "Top recommendations for user 48: [ 6903 12128 22215  2075 10380  7316 15180 26571 18332 20840]\n",
            "Top recommendations for user 607: [28028   396 17350 24701 18960 24243 25334 25982  6347 24745]\n",
            "Top recommendations for user 746: [15068  6885  7988 10807 11062  4385 14504 25568  5452  2002]\n",
            "Top recommendations for user 721: [23952 15068 23395  5974  6885 18724  7150 24161 27968  7031]\n",
            "Top recommendations for user 768: [ 8198 29434 23825 28213 11396  1451  2126 11062 28162 20931]\n",
            "Top recommendations for user 1635: [17350 11062  6347 24701  8198 15068 18960  4615 25334 29434]\n",
            "Top recommendations for user 1450: [27931  4723 24243  9398   396 28028  4572  8198 25334 28213]\n",
            "Top recommendations for user 813: [17350 28028  6347 21134  7031 22124  2003 24701 18960  6903]\n",
            "Top recommendations for user 628: [28028 26638 24907 11389  5238  6903 24243 20985 25362   396]\n",
            "Top recommendations for user 567: [15068 17350 23952 21134 28028  8198  7031  6347 25982 11062]\n",
            "Top recommendations for user 1833: [ 8198 11062 20487 29434 15068  4097 10807  4615 12581 13372]\n",
            "Top recommendations for user 351: [17350 11062 29434  4615  8198 15068  3479  6347 24701 25334]\n",
            "Top recommendations for user 945: [ 8198 15068 20487  4097 11062 20931 29434  1451 24160 23997]\n",
            "Top recommendations for user 47: [ 6885 19665 11268  5486 24907  7031 22215 22544 24621 15021]\n",
            "Top recommendations for user 375: [15068  8198 23952 17350 11062 28028  7031  3321 24161 24160]\n",
            "Top recommendations for user 1578: [28028 17350  6347  8198 25982 24160 21134  7031 20985 24701]\n",
            "Top recommendations for user 1186: [24701 18960   396 27931 24745 25334 24243  4723 29907  4572]\n",
            "Top recommendations for user 854: [10380  5472  4615  7453  2553 17504 24835 11268 20836 10710]\n",
            "Top recommendations for user 1161: [24907 17350  7031 15068  6885  6347 11062  2003 22215 24161]\n",
            "Top recommendations for user 114: [ 8198 28213  1451 15068 28028 15203  2319 24160 23825 20487]\n",
            "Top recommendations for user 1428: [28028  6903 17350 26638  1059 20985  8198 24907 18739  7316]\n",
            "Top recommendations for user 1519: [11062  6885 15068  4615 22215 11187  8198 20731 13372 29434]\n",
            "Top recommendations for user 1148: [11062 17350  4615  3479 17524 28162 18960  5486 10894  2245]\n",
            "Top recommendations for user 327: [ 6885  4615 11062  3479  3847 25401  4694  7988 14504 15068]\n",
            "Top recommendations for user 1282: [ 8198 26638 28028  9962  1451 28213 13881 20985  9536 17257]\n",
            "Top recommendations for user 1435: [ 8198 17350 29434 28028 11062 20931 25334  4615 24701   396]\n",
            "Top recommendations for user 1802: [ 8198 20487 15068 11978 19713 21806 12607 13771  8576 16697]\n",
            "Top recommendations for user 310: [24907  4615 17350 22215 20731 11062 20836  6885  7031  3479]\n",
            "Top recommendations for user 137: [ 6885 14504 10807  3710  9392 11062 24721 21058  8975 21229]\n",
            "Top recommendations for user 1708: [ 8198 20487  9962 15068  2404 16697 19680  4651  1451 14397]\n",
            "Top recommendations for user 56: [17350 11062 24701 25334 17524 21134 24745 15068 28028 15203]\n",
            "Top recommendations for user 1373: [11062  8198 15068 29434 17350  4615  3479 28162 25334 20931]\n",
            "Top recommendations for user 1142: [ 6885 18724 27968  4651 15068 11187 19680  2627 30156 23820]\n",
            "Top recommendations for user 640: [19682  2339  6885  5452  7988  5974 10706 14233  4694 20140]\n",
            "Top recommendations for user 643: [23952 24161   640  7031 20140 22124 10199  6347 28368  4651]\n",
            "Top recommendations for user 344: [11062  4615  6885  3479 29434 17350 15068 10894 22215 28162]\n",
            "Top recommendations for user 277: [11062 15068  8198 29434 22215  6885  2245 10807 14504 10894]\n",
            "Top recommendations for user 151: [24243  4723   396  8198 25334 23616  4572 24701 22755 24074]\n",
            "Top recommendations for user 1762: [11062 24701 25334 29434  8198 15068 27931 17350 24745 15203]\n",
            "Top recommendations for user 1456: [15068  8198 11062 17350 24701 23952 25982 25334 21134 15203]\n",
            "Top recommendations for user 1131: [11062 29434 15068  4615  6885 28162  3479  2245  4385 10807]\n",
            "Top recommendations for user 1281: [ 6885 17350 21134 15068  7031  6347  3404  2245 11062 23952]\n",
            "Top recommendations for user 1284: [ 4615 28162 11062 29434  3847  3479  1277  6492 22935 17350]\n",
            "Top recommendations for user 1748: [ 6885  4694 19682  7988  4615  3847  2339 25401 20140 18581]\n",
            "Top recommendations for user 1015: [ 6885 11062 10807 14504  3847 28162  7988  4615  9392 13352]\n",
            "Top recommendations for user 786: [17350 24907  7031 11062  2003 22215  6347  6885 15068  3479]\n",
            "Top recommendations for user 428: [17350 28028  6347  4615 20836   396  2003 11062 18960 24907]\n",
            "Top recommendations for user 1445: [29434 28162 11062  4615  8198  1449 20931 20555  4097 28679]\n",
            "Top recommendations for user 1202: [15068  8198 20487 11062  4385 23952  4097 24160 15203 25982]\n",
            "Top recommendations for user 91: [28028 17350  6903 26638 15068  8198 20985 23952  7031 20722]\n",
            "Top recommendations for user 1788: [ 6885 18332 14504 11158 19665 27968 28987 18817 24721  2858]\n",
            "Top recommendations for user 64: [11062 29434  4615 28162  3479 17350  1277  3847  1449 20931]\n",
            "Top recommendations for user 874: [15068 17350 11062  8198  6347 21134  7031 23952 24701  2245]\n",
            "Top recommendations for user 1713: [24907 28028 23997  6347 13881 17350 24161  7031  8198  4147]\n",
            "Top recommendations for user 311: [24243 13881 10738  8198  4097  2842 25596 24160  5238  7087]\n",
            "Top recommendations for user 128: [ 4615 17350 18960 28162   396 20836  2553  5472  6347 10927]\n",
            "Top recommendations for user 265: [17350 21134 17524 11062  2003  7031  6347  2245  6903 10894]\n",
            "Top recommendations for user 1505: [15068  6885 14504  7988  4385 11062 18724 20487 10807 20140]\n",
            "Top recommendations for user 1055: [11062 29434 24701 21058  1449  8975 17524 25334 17350 24745]\n",
            "Top recommendations for user 62: [23952 24161 24907  7031 15068 28028 24160  8198  5238 20985]\n",
            "Top recommendations for user 765: [17350 28028  6347 21134 22124  7031 25982 24701 23952 18960]\n",
            "Top recommendations for user 1371: [17350 28028 24701 18960   396  6347 21134 24745 25334 25982]\n",
            "Top recommendations for user 1473: [28028  8198 17350  6347 15068 24160 25982 24701 23997 25334]\n",
            "Top recommendations for user 921: [ 8198 28028 24160 24243 25982 17350 13881 25334   396 20931]\n",
            "Top recommendations for user 828: [11062 22215  5486  6885  2245 10894 14504 21058 17524  3479]\n",
            "Top recommendations for user 1649: [ 8198 15068 20487  1451  4097 24160 13881 11062 23997 20985]\n",
            "Top recommendations for user 1298: [22544 19665 10207 12434 24835 10527   889 17524 17350 26878]\n",
            "Top recommendations for user 1823: [24074  4615  2126 28162  3666 23616 12533  9886  6492  9086]\n",
            "Top recommendations for user 998: [27931  8975 23662  1449 15203 27801 11062 28213 13055  7613]\n",
            "Top recommendations for user 981: [28028  6903 17350 26638   396  1059  1634 24745 10380 12858]\n",
            "Top recommendations for user 377: [22124   640 23952 28368 21134 24243  7150  6347 24161 19187]\n",
            "Top recommendations for user 1776: [26638  6903 28028 21134  7890 24701 27569 27773 16208 27931]\n",
            "Top recommendations for user 1796: [22544 24835 12434 10207 17524  9468 17350 22124   889 27345]\n",
            "Top recommendations for user 302: [ 8198 13881 15068  4097 20487 24160 24243  1451  3676 18513]\n",
            "Top recommendations for user 1568: [15068 24701 11062 17350  8198 25334 25982 28028 21134 15203]\n"
          ]
        }
      ],
      "source": [
        "# Generate top recommendations for each user\n",
        "all_users = train_df['user_idx'].unique()\n",
        "all_books = train_df['book_idx'].unique()\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user in all_users:\n",
        "        user_idx_tensor = torch.LongTensor([user] * len(all_books))\n",
        "        book_idx_tensor = torch.LongTensor(all_books)\n",
        "        pred_ratings = model(user_idx_tensor, book_idx_tensor).squeeze()\n",
        "        top_books_idx = torch.argsort(pred_ratings, descending=True)[:10]  # Top 10 recommendations\n",
        "        top_books = all_books[top_books_idx]\n",
        "        recommendations.append((user, top_books))\n",
        "\n",
        "# Print top recommendations for each user\n",
        "for user, recs in recommendations:\n",
        "    print(f\"Top recommendations for user {user}: {recs}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIO1uH1qwmia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa_S9LDnzJyg"
      },
      "source": [
        "We want to use as much data as we can and compare all the models we can create. Another idea is to use the 'category' value, that can be taken form the \"metadata\" database, that maps every asin to its category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB4Ix-eEzQoB"
      },
      "outputs": [],
      "source": [
        "file_path=f\"/content/drive/MyDrive/meta-books-parquet/meta_books_chunk_1.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXbdhJMXzkTA"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "\n",
        "df_meta = pl.read_parquet(file_path)\n",
        "df_meta_pandas = df_meta.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N35KyOW957GH"
      },
      "outputs": [],
      "source": [
        "dataframes = []\n",
        "\n",
        "\n",
        "for i in range(1, 148):\n",
        "\n",
        "    file_path = f\"/content/drive/MyDrive/meta-books-parquet/meta-books-csv-chunk_{i}.parquet\"\n",
        "\n",
        "    df_meta = pl.read_parquet(file_path)\n",
        "\n",
        "    df_meta_pandas = df_meta.to_pandas()\n",
        "\n",
        "    dataframes.append(df_meta_pandas)\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRp9g-PN2zNG"
      },
      "outputs": [],
      "source": [
        "\n",
        "combined_df['category'] = combined_df['category'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff4E12je14hL"
      },
      "outputs": [],
      "source": [
        "counting_genres=combined_df['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksWfh_W76T-N"
      },
      "outputs": [],
      "source": [
        "counting_genres.to_csv(\"counting_genres.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv8aXX0SCis5",
        "outputId": "c25b038a-ede7-4815-cb26-b7254f4c77fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "category\n",
              "[]                                                                               389429\n",
              "['Books', 'Literature &amp; Fiction', 'Genre Fiction']                            70204\n",
              "['Books', 'History', 'Americas']                                                  68370\n",
              "['Books', 'New, Used &amp; Rental Textbooks', 'Humanities']                       43530\n",
              "['Books', 'Literature & Fiction', 'Genre Fiction']                                42191\n",
              "                                                                                  ...  \n",
              "['Books', 'History', 'Asia', 'India']                                                 1\n",
              "['Books', 'Travel', 'Europe', 'Norway', 'General']                                    1\n",
              "['Books', 'Arts &amp; Photography', 'History &amp; Criticism', 'History']             1\n",
              "['Books', 'Arts & Photography', 'Individual Artists', \"Artists' Books\"]               1\n",
              "['Books', 'Business & Money', 'Marketing & Sales', 'Marketing', 'Multilevel']         1\n",
              "Name: count, Length: 1962, dtype: int64"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counting_genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2BZcD4alDMD0",
        "outputId": "acc2e4ac-5527-4e2f-f2a4-d9308bd9a24f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1962,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1962,\n        \"samples\": [\n          \"['Books', 'Arts &amp; Photography', 'Music', 'Instruments', 'Piano']\",\n          \"['Books', \\\"Children's Books\\\", 'Cars, Trains & Things That Go']\",\n          \"['Books', 'Romance', 'Science Fiction']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9842,\n        \"min\": 1,\n        \"max\": 389429,\n        \"num_unique_values\": 688,\n        \"samples\": [\n          1703,\n          36,\n          2068\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bc14f659-886d-428a-abe3-97001fe5dcb8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[]</td>\n",
              "      <td>389429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['Books', 'Literature &amp;amp; Fiction', 'Genre F...</td>\n",
              "      <td>70204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['Books', 'History', 'Americas']</td>\n",
              "      <td>68370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['Books', 'New, Used &amp;amp; Rental Textbooks', ...</td>\n",
              "      <td>43530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['Books', 'Literature &amp; Fiction', 'Genre Ficti...</td>\n",
              "      <td>42191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc14f659-886d-428a-abe3-97001fe5dcb8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc14f659-886d-428a-abe3-97001fe5dcb8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc14f659-886d-428a-abe3-97001fe5dcb8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a37351dc-96d4-4c44-8f6d-e46213122906\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a37351dc-96d4-4c44-8f6d-e46213122906')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a37351dc-96d4-4c44-8f6d-e46213122906 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            category   count\n",
              "0                                                 []  389429\n",
              "1  ['Books', 'Literature &amp; Fiction', 'Genre F...   70204\n",
              "2                   ['Books', 'History', 'Americas']   68370\n",
              "3  ['Books', 'New, Used &amp; Rental Textbooks', ...   43530\n",
              "4  ['Books', 'Literature & Fiction', 'Genre Ficti...   42191"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "df = counting_genres.reset_index()\n",
        "df.columns = ['category', 'count']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj8afbfZD3Ff"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "def string_to_list(category_string):\n",
        "    return ast.literal_eval(category_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op8MGy4FA_ny",
        "outputId": "57be9185-86fe-4529-c628-0c13852cad9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        extracted_category\n",
            "0                  unknown\n",
            "1     Literature & Fiction\n",
            "2                  History\n",
            "3               Humanities\n",
            "4     Literature & Fiction\n",
            "...                    ...\n",
            "1957               History\n",
            "1958                Travel\n",
            "1959    Arts & Photography\n",
            "1960    Arts & Photography\n",
            "1961      Business & Money\n",
            "\n",
            "[1962 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# Function to extract the desired category based on the input in the metadat file\n",
        "def extract_category(categories):\n",
        "    if not categories:\n",
        "        return \"unknown\"  # we have edge case of \"[]\" in the categories, as you can see above\n",
        "    if len(categories) == 1:\n",
        "        return categories[0]\n",
        "    else:\n",
        "      second_item = categories[1]\n",
        "      third_item = categories[2] if len(categories) > 2 else None\n",
        "\n",
        "    # Check for specific categories\n",
        "    # if the second category is \"New, Used & Rental Textbooks\" we move to the third item\n",
        "    if re.match(r'New, Used (&amp;|&) Rental Textbooks', second_item):\n",
        "        if third_item:\n",
        "            return third_item\n",
        "        # this case is when there are only two categories (['Books', 'New, Used &amp; Rental Textbooks'] or ['Books', 'New, Used & Rental Textbooks'])\n",
        "        return \"unknown\"\n",
        "    else:\n",
        "        # Return the second category, with \"&amp\" replaced to \"&\"\n",
        "        return re.sub(r'&amp;', '&', second_item)\n",
        "df['category_list'] = df['category'].apply(string_to_list)\n",
        "\n",
        "# Apply the function to the category column\n",
        "df['extracted_category'] = df['category_list'].apply(extract_category)\n",
        "\n",
        "print(df[['extracted_category']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59VMfuggEBOW"
      },
      "outputs": [],
      "source": [
        "df['extracted_category'] = df['extracted_category'].astype(str)\n",
        "grouped_df = df.groupby('extracted_category')['count'].sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv-yMTmQFadV",
        "outputId": "1bc92f7d-6737-4be3-817a-d2b80aa91c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            extracted_category   count\n",
            "0                           Arts & Photography  158286\n",
            "1                        Biographies & Memoirs   91109\n",
            "2                           Business & Finance    1119\n",
            "3                             Business & Money   88142\n",
            "4                       Business &amp; Finance    3814\n",
            "5                                    Calendars   11047\n",
            "6                             Children's Books  241105\n",
            "7                     Christian Books & Bibles  150126\n",
            "8                      Comics & Graphic Novels   41571\n",
            "9                   Communication & Journalism      14\n",
            "10              Communication &amp; Journalism      83\n",
            "11                            Computer Science     668\n",
            "12                      Computers & Technology   51133\n",
            "13                      Cookbooks, Food & Wine   52505\n",
            "14                      Crafts, Hobbies & Home  100310\n",
            "15                                   Education    3996\n",
            "16                        Education & Teaching   43512\n",
            "17                Engineering & Transportation   51961\n",
            "18                   Health, Fitness & Dieting   51567\n",
            "19                                     History  138229\n",
            "20                                  Humanities   58410\n",
            "21                       Humor & Entertainment   49991\n",
            "22                                         Law   14602\n",
            "23  Lesbian, Gay, Bisexual & Transgender Books    7570\n",
            "24                        Literature & Fiction  355370\n",
            "25                               Medical Books   28897\n",
            "26                  Medicine & Health Sciences    7589\n",
            "27              Medicine &amp; Health Sciences   25748\n",
            "28                Mystery, Thriller & Suspense   83863\n",
            "29                   Parenting & Relationships   16205\n",
            "30                  Politics & Social Sciences   71042\n",
            "31                                   Reference   69827\n",
            "32                     Religion & Spirituality   77120\n",
            "33                                     Romance   58542\n",
            "34                              Science & Math   63406\n",
            "35                       Science & Mathematics    1475\n",
            "36                   Science &amp; Mathematics    4297\n",
            "37                   Science Fiction & Fantasy   61197\n",
            "38                                   Self-Help   53525\n",
            "39                             Social Sciences   18929\n",
            "40                           Sports & Outdoors   32915\n",
            "41                          Teen & Young Adult   43353\n",
            "42                    Test Prep & Study Guides     191\n",
            "43                Test Prep &amp; Study Guides     460\n",
            "44                            Test Preparation    5350\n",
            "45                                      Travel   48763\n",
            "46                                     unknown  396015\n"
          ]
        }
      ],
      "source": [
        "print(grouped_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir5-usW9Alyk"
      },
      "source": [
        "Now we combine categories based on common knowledge, and what we think that should be the same category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kh4YBOlWJKw5"
      },
      "outputs": [],
      "source": [
        "replacement_dict = {\n",
        "    'Business & Finance': 'Business & Money',\n",
        "    'Business &amp; Finance': 'Business & Money',\n",
        "    'Communication &amp; Journalism': 'Communication & Journalism',\n",
        "    'Computers & Technology': 'Computer Science',\n",
        "    'Education': 'Education & Teaching',\n",
        "    'Medicine & Health Sciences': 'Medical Books',\n",
        "    'Medicine &amp; Health Sciences': 'Medical Books',\n",
        "    'Science & Mathematics': 'Science & Math',\n",
        "    'Science &amp; Mathematics': 'Science & Math',\n",
        "    'Social Sciences': 'Politics & Social Sciences',\n",
        "    'Test Prep & Study Guides': 'Test Preparation',\n",
        "    'Test Prep &amp; Study Guides': 'Test Preparation'\n",
        "}\n",
        "\n",
        "\n",
        "df['extracted_category'] = df['extracted_category'].replace(replacement_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCnCywHJQoW",
        "outputId": "c94fa047-0cfb-44d4-c342-8af81962562c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            extracted_category   count\n",
            "0                           Arts & Photography  158286\n",
            "1                        Biographies & Memoirs   91109\n",
            "2                             Business & Money   93075\n",
            "3                                    Calendars   11047\n",
            "4                             Children's Books  241105\n",
            "5                     Christian Books & Bibles  150126\n",
            "6                      Comics & Graphic Novels   41571\n",
            "7                   Communication & Journalism      97\n",
            "8                             Computer Science   51801\n",
            "9                       Cookbooks, Food & Wine   52505\n",
            "10                      Crafts, Hobbies & Home  100310\n",
            "11                        Education & Teaching   47508\n",
            "12                Engineering & Transportation   51961\n",
            "13                   Health, Fitness & Dieting   51567\n",
            "14                                     History  138229\n",
            "15                                  Humanities   58410\n",
            "16                       Humor & Entertainment   49991\n",
            "17                                         Law   14602\n",
            "18  Lesbian, Gay, Bisexual & Transgender Books    7570\n",
            "19                        Literature & Fiction  355370\n",
            "20                               Medical Books   62234\n",
            "21                Mystery, Thriller & Suspense   83863\n",
            "22                   Parenting & Relationships   16205\n",
            "23                  Politics & Social Sciences   89971\n",
            "24                                   Reference   69827\n",
            "25                     Religion & Spirituality   77120\n",
            "26                                     Romance   58542\n",
            "27                              Science & Math   69178\n",
            "28                   Science Fiction & Fantasy   61197\n",
            "29                                   Self-Help   53525\n",
            "30                           Sports & Outdoors   32915\n",
            "31                          Teen & Young Adult   43353\n",
            "32                            Test Preparation    6001\n",
            "33                                      Travel   48763\n",
            "34                                     unknown  396015\n"
          ]
        }
      ],
      "source": [
        "grouped_df = df.groupby('extracted_category')['count'].sum().reset_index()\n",
        "print(grouped_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8u0doeVF2Lb"
      },
      "source": [
        " Merging the extracted categories (df) after all preprocessing, with the relevant asins from the combnined_df that contains all the metadata categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr1--oQPJjpZ"
      },
      "outputs": [],
      "source": [
        "combined_df = combined_df[[\"category\", \"asin\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3AZS09JfKNUb",
        "outputId": "40277f8b-44dd-4fb8-cfa5-65f282834d1c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-182c1328-1d6a-48af-8688-11544024292a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>asin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[]</td>\n",
              "      <td>0000092878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['Books', 'New, Used &amp; Rental Textbooks', 'Med...</td>\n",
              "      <td>000047715X</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['Books', 'Arts &amp; Photography', 'Music']</td>\n",
              "      <td>0000004545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['Books', 'Arts &amp; Photography', 'Music']</td>\n",
              "      <td>0000013765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[]</td>\n",
              "      <td>0000000116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-182c1328-1d6a-48af-8688-11544024292a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-182c1328-1d6a-48af-8688-11544024292a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-182c1328-1d6a-48af-8688-11544024292a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2df702f-4506-4061-8bb3-c080f0a3f743\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2df702f-4506-4061-8bb3-c080f0a3f743')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2df702f-4506-4061-8bb3-c080f0a3f743 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                            category        asin\n",
              "0                                                 []  0000092878\n",
              "1  ['Books', 'New, Used & Rental Textbooks', 'Med...  000047715X\n",
              "2           ['Books', 'Arts & Photography', 'Music']  0000004545\n",
              "3           ['Books', 'Arts & Photography', 'Music']  0000013765\n",
              "4                                                 []  0000000116"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLApI5t4Kew1"
      },
      "outputs": [],
      "source": [
        "result_df = combined_df.merge(df, on='category', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlsckBz6Kj0C"
      },
      "outputs": [],
      "source": [
        "result_df= result_df[[\"asin\", \"extracted_category\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsbvzQ6ILLDM",
        "outputId": "01679369-6103-419d-e110-c3fd0e46c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               asin            extracted_category\n",
            "0        0000092878                       unknown\n",
            "1        000047715X                 Medical Books\n",
            "2        0000004545            Arts & Photography\n",
            "3        0000013765            Arts & Photography\n",
            "4        0000000116                       unknown\n",
            "...             ...                           ...\n",
            "2934944  B01HJBPTUI  Mystery, Thriller & Suspense\n",
            "2934945  B01HJC63ZM          Literature & Fiction\n",
            "2934946  B01HJEB422                       unknown\n",
            "2934947  B01HJDS76Y       Religion & Spirituality\n",
            "2934948  B01HJFHYMA                       unknown\n",
            "\n",
            "[2934949 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrWEb7b8PWIx"
      },
      "outputs": [],
      "source": [
        "result_df.to_parquet(\"asin_to_category.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlvOooZ8QdZp"
      },
      "source": [
        "Now we combine the first file  we have now in the memory with the asin_to_category, to get the reviews, with their categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNU-BX8EQ1Ht"
      },
      "outputs": [],
      "source": [
        "asin_file=f\"/content/drive/MyDrive/asin_to_category.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVESdIEKQl2R"
      },
      "outputs": [],
      "source": [
        "asin_to_category = pd.read_parquet(asin_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfC2Hmxca3IB"
      },
      "source": [
        "The result of next line is the reviews, with generes extracted from the metadata file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJnScqOuP89k"
      },
      "outputs": [],
      "source": [
        "result_df = df_filtered.merge(asin_to_category, on='asin', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC6ppHBhlhv-",
        "outputId": "51ce6818-bf69-484a-e75a-e0ce5ba04c23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "result_df['extracted_category'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atb99a_YNr9K",
        "outputId": "a791163b-fd3d-4ff0-8bb4-d14e698708a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['unknown', 'Literature & Fiction', 'Science Fiction & Fantasy',\n",
              "       'Science & Math', 'Travel', \"Children's Books\",\n",
              "       'Biographies & Memoirs', 'Self-Help', 'Christian Books & Bibles',\n",
              "       'Mystery, Thriller & Suspense', 'Business & Money', 'Romance',\n",
              "       'Comics & Graphic Novels', 'Sports & Outdoors',\n",
              "       'Politics & Social Sciences', 'Humanities', 'Medical Books',\n",
              "       'Humor & Entertainment', 'Health, Fitness & Dieting', 'History',\n",
              "       'Cookbooks, Food & Wine', 'Law', 'Reference', 'Arts & Photography',\n",
              "       'Religion & Spirituality',\n",
              "       'Lesbian, Gay, Bisexual & Transgender Books', 'Teen & Young Adult',\n",
              "       'Crafts, Hobbies & Home', 'Computer Science',\n",
              "       'Parenting & Relationships', 'Engineering & Transportation',\n",
              "       'Education & Teaching', 'Test Preparation'], dtype=object)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df['extracted_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbrtP83WT1HB"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(result_df, test_size=0.2)\n",
        "\n",
        "user_to_idx = {user_id: i for i, user_id in enumerate(result_df['reviewerID'].unique())}\n",
        "book_to_idx = {asin: i for i, asin in enumerate(result_df['asin'].unique())}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_to_idx = {}\n",
        "categories = result_df['extracted_category'].unique()\n",
        "categories_series = pd.Series(categories)\n",
        "\n",
        "# Drop \"unknown\" category\n",
        "categories = categories_series[categories_series != \"unknown\"].to_numpy()\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "        category_to_idx[category] = i\n",
        "\n",
        "# Add the special index for unknown categories\n",
        "category_to_idx['unknown'] = i+1"
      ],
      "metadata": {
        "id": "5wE2Du_c2-mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFh170foohQI"
      },
      "outputs": [],
      "source": [
        "train_df['user_idx'] = train_df['reviewerID'].map(user_to_idx)\n",
        "train_df['book_idx'] = train_df['asin'].map(book_to_idx)\n",
        "train_df['category_idx'] = train_df['extracted_category'].map(category_to_idx)\n",
        "test_df['user_idx'] = test_df['reviewerID'].map(user_to_idx)\n",
        "test_df['book_idx'] = test_df['asin'].map(book_to_idx)\n",
        "test_df['category_idx'] = test_df['extracted_category'].map(category_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(\n",
        "    torch.LongTensor(train_df['user_idx'].values),\n",
        "    torch.LongTensor(train_df['book_idx'].values),\n",
        "    torch.LongTensor(train_df['category_idx'].values),\n",
        "    torch.FloatTensor(train_df['overall'].values)\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.LongTensor(test_df['user_idx'].values),\n",
        "    torch.LongTensor(test_df['book_idx'].values),\n",
        "    torch.LongTensor(test_df['category_idx'].values),\n",
        "    torch.FloatTensor(test_df['overall'].values)\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "GFV0t5jU3D9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnTzdmR-dt7N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4NKh39AS7jd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CollaborativeFilteringModel(nn.Module):\n",
        "    def __init__(self, result_df, n_factors=5, y_range=(0, 5.5)):\n",
        "        super(CollaborativeFilteringModel, self).__init__()\n",
        "        n_users, n_books, n_categories = result_df['reviewerID'].nunique(), result_df['asin'].nunique(), result_df['extracted_category'].nunique()\n",
        "        self.n_categories = n_categories\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.user_bias = nn.Embedding(n_users, 1)\n",
        "        self.book_factors = nn.Embedding(n_books, n_factors)\n",
        "        self.book_bias = nn.Embedding(n_books, 1)\n",
        "        self.category_factors = nn.Embedding(n_categories, n_factors)\n",
        "        self.y_range = y_range\n",
        "\n",
        "    def forward(self, user_idx, book_idx, category_idx):\n",
        "        user_embed = self.user_factors(user_idx)\n",
        "        book_embed = self.book_factors(book_idx)\n",
        "        # Handle unknown categories by using a neutral vector\n",
        "        category_embed = self.category_factors(category_idx)\n",
        "        unknown_category_mask = category_idx == (self.n_categories - 1)\n",
        "        category_embed = torch.where(unknown_category_mask.unsqueeze(1), torch.zeros_like(category_embed), category_embed)\n",
        "\n",
        "\n",
        "        user_bias = self.user_bias(user_idx)\n",
        "        book_bias = self.book_bias(book_idx)\n",
        "\n",
        "        res = (user_embed * book_embed * category_embed).sum(dim=1, keepdim=True)\n",
        "        res += user_bias + book_bias\n",
        "        predicted_rating = torch.sigmoid(res)\n",
        "        return predicted_rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Mh2x-BUaYv"
      },
      "outputs": [],
      "source": [
        "n_categories = 33\n",
        "model = CollaborativeFilteringModel(result_df).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_idx_tensor = torch.LongTensor(train_df['user_idx'].values).to(device)\n",
        "book_idx_tensor = torch.LongTensor(train_df['book_idx'].values).to(device)\n",
        "ratings_tensor = torch.FloatTensor(train_df['overall'].values).to(device)\n",
        "category_idx_tensor = torch.LongTensor(train_df['category_idx'].values).to(device)\n",
        "\n",
        "train_dataset = TensorDataset(user_idx_tensor, book_idx_tensor, ratings_tensor, category_idx_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
      ],
      "metadata": {
        "id": "xwHkQp2C35if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_idx_batch, book_idx_batch, ratings_batch, category_idx_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred_ratings = model(user_idx_batch, book_idx_batch, category_idx_batch).squeeze()\n",
        "        loss = criterion(pred_ratings, ratings_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/models/fc_with_category.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895f531a-dd39-4275-ab55-d93121cdc849",
        "id": "SwhasKj035ig"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 4.282228066921234\n",
            "Epoch 2/20, Average Loss: 2.6526901054382326\n",
            "Epoch 3/20, Average Loss: 2.2716212272644043\n",
            "Epoch 4/20, Average Loss: 2.1730966806411742\n",
            "Epoch 5/20, Average Loss: 2.147646152973175\n",
            "Epoch 6/20, Average Loss: 2.140653862953186\n",
            "Epoch 7/20, Average Loss: 2.1394695353507998\n",
            "Epoch 8/20, Average Loss: 2.1389426231384276\n",
            "Epoch 9/20, Average Loss: 2.139161709547043\n",
            "Epoch 10/20, Average Loss: 2.1384758377075195\n",
            "Epoch 11/20, Average Loss: 2.138854269981384\n",
            "Epoch 12/20, Average Loss: 2.1395420241355896\n",
            "Epoch 13/20, Average Loss: 2.1400882530212404\n",
            "Epoch 14/20, Average Loss: 2.1397496509552\n",
            "Epoch 15/20, Average Loss: 2.13979110956192\n",
            "Epoch 16/20, Average Loss: 2.1400566840171815\n",
            "Epoch 17/20, Average Loss: 2.140084183216095\n",
            "Epoch 18/20, Average Loss: 2.139975986480713\n",
            "Epoch 19/20, Average Loss: 2.1410895931720733\n",
            "Epoch 20/20, Average Loss: 2.1405095374584198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "user_idx_tensor_test = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
        "\n",
        "book_idx_tensor_test = torch.LongTensor(test_df['book_idx'].values).to(device)\n",
        "category_idx_tensor = torch.LongTensor(test_df['category_idx'].values).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_ratings_test = model(user_idx_tensor_test, book_idx_tensor_test, category_idx_tensor).squeeze().cpu().numpy()\n",
        "true_ratings = test_df['overall'].values\n",
        "true_ratings, pred_ratings_test =  true_ratings.reshape(-1), pred_ratings_test.reshape(-1)\n",
        "\n",
        "ndcg = ndcg_score([true_ratings], [pred_ratings_test])\n",
        "\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings_test)\n",
        "\n",
        "mrr = calculate_mrr(true_ratings, pred_ratings_test)\n",
        "\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_ratings, pred_ratings_test.round(), average='macro')\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings_test, squared=False))\n",
        "\n",
        "map_score = calculate_map(true_ratings, pred_ratings_test)\n",
        "print(f\"Metrics:\\nNDCG: {ndcg:.4f}\\nMAE: {mae:.4f}\\nMRR: {mrr:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\\nRMSE: {rmse:.4f}\\nMAP: {map_score:.4f}\")\n",
        "save_performance_data('fc_with_category', mae, rmse, precision, recall, f1, ndcg, mrr, map_score)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b61d586-a7cf-4cbe-c98a-4cd09ee2cf0c",
        "id": "5GRj3vZb35ii"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "NDCG: 0.9884\n",
            "MAE: 1.2925\n",
            "MRR: 0.0004\n",
            "Precision: 0.0304\n",
            "Recall: 0.2000\n",
            "F1-score: 0.0528\n",
            "RMSE: 1.2096\n",
            "MAP: 0.2268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment"
      ],
      "metadata": {
        "id": "ICAJfxjDRchl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we want to generate a senitment based on our features \"reviewText\", and \"summary\", working with already made models"
      ],
      "metadata": {
        "id": "8HFm5qB4B6ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p_o5HEku0aL",
        "outputId": "4d9de845-7a5e-4fa4-b53a-0f125a5a53f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qKPhGSrIxrs4",
        "outputId": "9b949db2-20f3-45a7-b564-88c6f5e6c1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        overall  verified      reviewerID        asin  \\\n",
              "0             5     False  A3H9YD6K9TVKDP  0001713353   \n",
              "1             5     False  A3QYDL5CDNYN66  0001061240   \n",
              "2             5     False  A1BNWEJ7RVPLQ1  0001712799   \n",
              "3             4     False  A3CKPNSGA7JOLK  0001712799   \n",
              "4             5     False  A2MOBMVHECYVLE  0002006448   \n",
              "...         ...       ...             ...         ...   \n",
              "127670        5     False  A1NSJ7IFI4IZ4Q  0316403490   \n",
              "127671        5     False  A25OO2VLCIWV51  0316403490   \n",
              "127672        5     False  A2GGSOEKD5VCPK  0316403784   \n",
              "127673        1      True  A1MD99Z7WM27LS  0316403784   \n",
              "127674        4      True  A2CISZ4JLKQRJA  0316403784   \n",
              "\n",
              "                            style  \\\n",
              "0       {'Format:': ' Hardcover'}   \n",
              "1       {'Format:': ' Hardcover'}   \n",
              "2       {'Format:': ' Hardcover'}   \n",
              "3       {'Format:': ' Hardcover'}   \n",
              "4       {'Format:': ' Hardcover'}   \n",
              "...                           ...   \n",
              "127670  {'Format:': ' Hardcover'}   \n",
              "127671  {'Format:': ' Hardcover'}   \n",
              "127672  {'Format:': ' Paperback'}   \n",
              "127673  {'Format:': ' Paperback'}   \n",
              "127674  {'Format:': ' Paperback'}   \n",
              "\n",
              "                                               reviewText  \\\n",
              "0       Over and over the king has problems.  Fortunat...   \n",
              "1       This was a favorite. I think it changed my lif...   \n",
              "2       In this early reader, Dr. Seuss explores the c...   \n",
              "3       Dr. Suess's scansion and made-up words annoy m...   \n",
              "4       Christopher Kremmer's book takes you on a jour...   \n",
              "...                                                   ...   \n",
              "127670  Eleven-year-old Cornelia Warne is destitute wh...   \n",
              "127671  Summary:  11-year-old Nell Warne has had a tou...   \n",
              "127672  Jim Thompson has a well-deserved reputation as...   \n",
              "127673  Yes, readers luv it. They swoon over it.\\n\\nI ...   \n",
              "127674  This book, by Jim Thompson, is narrated by the...   \n",
              "\n",
              "                                                  summary  __index_level_0__  \\\n",
              "0                                           Not Nice Mice                 15   \n",
              "1                                         Changed my life                 47   \n",
              "2                     Two thumbs up for this early reader                 72   \n",
              "3          A fine first read with wonderful illustrations                 98   \n",
              "4                                       A compelling read                106   \n",
              "...                                                   ...                ...   \n",
              "127670         Courtesy of Mother Daughter Book Club. com            3739741   \n",
              "127671  Like a series of 19th-century Nancy Drew myste...            3739760   \n",
              "127672                     A Literary Masterpiece Of Pulp            3739796   \n",
              "127673            A WTF OUTTA MIND ADVENTURE ZZZZZZZZZZZZ            3739805   \n",
              "127674                        Things Are Not As They Seem            3739818   \n",
              "\n",
              "                  extracted_category  \n",
              "0                   Children's Books  \n",
              "1                   Children's Books  \n",
              "2                   Children's Books  \n",
              "3                   Children's Books  \n",
              "4                             Travel  \n",
              "...                              ...  \n",
              "127670              Children's Books  \n",
              "127671              Children's Books  \n",
              "127672  Mystery, Thriller & Suspense  \n",
              "127673  Mystery, Thriller & Suspense  \n",
              "127674  Mystery, Thriller & Suspense  \n",
              "\n",
              "[127675 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b7945fb-076b-469a-aad3-1d58c0eee960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>extracted_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A3H9YD6K9TVKDP</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Over and over the king has problems.  Fortunat...</td>\n",
              "      <td>Not Nice Mice</td>\n",
              "      <td>15</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A3QYDL5CDNYN66</td>\n",
              "      <td>0001061240</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>This was a favorite. I think it changed my lif...</td>\n",
              "      <td>Changed my life</td>\n",
              "      <td>47</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1BNWEJ7RVPLQ1</td>\n",
              "      <td>0001712799</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>In this early reader, Dr. Seuss explores the c...</td>\n",
              "      <td>Two thumbs up for this early reader</td>\n",
              "      <td>72</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A3CKPNSGA7JOLK</td>\n",
              "      <td>0001712799</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Dr. Suess's scansion and made-up words annoy m...</td>\n",
              "      <td>A fine first read with wonderful illustrations</td>\n",
              "      <td>98</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A2MOBMVHECYVLE</td>\n",
              "      <td>0002006448</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Christopher Kremmer's book takes you on a jour...</td>\n",
              "      <td>A compelling read</td>\n",
              "      <td>106</td>\n",
              "      <td>Travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127670</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1NSJ7IFI4IZ4Q</td>\n",
              "      <td>0316403490</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Eleven-year-old Cornelia Warne is destitute wh...</td>\n",
              "      <td>Courtesy of Mother Daughter Book Club. com</td>\n",
              "      <td>3739741</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127671</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A25OO2VLCIWV51</td>\n",
              "      <td>0316403490</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Summary:  11-year-old Nell Warne has had a tou...</td>\n",
              "      <td>Like a series of 19th-century Nancy Drew myste...</td>\n",
              "      <td>3739760</td>\n",
              "      <td>Children's Books</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127672</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A2GGSOEKD5VCPK</td>\n",
              "      <td>0316403784</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Jim Thompson has a well-deserved reputation as...</td>\n",
              "      <td>A Literary Masterpiece Of Pulp</td>\n",
              "      <td>3739796</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127673</th>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>A1MD99Z7WM27LS</td>\n",
              "      <td>0316403784</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Yes, readers luv it. They swoon over it.\\n\\nI ...</td>\n",
              "      <td>A WTF OUTTA MIND ADVENTURE ZZZZZZZZZZZZ</td>\n",
              "      <td>3739805</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127674</th>\n",
              "      <td>4</td>\n",
              "      <td>True</td>\n",
              "      <td>A2CISZ4JLKQRJA</td>\n",
              "      <td>0316403784</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>This book, by Jim Thompson, is narrated by the...</td>\n",
              "      <td>Things Are Not As They Seem</td>\n",
              "      <td>3739818</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127675 rows  9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b7945fb-076b-469a-aad3-1d58c0eee960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b7945fb-076b-469a-aad3-1d58c0eee960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b7945fb-076b-469a-aad3-1d58c0eee960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4d03d6ef-e6c5-4743-9bd4-49cd73cb2cdc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d03d6ef-e6c5-4743-9bd4-49cd73cb2cdc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4d03d6ef-e6c5-4743-9bd4-49cd73cb2cdc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2a7daf40-0484-41de-b1a4-073d8106d19d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2a7daf40-0484-41de-b1a4-073d8106d19d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result_df"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using distilbert-base-uncased-finetuned-sst-2-english model which it's performance considered pretty good, and for each text returns \"NEGATIVE\", or \"POSITIVE\" and a number between 0.5 and 1, representing how much positive or negative the sentiment is."
      ],
      "metadata": {
        "id": "unNp6C8sCyUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, DistilBertTokenizer\n",
        "\n",
        "sentiment_analysis = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
      ],
      "metadata": {
        "id": "Ke4bDFxILiDt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "1e04fd2c383a4d5abc255b11ea1851aa",
            "3499810d06f9487eb3a9eb166a2cabd1",
            "1eb1c87e8e584536abaa8f6e93ba3d56",
            "86ca7862a67441aabd4854a130a14605",
            "3ece3bb99ca34c089f6e7d778b2789f6",
            "f3f310acd4f64ab2b2ea91bc2b0f9a7b",
            "eb33993a2f5f41f693b1160accfb6ed7",
            "f750d7d9d9214645a7938cb15ab1d509",
            "5ea450cf12e44dfabfbd6fc7a676e415",
            "0fb99cae62ce46dcbfe3e28bf74d742a",
            "b9c080b64dfb4de9a24e8b5d539f63ae",
            "1417c8d85b5a4124807b1562163839a6",
            "bf2f94f27d4041fbb333f2d794e99963",
            "4b54148316f044a48c00eab2965b756b",
            "7ec73797c312442e80b28e3a7e63cc4a",
            "09a5198f42c245c09e34d11fea465ea0",
            "7e0705935be840d9af8c58cf966dbaf6",
            "6aeb027bf9a34d74965ecac0b1a2b208",
            "6fd94deb5e4c4addba1c676dbe90c8aa",
            "6ae173fdcaab453d83197f5a7ea41cbe",
            "94cf78a7827744a2941b5aa6a5d3c6b4",
            "9ed056f5b31e43849e464f706fb3d5a9",
            "972ed82122884640a48fafd68d809f00",
            "00ad02045fb64d9d8b1f7d615ab06b02",
            "0547f9bbd0014c2fba260642fbf1d910",
            "522b3f265dc84dd3961fd6403982c7f6",
            "6e86ff3bf82744fa82ad157a472daa58",
            "9dc1d01f8cba43309681b3752574dfc8",
            "ebb106eaf4bb49dca55fecbc17559546",
            "0f55ca38a2cb43dda97ff67d78bfa4ec",
            "98efeba44bbd427e8d7f740bc9e287d3",
            "af6dd1495cd942ca83c759c98fd003d5",
            "96fddb7d521f4badbb3c0bc9a732d09b",
            "a11656b1d1ca43028152127a5ebccb32",
            "22bd775fdd004b1ea19bae263f00e732",
            "9982e5bd911445afbd8c7bbedd014c3c",
            "be55b822b075406292565e8ba624c6f1",
            "1a68d5dafa044fc09656757ac1306703",
            "0cfd4ce806c849ceb0de44b886ecf661",
            "a14adf5a9c02485780104af20087dd30",
            "fb97208bb7534237b1d170a8bf62c321",
            "6784d70c3d7441a3ab94f2172c640bd4",
            "c3dd5144b85a4381b29d7d68d2bab24e",
            "d00cd1a394a74378a2a17a200bb890d8"
          ]
        },
        "outputId": "c98b2bdd-328b-4937-cd16-f3a7d898ce8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e04fd2c383a4d5abc255b11ea1851aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1417c8d85b5a4124807b1562163839a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "972ed82122884640a48fafd68d809f00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a11656b1d1ca43028152127a5ebccb32"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, we first created a code that uses summary if exists, and only if not we would get the sentiment of the full review text. We saw by manually examiming the created sentiment that the sentiment of the summary usually generate poor results, like a row of 5 star rating, that had a summary \"try it\" and got 0.6 in the positive sentiment (range is 0.5-1, which means it wasn't adding inforamtion for our model). Then after seeing that sometimes a review of 3, might have very criticizing reveiew text, and some of the 3-stars records do have pretty good reviews, so we decided that we want to run the sentiment for the full review text. However, due to the limitation of 512 tokens maximus passed to sentiment models, and due to the amount of data that needs to be calculated the sentiment of and lack of resources (time and memory this case), we would take only first 512 tokens. For some reason, even that is too much for the sentiment model, so we decreased it even less, so now the model is using only first 510 tokens from every reveiw."
      ],
      "metadata": {
        "id": "7vx993bEYEX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it is quite heavy to run, we will show some statistics based on only the first 1000. Later we created sentiment (in parts) of all the records and it will be uploaded to the model."
      ],
      "metadata": {
        "id": "G366_DtfRzLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_score(row):\n",
        "    text = row['reviewText']\n",
        "    if pd.isna(text):\n",
        "        return None  # Return None if  reviewText is missing\n",
        "\n",
        "    # Tokenize the text and truncate to the first 510 tokens\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True, max_length=510, truncation=True)\n",
        "\n",
        "    # Convert tokens back to text string if necessary\n",
        "    truncated_text = tokenizer.decode(tokens)\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    result = sentiment_analysis(truncated_text)[0]\n",
        "    if result['label'] == \"NEGATIVE\":\n",
        "        score = -result['score']\n",
        "    else:\n",
        "        score = result['score']\n",
        "    return (score, result['label'])  # Return tuple\n",
        "\n",
        "result_df_head = result_df.head(1000)\n",
        "result_df_head['sentiment_result'] = result_df_head.apply(get_sentiment_score, axis=1)"
      ],
      "metadata": {
        "id": "V9dvgrA5aBAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcbe97c-3b03-40a7-d225-43f5d0ae09cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-a4b220a14be3>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  result_df_head['sentiment_result'] = result_df_head.apply(get_sentiment_score, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df_head[['sentiment_score', 'sentiment_label']] = pd.DataFrame(result_df_head['sentiment_result'].tolist(), index=result_df_head.index)\n",
        "print(result_df_head.head())"
      ],
      "metadata": {
        "id": "ColKJ91lBCpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77665b9-5bc7-43da-a17c-b48f1baff3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   overall  verified      reviewerID        asin                      style  \\\n",
            "0        5     False  A3H9YD6K9TVKDP  0001713353  {'Format:': ' Hardcover'}   \n",
            "1        5     False  A3QYDL5CDNYN66  0001061240  {'Format:': ' Hardcover'}   \n",
            "2        5     False  A1BNWEJ7RVPLQ1  0001712799  {'Format:': ' Hardcover'}   \n",
            "3        4     False  A3CKPNSGA7JOLK  0001712799  {'Format:': ' Hardcover'}   \n",
            "4        5     False  A2MOBMVHECYVLE  0002006448  {'Format:': ' Hardcover'}   \n",
            "\n",
            "                                          reviewText  \\\n",
            "0  Over and over the king has problems.  Fortunat...   \n",
            "1  This was a favorite. I think it changed my lif...   \n",
            "2  In this early reader, Dr. Seuss explores the c...   \n",
            "3  Dr. Suess's scansion and made-up words annoy m...   \n",
            "4  Christopher Kremmer's book takes you on a jour...   \n",
            "\n",
            "                                          summary  __index_level_0__  \\\n",
            "0                                   Not Nice Mice                 15   \n",
            "1                                 Changed my life                 47   \n",
            "2             Two thumbs up for this early reader                 72   \n",
            "3  A fine first read with wonderful illustrations                 98   \n",
            "4                               A compelling read                106   \n",
            "\n",
            "  extracted_category                 sentiment_result  sentiment_score  \\\n",
            "0   Children's Books  (-0.9885287880897522, NEGATIVE)        -0.988529   \n",
            "1   Children's Books   (0.9991274476051331, POSITIVE)         0.999127   \n",
            "2   Children's Books   (0.9996360540390015, POSITIVE)         0.999636   \n",
            "3   Children's Books   (0.9771526455879211, POSITIVE)         0.977153   \n",
            "4             Travel   (0.9988719820976257, POSITIVE)         0.998872   \n",
            "\n",
            "  sentiment_label  \n",
            "0        NEGATIVE  \n",
            "1        POSITIVE  \n",
            "2        POSITIVE  \n",
            "3        POSITIVE  \n",
            "4        POSITIVE  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-6395239f7345>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  result_df_head[['sentiment_score', 'sentiment_label']] = pd.DataFrame(result_df_head['sentiment_result'].tolist(), index=result_df_head.index)\n",
            "<ipython-input-51-6395239f7345>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  result_df_head[['sentiment_score', 'sentiment_label']] = pd.DataFrame(result_df_head['sentiment_result'].tolist(), index=result_df_head.index)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df_head[\"sentiment_label\" ].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU1q-VZH9Q_c",
        "outputId": "de66fd7f-a003-420c-f5f0-6fe18990c598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment_label\n",
              "POSITIVE    741\n",
              "NEGATIVE    259\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df_head[\"sentiment_score\" ].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12K1NlSeCRdu",
        "outputId": "c2c9f350-1ce1-4ab4-a2af-49975010e7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1000.000000\n",
              "mean        0.478907\n",
              "std         0.845089\n",
              "min        -0.999821\n",
              "25%        -0.615790\n",
              "50%         0.994503\n",
              "75%         0.999127\n",
              "max         0.999890\n",
              "Name: sentiment_score, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Creating a histogram of the sentiment scores\n",
        "plt.hist(result_df_head[\"sentiment_score\"], bins=20, color='blue', edgecolor='black')\n",
        "plt.title('Histogram of Sentiment Scores')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oEvurjreC7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "59bc5557-6f9e-41f7-c54e-a47812477fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from our sample data, most of the data that is extracted from the review text is either very close to 1 and -1, meaning very positive or negative, and there is a minority that is between 0.5-0.9 and -0.5 and -0.9.\n",
        "The reason why there is no data between -0.5 and 0.5 is that the sentiment originally is values from 0.5-1 for either positive or negative, meaning, 0.4 of positive is equal to 0.6 negative, so the model is trained to decide either negative or positive. Since of this distribution of the data, we would like to perform discritization, so eventually there would be only 4 values for the sentiment: values that are positive above 0.9 would be classified as \"loved\", where below that would be \"liked\". for negative sentiment, we would define values above 0.9 as \"hated\", where below would be \"disliked\"."
      ],
      "metadata": {
        "id": "Mf1MlhGZak4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_sentiment = result_df_head.loc[(result_df_head['sentiment_score'] > 0.5) & (result_df_head['sentiment_score'] < 0.9)]\n",
        "average_sentiment.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AO4TeQ9bDOvk",
        "outputId": "199e175d-233c-4ce8-e703-c7a6dc0358b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     overall  verified      reviewerID        asin  \\\n",
              "7          3     False  A3MBENSVWGUFRK  0002005263   \n",
              "34         5      True   ACABSLRSDYYE5  0001720392   \n",
              "35         5     False  A1PWJRR0T57SLB  0001720392   \n",
              "66         5     False  A1V09ASZ9HWQAE  0002051850   \n",
              "162        4     False  A3QYDL5CDNYN66  0002005549   \n",
              "165        4     False  A2SLA0SXRQV8M0  0002005549   \n",
              "176        3     False  A3J0OXB9KIC5SS  0002005549   \n",
              "189        3     False  A23E9QQHJLNGUI  0002005549   \n",
              "197        5     False  A1VJU3NG5W15L4  0002005549   \n",
              "245        4     False   A6Y5R84GTS368  0002226723   \n",
              "261        4     False  A3NT1B6DGJJWNV  0002173611   \n",
              "269        5     False  A1I57084EFN1K8  0002172690   \n",
              "274        5     False   AU7ND6NOCX9IA  0002172690   \n",
              "290        5     False  A3NUMRC41CR0UU  0001048767   \n",
              "296        2     False  A2DCS39GFBJV24  0001048767   \n",
              "297        5      True  A3SOV9HFM5H9T1  0001048767   \n",
              "301        3     False  A2T110FW5JH644  0001048767   \n",
              "323        5     False  A33FA68V0NCM5E  0001048767   \n",
              "341        5     False  A2JXAQ92WYPAAR  0002257556   \n",
              "348        4     False  A1V8NBMG0PDVK1  0002318075   \n",
              "\n",
              "                                     style  \\\n",
              "7    {'Format:': ' Mass Market Paperback'}   \n",
              "34               {'Format:': ' Hardcover'}   \n",
              "35               {'Format:': ' Hardcover'}   \n",
              "66               {'Format:': ' Paperback'}   \n",
              "162  {'Format:': ' Mass Market Paperback'}   \n",
              "165  {'Format:': ' Mass Market Paperback'}   \n",
              "176              {'Format:': ' Hardcover'}   \n",
              "189              {'Format:': ' Hardcover'}   \n",
              "197              {'Format:': ' Hardcover'}   \n",
              "245              {'Format:': ' Paperback'}   \n",
              "261              {'Format:': ' Paperback'}   \n",
              "269              {'Format:': ' Paperback'}   \n",
              "274  {'Format:': ' Mass Market Paperback'}   \n",
              "290              {'Format:': ' Paperback'}   \n",
              "296              {'Format:': ' Paperback'}   \n",
              "297              {'Format:': ' Paperback'}   \n",
              "301              {'Format:': ' Hardcover'}   \n",
              "323  {'Format:': ' Mass Market Paperback'}   \n",
              "341              {'Format:': ' Paperback'}   \n",
              "348  {'Format:': ' Mass Market Paperback'}   \n",
              "\n",
              "                                            reviewText  \\\n",
              "7    This novel starts off with Navajo Tribal Polic...   \n",
              "34   I bought Dr. Suess when my kids were young and...   \n",
              "35   The first time I took my brother to the main V...   \n",
              "66   Controversy has swirled around Ernest Hemingwa...   \n",
              "162  I liked this book, altho I felt he did too muc...   \n",
              "165  A very good technological thriller and caution...   \n",
              "176  If you don't have time to read Crichton's new ...   \n",
              "189  I have really enjoyed some of Mr. Crichton's b...   \n",
              "197  Prey is Crichton at his best.\\nMeet Jack, out-...   \n",
              "245  In this ambitious novel, Gaskell superbly depi...   \n",
              "261  Hughes' account of Australia's beginnings, and...   \n",
              "269  George Jonas was a successful writer and journ...   \n",
              "274  I'm sure that none of us will be surprised to ...   \n",
              "290  This year marks my first reading of \"Hamlet,\" ...   \n",
              "296  I don't consider myself a huge Shakespeare buf...   \n",
              "297  This is an important play. The guy who wrote i...   \n",
              "301  This is the seventh book of the \"Masters of Ro...   \n",
              "323  \"Julius Caesar\", as a play, breathes the altog...   \n",
              "341  Hard to believe that Lessing wrote this as a v...   \n",
              "348  Hercule Poirot has just completed his analysis...   \n",
              "\n",
              "                                               summary  __index_level_0__  \\\n",
              "7                                        A Solid Novel                134   \n",
              "34                                           Delicious               2950   \n",
              "35                                    Made me smile...               2963   \n",
              "66             The Book That Ripped the Pulitzer Apart               3861   \n",
              "162                         A little escapist thriller               9010   \n",
              "165              Very good techno-thriller by Crichton               9045   \n",
              "176                       \"Andromeda Strain\" Revisited               9242   \n",
              "189                                 Not quite 3 stars!               9547   \n",
              "197           The Andromeda Strain meets Jurassic Park               9734   \n",
              "245               A changing England captured expertly              11849   \n",
              "261  Richly detailed history of convict system and ...              12675   \n",
              "269         Where does vengeance end and terror begin?              12809   \n",
              "274  A fascinating account of a counter-terrorist m...              12973   \n",
              "290  I was pleasantly surprised by how engaging the...              13844   \n",
              "296  Interesting premise, but sloppily executed -- ...              15070   \n",
              "297  the notes tell me more than I ever knew about ...              15200   \n",
              "301                             Last of a Great Series              15659   \n",
              "323                       Shakespeare's vying supermen              16497   \n",
              "341                                    More of Lessing              16849   \n",
              "348                 The thirtieth Hercule Poirot novel              16994   \n",
              "\n",
              "               extracted_category                sentiment_result  \\\n",
              "7                         unknown  (0.8590920567512512, POSITIVE)   \n",
              "34               Children's Books  (0.6194364428520203, POSITIVE)   \n",
              "35               Children's Books  (0.6223681569099426, POSITIVE)   \n",
              "66           Literature & Fiction  (0.6365569233894348, POSITIVE)   \n",
              "162  Mystery, Thriller & Suspense  (0.8901907205581665, POSITIVE)   \n",
              "165  Mystery, Thriller & Suspense  (0.6985737085342407, POSITIVE)   \n",
              "176  Mystery, Thriller & Suspense  (0.8966445922851562, POSITIVE)   \n",
              "189  Mystery, Thriller & Suspense   (0.895078718662262, POSITIVE)   \n",
              "197  Mystery, Thriller & Suspense  (0.6218786239624023, POSITIVE)   \n",
              "245          Literature & Fiction  (0.8211759328842163, POSITIVE)   \n",
              "261    Politics & Social Sciences  (0.8908010721206665, POSITIVE)   \n",
              "269    Politics & Social Sciences  (0.8287687301635742, POSITIVE)   \n",
              "274    Politics & Social Sciences   (0.613701581954956, POSITIVE)   \n",
              "290                    Humanities  (0.8977018594741821, POSITIVE)   \n",
              "296                    Humanities  (0.8317016959190369, POSITIVE)   \n",
              "297                    Humanities  (0.8959997892379761, POSITIVE)   \n",
              "301                    Humanities   (0.766709566116333, POSITIVE)   \n",
              "323                    Humanities   (0.607483983039856, POSITIVE)   \n",
              "341          Literature & Fiction  (0.8568945527076721, POSITIVE)   \n",
              "348  Mystery, Thriller & Suspense  (0.5946194529533386, POSITIVE)   \n",
              "\n",
              "     sentiment_score sentiment_label  \n",
              "7           0.859092        POSITIVE  \n",
              "34          0.619436        POSITIVE  \n",
              "35          0.622368        POSITIVE  \n",
              "66          0.636557        POSITIVE  \n",
              "162         0.890191        POSITIVE  \n",
              "165         0.698574        POSITIVE  \n",
              "176         0.896645        POSITIVE  \n",
              "189         0.895079        POSITIVE  \n",
              "197         0.621879        POSITIVE  \n",
              "245         0.821176        POSITIVE  \n",
              "261         0.890801        POSITIVE  \n",
              "269         0.828769        POSITIVE  \n",
              "274         0.613702        POSITIVE  \n",
              "290         0.897702        POSITIVE  \n",
              "296         0.831702        POSITIVE  \n",
              "297         0.896000        POSITIVE  \n",
              "301         0.766710        POSITIVE  \n",
              "323         0.607484        POSITIVE  \n",
              "341         0.856895        POSITIVE  \n",
              "348         0.594619        POSITIVE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3e26a15-4533-4f6f-8606-7d832ae60f9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>extracted_category</th>\n",
              "      <th>sentiment_result</th>\n",
              "      <th>sentiment_score</th>\n",
              "      <th>sentiment_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>A3MBENSVWGUFRK</td>\n",
              "      <td>0002005263</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>This novel starts off with Navajo Tribal Polic...</td>\n",
              "      <td>A Solid Novel</td>\n",
              "      <td>134</td>\n",
              "      <td>unknown</td>\n",
              "      <td>(0.8590920567512512, POSITIVE)</td>\n",
              "      <td>0.859092</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>ACABSLRSDYYE5</td>\n",
              "      <td>0001720392</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>I bought Dr. Suess when my kids were young and...</td>\n",
              "      <td>Delicious</td>\n",
              "      <td>2950</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>(0.6194364428520203, POSITIVE)</td>\n",
              "      <td>0.619436</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1PWJRR0T57SLB</td>\n",
              "      <td>0001720392</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>The first time I took my brother to the main V...</td>\n",
              "      <td>Made me smile...</td>\n",
              "      <td>2963</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>(0.6223681569099426, POSITIVE)</td>\n",
              "      <td>0.622368</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1V09ASZ9HWQAE</td>\n",
              "      <td>0002051850</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Controversy has swirled around Ernest Hemingwa...</td>\n",
              "      <td>The Book That Ripped the Pulitzer Apart</td>\n",
              "      <td>3861</td>\n",
              "      <td>Literature &amp; Fiction</td>\n",
              "      <td>(0.6365569233894348, POSITIVE)</td>\n",
              "      <td>0.636557</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A3QYDL5CDNYN66</td>\n",
              "      <td>0002005549</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>I liked this book, altho I felt he did too muc...</td>\n",
              "      <td>A little escapist thriller</td>\n",
              "      <td>9010</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.8901907205581665, POSITIVE)</td>\n",
              "      <td>0.890191</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A2SLA0SXRQV8M0</td>\n",
              "      <td>0002005549</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>A very good technological thriller and caution...</td>\n",
              "      <td>Very good techno-thriller by Crichton</td>\n",
              "      <td>9045</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.6985737085342407, POSITIVE)</td>\n",
              "      <td>0.698574</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>A3J0OXB9KIC5SS</td>\n",
              "      <td>0002005549</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>If you don't have time to read Crichton's new ...</td>\n",
              "      <td>\"Andromeda Strain\" Revisited</td>\n",
              "      <td>9242</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.8966445922851562, POSITIVE)</td>\n",
              "      <td>0.896645</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>A23E9QQHJLNGUI</td>\n",
              "      <td>0002005549</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>I have really enjoyed some of Mr. Crichton's b...</td>\n",
              "      <td>Not quite 3 stars!</td>\n",
              "      <td>9547</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.895078718662262, POSITIVE)</td>\n",
              "      <td>0.895079</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1VJU3NG5W15L4</td>\n",
              "      <td>0002005549</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>Prey is Crichton at his best.\\nMeet Jack, out-...</td>\n",
              "      <td>The Andromeda Strain meets Jurassic Park</td>\n",
              "      <td>9734</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.6218786239624023, POSITIVE)</td>\n",
              "      <td>0.621879</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A6Y5R84GTS368</td>\n",
              "      <td>0002226723</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>In this ambitious novel, Gaskell superbly depi...</td>\n",
              "      <td>A changing England captured expertly</td>\n",
              "      <td>11849</td>\n",
              "      <td>Literature &amp; Fiction</td>\n",
              "      <td>(0.8211759328842163, POSITIVE)</td>\n",
              "      <td>0.821176</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A3NT1B6DGJJWNV</td>\n",
              "      <td>0002173611</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Hughes' account of Australia's beginnings, and...</td>\n",
              "      <td>Richly detailed history of convict system and ...</td>\n",
              "      <td>12675</td>\n",
              "      <td>Politics &amp; Social Sciences</td>\n",
              "      <td>(0.8908010721206665, POSITIVE)</td>\n",
              "      <td>0.890801</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A1I57084EFN1K8</td>\n",
              "      <td>0002172690</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>George Jonas was a successful writer and journ...</td>\n",
              "      <td>Where does vengeance end and terror begin?</td>\n",
              "      <td>12809</td>\n",
              "      <td>Politics &amp; Social Sciences</td>\n",
              "      <td>(0.8287687301635742, POSITIVE)</td>\n",
              "      <td>0.828769</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>AU7ND6NOCX9IA</td>\n",
              "      <td>0002172690</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>I'm sure that none of us will be surprised to ...</td>\n",
              "      <td>A fascinating account of a counter-terrorist m...</td>\n",
              "      <td>12973</td>\n",
              "      <td>Politics &amp; Social Sciences</td>\n",
              "      <td>(0.613701581954956, POSITIVE)</td>\n",
              "      <td>0.613702</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A3NUMRC41CR0UU</td>\n",
              "      <td>0001048767</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>This year marks my first reading of \"Hamlet,\" ...</td>\n",
              "      <td>I was pleasantly surprised by how engaging the...</td>\n",
              "      <td>13844</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>(0.8977018594741821, POSITIVE)</td>\n",
              "      <td>0.897702</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>A2DCS39GFBJV24</td>\n",
              "      <td>0001048767</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>I don't consider myself a huge Shakespeare buf...</td>\n",
              "      <td>Interesting premise, but sloppily executed -- ...</td>\n",
              "      <td>15070</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>(0.8317016959190369, POSITIVE)</td>\n",
              "      <td>0.831702</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>5</td>\n",
              "      <td>True</td>\n",
              "      <td>A3SOV9HFM5H9T1</td>\n",
              "      <td>0001048767</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>This is an important play. The guy who wrote i...</td>\n",
              "      <td>the notes tell me more than I ever knew about ...</td>\n",
              "      <td>15200</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>(0.8959997892379761, POSITIVE)</td>\n",
              "      <td>0.896000</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>3</td>\n",
              "      <td>False</td>\n",
              "      <td>A2T110FW5JH644</td>\n",
              "      <td>0001048767</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>This is the seventh book of the \"Masters of Ro...</td>\n",
              "      <td>Last of a Great Series</td>\n",
              "      <td>15659</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>(0.766709566116333, POSITIVE)</td>\n",
              "      <td>0.766710</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A33FA68V0NCM5E</td>\n",
              "      <td>0001048767</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>\"Julius Caesar\", as a play, breathes the altog...</td>\n",
              "      <td>Shakespeare's vying supermen</td>\n",
              "      <td>16497</td>\n",
              "      <td>Humanities</td>\n",
              "      <td>(0.607483983039856, POSITIVE)</td>\n",
              "      <td>0.607484</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>5</td>\n",
              "      <td>False</td>\n",
              "      <td>A2JXAQ92WYPAAR</td>\n",
              "      <td>0002257556</td>\n",
              "      <td>{'Format:': ' Paperback'}</td>\n",
              "      <td>Hard to believe that Lessing wrote this as a v...</td>\n",
              "      <td>More of Lessing</td>\n",
              "      <td>16849</td>\n",
              "      <td>Literature &amp; Fiction</td>\n",
              "      <td>(0.8568945527076721, POSITIVE)</td>\n",
              "      <td>0.856895</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>4</td>\n",
              "      <td>False</td>\n",
              "      <td>A1V8NBMG0PDVK1</td>\n",
              "      <td>0002318075</td>\n",
              "      <td>{'Format:': ' Mass Market Paperback'}</td>\n",
              "      <td>Hercule Poirot has just completed his analysis...</td>\n",
              "      <td>The thirtieth Hercule Poirot novel</td>\n",
              "      <td>16994</td>\n",
              "      <td>Mystery, Thriller &amp; Suspense</td>\n",
              "      <td>(0.5946194529533386, POSITIVE)</td>\n",
              "      <td>0.594619</td>\n",
              "      <td>POSITIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3e26a15-4533-4f6f-8606-7d832ae60f9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3e26a15-4533-4f6f-8606-7d832ae60f9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3e26a15-4533-4f6f-8606-7d832ae60f9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1aa3b4a0-ec1d-4a19-a196-aad44e7a538a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1aa3b4a0-ec1d-4a19-a196-aad44e7a538a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1aa3b4a0-ec1d-4a19-a196-aad44e7a538a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "average_sentiment",
              "summary": "{\n  \"name\": \"average_sentiment\",\n  \"rows\": 53,\n  \"fields\": [\n    {\n      \"column\": \"overall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verified\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviewerID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 52,\n        \"samples\": [\n          \"A1V8NBMG0PDVK1\",\n          \"A2WCZZ304SM5HC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"asin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33,\n        \"samples\": [\n          \"0006152465\",\n          \"000255383X\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"style\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"{'Format:': ' Mass Market Paperback'}\",\n          \"{'Format:': ' Hardcover'}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviewText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"Hercule Poirot has just completed his analysis of great writers of detective fiction when he is interrupted by Miss Restarick, an unimpressive lass of twenty or so with long straggly hair. The young girl starts by explaining that she might have killed someone, but before Poirot can ask more information, she says she's changed her mind and must leave. Before she closes the door, she adds \\\"You are too old. Nobody told me you were so old... I'm really very sorry.\\\" Poirot is intrigued by the girl, and enlists Mrs. Oliver's help in investigating Miss Restarick. The detective duo soon discovers that not only is the girl nowhere to be found, but that no one seems to care that she is missing.\\nIn his thirtieth appearance in a novel, Hercule Poirot is claimed to be too old. But that is surely not what the reader will think of the author's wit and cleverness. At the age of seventy-five Agatha Christie still succeeds in composing a quite entertaining mystery. Admittedly The Third Girl is not one of her masterpieces, but it still has the basic ingredients of a good detective story. The things that have changed more dramatically, in comparison to the novels she wrote in the 30's, are the flamboyant characters that make up the story. It is clear that Agatha Christie does not totally agree with the way teenagers are beginning to behave in the sixties. All they seem interested in is \\\"sniffing snow\\\", \\\"swallowing LSD\\\" and \\\"using hemp\\\". Surely, this is an exaggeration in which Agatha Christie reveals slowly losing touch with modern age.\\nNevertheless Poirot is as absurd and as able as ever, which pulls this story out of the pool of mediocrity. And be warned: the book starts of with a vital clue, so try to avoid reading the denouement while blaming yourself: \\\"I should have known it!\\\"\",\n          \"I avoided this book for a long time. After reading dozens of books on Tudor history over the years I didn't see the need to read a fictional telling of the tale of Anne Boleyn. But several friends whose taste I respect praised this book to the heavens and finally I succumbed. Unfortunately, I didn't like it.\\n\\nI don't fault this book for changing the facts of Anne and Mary Boleyn's lives or for imposing motivations that don't jibe with the current academic opinion or for putting distinctly 20th century dialogue in the mouths of 16th century characters. This is fiction and Philippa Gregory is entitled to use history to suit her story.\\n\\nI've enjoyed fictionalizations of true stories before and I knew going in that Gregory makes controversial dramatic choices. The problem wasn't the fiction, it was that the fiction wasn't compelling enough to make me forget what I knew and just be carried along by the story. No matter how I tried I'd find myself thinking, that's not what happened, or, wait a minute, that's not why Anne went to France, etc. Gregory's 20th century phrasing and 20th century motivations kept creeping in, too, jolting me out of the story. Then I'd start noticing things that didn't jibe with the facts and the way people thought in the 16th century. It didn't help that so few of the characters in the book are truly likable either.\\n\\nI've come to the conclusion that if you're a Tudor-phile, if the names Chapuys, Campeggio, Eric Ives and Retha Warnicke ring any bells, then you'll likely have a hard time suspending disbelief enough to enjoy this book. It's a weird thing but the more likely you are to be interested in the subject matter, the less entertainig this book is. If you know only the bare outlines of Anne Boleyn's story, or less, then you might enjoy this book as much as my friends have.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          \"The thirtieth Hercule Poirot novel\",\n          \"I hated it but you might love it\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"__index_level_0__\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12300,\n        \"min\": 134,\n        \"max\": 41720,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          16994,\n          37184\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extracted_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"Christian Books & Bibles\",\n          \"Reference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_result\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          [\n            0.5946194529533386,\n            \"POSITIVE\"\n          ],\n          [\n            0.5473694801330566,\n            \"POSITIVE\"\n          ]\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1220400244130667,\n        \"min\": 0.505642294883728,\n        \"max\": 0.8977479338645935,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          0.5946194529533386,\n          0.5473694801330566\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"POSITIVE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment_category(text):\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True, max_length=510, truncation=True)\n",
        "\n",
        "    truncated_text = tokenizer.decode(tokens)\n",
        "\n",
        "    result = sentiment_analysis(truncated_text)[0]\n",
        "    score = result['score']\n",
        "    sentiment = result['label']\n",
        "\n",
        "    if sentiment == \"POSITIVE\":\n",
        "        if score > 0.9:\n",
        "            category = 3 # \"Loved\"\n",
        "        else:\n",
        "            category = 2 # \"Liked\"\n",
        "    elif sentiment == \"NEGATIVE\":\n",
        "        if score > 0.9:\n",
        "            category = 0 # \"Hated\"\n",
        "        else:\n",
        "          category = 1 # \"Disliked\"\n",
        "\n",
        "    return category  # Return the category\n",
        "result_df['sentiment_category'] = result_df[\"reviewText\"].apply(get_sentiment_category)\n",
        "result_df = result_df.drop(columns=[\"verified\", \"reviewText\", \"summary\"])\n",
        "result_df.to_parquet('/content/drive/MyDrive/books_to_kindle_and_books_parquets/books/books_1_with_sentiment.parquet')\n",
        "result_df.head()\n"
      ],
      "metadata": {
        "id": "t2Y3d3xAu6v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We kept the summary and review text for a long time, however, we can't extract really important data from the review text rather the sentiment, so it seems like the right time to drop them. In addition, we first thought that we would use the feature \"verified: True/False\" but this also seems meaningless, so we would drop that as well."
      ],
      "metadata": {
        "id": "7l--kfdOcOkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here on we would use the sentiment in our model\n"
      ],
      "metadata": {
        "id": "eHEYYTxVYKjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH-2rauNZzOx"
      },
      "outputs": [],
      "source": [
        "sentiment_path=f\"/content/drive/MyDrive/books_to_kindle_and_books_parquets/books/combined_books_1_with_sentiment.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WznB3C76ZzO7"
      },
      "outputs": [],
      "source": [
        "result_df = pd.read_parquet(sentiment_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Pqze6EYwaabT",
        "outputId": "56555511-79bc-48f9-a1e8-682db8cfbcce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   overall      reviewerID        asin                      style  \\\n",
              "0        5  A3H9YD6K9TVKDP  0001713353  {'Format:': ' Hardcover'}   \n",
              "1        5  A3QYDL5CDNYN66  0001061240  {'Format:': ' Hardcover'}   \n",
              "2        5  A1BNWEJ7RVPLQ1  0001712799  {'Format:': ' Hardcover'}   \n",
              "3        4  A3CKPNSGA7JOLK  0001712799  {'Format:': ' Hardcover'}   \n",
              "4        5  A2MOBMVHECYVLE  0002006448  {'Format:': ' Hardcover'}   \n",
              "\n",
              "   __index_level_0__ extracted_category  sentiment_category  \n",
              "0                 15   Children's Books                   0  \n",
              "1                 47   Children's Books                   3  \n",
              "2                 72   Children's Books                   3  \n",
              "3                 98   Children's Books                   3  \n",
              "4                106             Travel                   3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd9d1a80-846c-426f-ba7b-660fee657be4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>style</th>\n",
              "      <th>__index_level_0__</th>\n",
              "      <th>extracted_category</th>\n",
              "      <th>sentiment_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>A3H9YD6K9TVKDP</td>\n",
              "      <td>0001713353</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>15</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>A3QYDL5CDNYN66</td>\n",
              "      <td>0001061240</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>47</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>A1BNWEJ7RVPLQ1</td>\n",
              "      <td>0001712799</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>72</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A3CKPNSGA7JOLK</td>\n",
              "      <td>0001712799</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>98</td>\n",
              "      <td>Children's Books</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>A2MOBMVHECYVLE</td>\n",
              "      <td>0002006448</td>\n",
              "      <td>{'Format:': ' Hardcover'}</td>\n",
              "      <td>106</td>\n",
              "      <td>Travel</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd9d1a80-846c-426f-ba7b-660fee657be4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bd9d1a80-846c-426f-ba7b-660fee657be4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bd9d1a80-846c-426f-ba7b-660fee657be4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90967a76-b532-4653-9b13-bb7dfa07bcab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90967a76-b532-4653-9b13-bb7dfa07bcab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90967a76-b532-4653-9b13-bb7dfa07bcab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result_df"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentModel(nn.Module):\n",
        "    def __init__(self, n_users, n_books, n_categories, n_sentiments=4, n_factors=5, y_range=(0, 5.5)):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.n_categories = n_categories\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.user_bias = nn.Embedding(n_users, 1)\n",
        "        self.book_factors = nn.Embedding(n_books, n_factors)\n",
        "        self.book_bias = nn.Embedding(n_books, 1)\n",
        "        self.category_factors = nn.Embedding(n_categories, n_factors)\n",
        "        self.sentiment_factors = nn.Embedding(n_sentiments, n_factors)\n",
        "        self.y_range = y_range\n",
        "\n",
        "    def forward(self, user_idx, book_idx, category_idx, sentiment_idx):\n",
        "        user_embed = self.user_factors(user_idx)\n",
        "        book_embed = self.book_factors(book_idx)\n",
        "        category_embed = self.category_factors(category_idx)\n",
        "        sentiment_embed = self.sentiment_factors(sentiment_idx)\n",
        "\n",
        "        unknown_category_mask = category_idx == (self.n_categories - 1)\n",
        "        category_embed = torch.where(unknown_category_mask.unsqueeze(1), torch.zeros_like(category_embed), category_embed)\n",
        "\n",
        "        user_bias = self.user_bias(user_idx)\n",
        "        book_bias = self.book_bias(book_idx)\n",
        "\n",
        "        res = (user_embed * book_embed * category_embed * sentiment_embed).sum(dim=1, keepdim=True)\n",
        "        res += user_bias + book_bias\n",
        "        predicted_rating = torch.sigmoid(res)\n",
        "        return predicted_rating * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n"
      ],
      "metadata": {
        "id": "WNDLo2aLdy7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(result_df, test_size=0.2)\n",
        "\n",
        "# Create dictionaries to map user and book IDs to integers\n",
        "user_to_idx = {user_id: i for i, user_id in enumerate(result_df['reviewerID'].unique())}\n",
        "book_to_idx = {asin: i for i, asin in enumerate(result_df['asin'].unique())}\n",
        "category_to_idx = {}\n",
        "categories = result_df['extracted_category'].unique()\n",
        "categories_series = pd.Series(categories)\n",
        "\n",
        "# Drop \"unknown\" category\n",
        "categories = categories_series[categories_series != \"unknown\"].to_numpy()\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    category_to_idx[category] = i\n",
        "\n",
        "# Add the special index for unknown categories\n",
        "category_to_idx['unknown'] = i + 1"
      ],
      "metadata": {
        "id": "pKobKOM2e9tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['user_idx'] = train_df['reviewerID'].map(user_to_idx)\n",
        "train_df['book_idx'] = train_df['asin'].map(book_to_idx)\n",
        "train_df['category_idx'] = train_df['extracted_category'].map(category_to_idx)\n",
        "test_df['user_idx'] = test_df['reviewerID'].map(user_to_idx)\n",
        "test_df['book_idx'] = test_df['asin'].map(book_to_idx)\n",
        "test_df['category_idx'] = test_df['extracted_category'].map(category_to_idx)\n"
      ],
      "metadata": {
        "id": "jIzwdvy2fB61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(\n",
        "    torch.LongTensor(train_df['user_idx'].values),\n",
        "    torch.LongTensor(train_df['book_idx'].values),\n",
        "    torch.LongTensor(train_df['category_idx'].values),\n",
        "    torch.LongTensor(train_df['sentiment_category'].values),\n",
        "    torch.FloatTensor(train_df['overall'].values)\n",
        ")\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    torch.LongTensor(test_df['user_idx'].values),\n",
        "    torch.LongTensor(test_df['book_idx'].values),\n",
        "    torch.LongTensor(test_df['category_idx'].values),\n",
        "    torch.LongTensor(test_df['sentiment_category'].values),\n",
        "    torch.FloatTensor(test_df['overall'].values)\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "A9vM1P-qfIOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_users = result_df['reviewerID'].nunique()\n",
        "n_books = result_df['asin'].nunique()\n",
        "n_categories = result_df['extracted_category'].nunique()\n",
        "n_sentiments = 4"
      ],
      "metadata": {
        "id": "fOG5tiFud_hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentModel(n_users,n_books,n_categories,n_sentiments).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
      ],
      "metadata": {
        "id": "G4C7abcGePb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for user_idx_batch, book_idx_batch, category_idx_batch, sentiment_idx_batch, ratings_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        pred_ratings = model(user_idx_batch, book_idx_batch, category_idx_batch, sentiment_idx_batch).squeeze()\n",
        "        loss = criterion(pred_ratings, ratings_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}')\n",
        "\n",
        "# Save model parameters\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/models/cf_with_sentiment.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0KQIp_fcyS",
        "outputId": "b7d603a5-b48d-4ffe-f240-0ec4cd49ba0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 3.94796217918396\n",
            "Epoch 2/20, Average Loss: 2.6270414924621583\n",
            "Epoch 3/20, Average Loss: 2.275505120754242\n",
            "Epoch 4/20, Average Loss: 2.177015006542206\n",
            "Epoch 5/20, Average Loss: 2.1475828766822813\n",
            "Epoch 6/20, Average Loss: 2.140223925113678\n",
            "Epoch 7/20, Average Loss: 2.1375300121307372\n",
            "Epoch 8/20, Average Loss: 2.135113401412964\n",
            "Epoch 9/20, Average Loss: 2.13701429605484\n",
            "Epoch 10/20, Average Loss: 2.1349894869327546\n",
            "Epoch 11/20, Average Loss: 2.137885932922363\n",
            "Epoch 12/20, Average Loss: 2.1357711291313173\n",
            "Epoch 13/20, Average Loss: 2.13765629529953\n",
            "Epoch 14/20, Average Loss: 2.1364680767059325\n",
            "Epoch 15/20, Average Loss: 2.1360452342033387\n",
            "Epoch 16/20, Average Loss: 2.1376677918434144\n",
            "Epoch 17/20, Average Loss: 2.1367310285568237\n",
            "Epoch 18/20, Average Loss: 2.137100167274475\n",
            "Epoch 19/20, Average Loss: 2.1376393866539\n",
            "Epoch 20/20, Average Loss: 2.138012716770172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_idx_tensor_test = torch.LongTensor(test_df['user_idx'].values).to(device)\n",
        "book_idx_tensor_test = torch.LongTensor(test_df['book_idx'].values).to(device)\n",
        "category_idx_tensor_test = torch.LongTensor(test_df['category_idx'].values).to(device)\n",
        "sentiment_idx_tensor_test = torch.LongTensor(test_df['sentiment_category'].values).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_ratings_test = model(user_idx_tensor_test, book_idx_tensor_test, category_idx_tensor_test, sentiment_idx_tensor_test).squeeze().cpu().numpy()\n",
        "\n",
        "true_ratings = test_df['overall'].values\n",
        "true_ratings, pred_ratings_test = true_ratings.reshape(-1), pred_ratings_test.reshape(-1)\n",
        "\n",
        "ndcg = ndcg_score([true_ratings], [pred_ratings_test])\n",
        "mae = mean_absolute_error(true_ratings, pred_ratings_test)\n",
        "mrr = calculate_mrr(true_ratings, pred_ratings_test)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_ratings, pred_ratings_test.round(), average='macro')\n",
        "rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings_test))\n",
        "map_score = calculate_map(true_ratings, pred_ratings_test)\n",
        "\n",
        "print(f\"Metrics:\\nNDCG: {ndcg:.4f}\\nMAE: {mae:.4f}\\nMRR: {mrr:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\\nRMSE: {rmse:.4f}\\nMAP: {map_score:.4f}\")\n",
        "save_performance_data('cf_with_sentiment', mae, rmse, precision, recall, f1, ndcg, mrr, map_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLRDUQpyfjpX",
        "outputId": "084d7552-60a8-49bf-c88b-f31e2aa25fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics:\n",
            "NDCG: 0.9884\n",
            "MAE: 1.2990\n",
            "MRR: 0.0004\n",
            "Precision: 0.0305\n",
            "Recall: 0.2000\n",
            "F1-score: 0.0530\n",
            "RMSE: 1.4697\n",
            "MAP: 0.2235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thoughts going forward"
      ],
      "metadata": {
        "id": "g_B3G1QAZtKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We see that some models work better than others. Especially when adding bias. The next stpes will be comparing all of the models, choosting top 2 or top 3, then conducting error anaylisys and going forward we will choose only one to try and perfect it as much as possible."
      ],
      "metadata": {
        "id": "uX5nDxx-Zv_d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e04fd2c383a4d5abc255b11ea1851aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3499810d06f9487eb3a9eb166a2cabd1",
              "IPY_MODEL_1eb1c87e8e584536abaa8f6e93ba3d56",
              "IPY_MODEL_86ca7862a67441aabd4854a130a14605"
            ],
            "layout": "IPY_MODEL_3ece3bb99ca34c089f6e7d778b2789f6"
          }
        },
        "3499810d06f9487eb3a9eb166a2cabd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3f310acd4f64ab2b2ea91bc2b0f9a7b",
            "placeholder": "",
            "style": "IPY_MODEL_eb33993a2f5f41f693b1160accfb6ed7",
            "value": "config.json:100%"
          }
        },
        "1eb1c87e8e584536abaa8f6e93ba3d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f750d7d9d9214645a7938cb15ab1d509",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea450cf12e44dfabfbd6fc7a676e415",
            "value": 629
          }
        },
        "86ca7862a67441aabd4854a130a14605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb99cae62ce46dcbfe3e28bf74d742a",
            "placeholder": "",
            "style": "IPY_MODEL_b9c080b64dfb4de9a24e8b5d539f63ae",
            "value": "629/629[00:00&lt;00:00,14.2kB/s]"
          }
        },
        "3ece3bb99ca34c089f6e7d778b2789f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3f310acd4f64ab2b2ea91bc2b0f9a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb33993a2f5f41f693b1160accfb6ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f750d7d9d9214645a7938cb15ab1d509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea450cf12e44dfabfbd6fc7a676e415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fb99cae62ce46dcbfe3e28bf74d742a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c080b64dfb4de9a24e8b5d539f63ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1417c8d85b5a4124807b1562163839a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf2f94f27d4041fbb333f2d794e99963",
              "IPY_MODEL_4b54148316f044a48c00eab2965b756b",
              "IPY_MODEL_7ec73797c312442e80b28e3a7e63cc4a"
            ],
            "layout": "IPY_MODEL_09a5198f42c245c09e34d11fea465ea0"
          }
        },
        "bf2f94f27d4041fbb333f2d794e99963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e0705935be840d9af8c58cf966dbaf6",
            "placeholder": "",
            "style": "IPY_MODEL_6aeb027bf9a34d74965ecac0b1a2b208",
            "value": "model.safetensors:100%"
          }
        },
        "4b54148316f044a48c00eab2965b756b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd94deb5e4c4addba1c676dbe90c8aa",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ae173fdcaab453d83197f5a7ea41cbe",
            "value": 267832558
          }
        },
        "7ec73797c312442e80b28e3a7e63cc4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94cf78a7827744a2941b5aa6a5d3c6b4",
            "placeholder": "",
            "style": "IPY_MODEL_9ed056f5b31e43849e464f706fb3d5a9",
            "value": "268M/268M[00:05&lt;00:00,68.7MB/s]"
          }
        },
        "09a5198f42c245c09e34d11fea465ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e0705935be840d9af8c58cf966dbaf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aeb027bf9a34d74965ecac0b1a2b208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fd94deb5e4c4addba1c676dbe90c8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae173fdcaab453d83197f5a7ea41cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94cf78a7827744a2941b5aa6a5d3c6b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ed056f5b31e43849e464f706fb3d5a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "972ed82122884640a48fafd68d809f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00ad02045fb64d9d8b1f7d615ab06b02",
              "IPY_MODEL_0547f9bbd0014c2fba260642fbf1d910",
              "IPY_MODEL_522b3f265dc84dd3961fd6403982c7f6"
            ],
            "layout": "IPY_MODEL_6e86ff3bf82744fa82ad157a472daa58"
          }
        },
        "00ad02045fb64d9d8b1f7d615ab06b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc1d01f8cba43309681b3752574dfc8",
            "placeholder": "",
            "style": "IPY_MODEL_ebb106eaf4bb49dca55fecbc17559546",
            "value": "tokenizer_config.json:100%"
          }
        },
        "0547f9bbd0014c2fba260642fbf1d910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f55ca38a2cb43dda97ff67d78bfa4ec",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98efeba44bbd427e8d7f740bc9e287d3",
            "value": 48
          }
        },
        "522b3f265dc84dd3961fd6403982c7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6dd1495cd942ca83c759c98fd003d5",
            "placeholder": "",
            "style": "IPY_MODEL_96fddb7d521f4badbb3c0bc9a732d09b",
            "value": "48.0/48.0[00:00&lt;00:00,404B/s]"
          }
        },
        "6e86ff3bf82744fa82ad157a472daa58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dc1d01f8cba43309681b3752574dfc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb106eaf4bb49dca55fecbc17559546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f55ca38a2cb43dda97ff67d78bfa4ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98efeba44bbd427e8d7f740bc9e287d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af6dd1495cd942ca83c759c98fd003d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96fddb7d521f4badbb3c0bc9a732d09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a11656b1d1ca43028152127a5ebccb32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22bd775fdd004b1ea19bae263f00e732",
              "IPY_MODEL_9982e5bd911445afbd8c7bbedd014c3c",
              "IPY_MODEL_be55b822b075406292565e8ba624c6f1"
            ],
            "layout": "IPY_MODEL_1a68d5dafa044fc09656757ac1306703"
          }
        },
        "22bd775fdd004b1ea19bae263f00e732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cfd4ce806c849ceb0de44b886ecf661",
            "placeholder": "",
            "style": "IPY_MODEL_a14adf5a9c02485780104af20087dd30",
            "value": "vocab.txt:100%"
          }
        },
        "9982e5bd911445afbd8c7bbedd014c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb97208bb7534237b1d170a8bf62c321",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6784d70c3d7441a3ab94f2172c640bd4",
            "value": 231508
          }
        },
        "be55b822b075406292565e8ba624c6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3dd5144b85a4381b29d7d68d2bab24e",
            "placeholder": "",
            "style": "IPY_MODEL_d00cd1a394a74378a2a17a200bb890d8",
            "value": "232k/232k[00:00&lt;00:00,1.98MB/s]"
          }
        },
        "1a68d5dafa044fc09656757ac1306703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cfd4ce806c849ceb0de44b886ecf661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a14adf5a9c02485780104af20087dd30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb97208bb7534237b1d170a8bf62c321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6784d70c3d7441a3ab94f2172c640bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3dd5144b85a4381b29d7d68d2bab24e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00cd1a394a74378a2a17a200bb890d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}